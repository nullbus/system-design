# 시스템 디자인 강의

안녕하세요, 이 강의에 오신 것을 환영합니다. 이 강의가 놀라운 학습 경험을 불어넣으주었으면 좋겠습니다.

_이 코스는 제 [웹사이트](https://karanpratapsingh.com/courses/system-design)에서도 볼 수 있습니다. 도움이 된다면 ⭐을 눌러주세요!_

# 목차

- **시작하기**

  - [시스템 디자인이란?](#시스템-디자인이란)

- **1장**

  - [인터넷 프로토콜(IP)](#인터넷-프로토콜(ip))
  - [OSI 계층 모델](#osi-계층-모델)
  - [TCP와 UDP](#tcp와-udp)
  - [도메인 이름 시스템(Domain Name System, DNS)](#도메인-이름-시스템(domain-name-system-dns))
  - [로드 밸런싱](#로드-밸런싱)
  - [클러스터링(Clustering)](#클러스터링(clustering))
  - [캐싱](#캐싱)
  - [콘텐츠 배포 네트워크(CDN)](#콘텐츠-배포-네트워크(cdn))
  - [프록시](#프록시)
  - [가용성](#가용성)
  - [확장성](#확장성)
  - [저장](#저장)

- **2장**

  - [데이터베이스와 DBMS](#데이터베이스와-dbms)
  - [SQL 데이터베이스](#sql-데이터베이스)
  - [NoSQL 데이터베이스](#nosql-데이터베이스)
  - [SQL vs NoSQL 데이터베이스](#sql-vs-nosql-데이터베이스)
  - [데이터베이스 복제](#데이터베이스-복제)
  - [색인](#색인)
  - [정규화와 비정규화](#정규화와-비정규화)
  - [ACID와 BASE 일관성 모델](#acid와-base-일관성-모델)
  - [CAP 이론](#cap-이론)
  - [PACELC 이론](#pacelc-이론)
  - [트랜잭선](#트랜잭션)
  - [분산 트랜잭션](#분산-트랜잭션)
  - [샤딩](#샤딩)
  - [일관된 해싱](#일관된-해싱)
  - [연합 데이터베이스](#연합-데이터베이스)

- **3장**

  - [N-티어 구조](#n-티어-구조)
  - [메시지 브로커](#메시지-브로커)
  - [메시지 큐](#메시지-큐)
  - [발행-구독](#발행-구독)
  - [엔터프라이즈 서비스 버스](#엔터프라이즈-서비스-버스)
  - [모놀리스와 마이크로서비스](#모놀리스와-마이크로서비스)
  - [이벤트 기반 설계(Event Driven Architecture, EDA)](#이벤트-기반-설계-event-driven-architecture-eda)
  - [이벤트 소싱(Event Sourcing)](#이벤트-소싱-event-sourcing)
  - [명령과 질의 책임 분리(Command and Query Responsibility Segregation, CQRS)](#명령과-질의-책임-분리-command-and-query-responsibility-segregation-cqrs)
  - [API 게이트웨이(API Gateway)](#api-게이트웨이-api-gateway)
  - [REST, GraphQL, gRPC](#rest-graphql-grpc)
  - [롱폴링, 웹소켓, SSE](#롱폴링-웹소켓-sse)

- **4장**

  - [지오해싱과 쿼드트리](#지오해싱과-쿼드트리)
  - [서킷 브레이커](#서킷-브레이커)
  - [빈도 제한](#빈도-제한)
  - [서비스 발견](#서비스-발견)
  - [SLA, SSL, SLI](sla-ssl-sli)
  - [재해 복구](#재해-복구)
  - [가상 머신과 컨테이너](#가상-머신과-컨테이너)
  - [OAuth 2.0과 OpenID Connect(OIDC)](#OAuth-20과-openid-connect-oidc)
  - [싱글 사인온](#싱글-사인온)
  - [SSL, TLS, mTLS](#ssl-tls-mtls)

- **5장**

  - [시스템 디자인 인터뷰](#시스템-디자인-인터뷰)
  - [축약 URL(URL Shortener)](#축약-url-url-shortener)
  - [WhatsApp](#whatsapp)
  - [Twitter](#twitter)
  - [Netflix](#netflix)
  - [Uber](#uber)

- **부록**

  - [다음 단계](#다음-단계)
  - [참고 목록](#참고-목록)

# 시스템 디자인이란?

이 강의를 시작하기 전에, 일단 시스템 디자인이라는 것이 무엇인지부터 이야기해 봅시다.

시스템 디자인은 특정 요구사항을 만족시키는 시스템에 필요한 전반적인 구조와 인터페이스, 데이터를 정의하는 과정입니다.
시스템 디자인을 통해 논리적이고 효율적인 시스템으로 여러분의 사업이나 조직의 요구사항을 만족시킬 수 있습니다.
그런 공학 시스템을 만들기 위해서는 체계적인 접근 방법이 필요합니다. 좋은 시스템 디자인은 인프라에서부터
실제 데이터가 어떻게 저장되는지까지 모든 부분에 대해 고려하도록 만듭니다.

## 시스템 디자인이 왜 중요한가요?

시스템 디자인은 사업 요구사항을 충족시키는 해결책을 정의하는 데 도움이 됩니다.
시스템 디자인은 실제 시스템을 만들기 전에 내려야 하는 가장 빠른 결정 중 하나이기도 합니다.
나중에 이 결정들을 바꾸는 게 매우 어렵기 때문에 미리 높은 레벨에서 생각하는 것이 필수적입니다.
또한 시스템이 발전하면서 생기는 구조적인 변경을 더 쉽게 추론하고 관리할 수 있습니다.

# 인터넷 프로토콜(IP)

IP 주소는 인터넷이나 로컬 네트워크에서 기기를 구별해주는 유일한 주소입니다. IP는 _"Internet Protocol(인터넷 프로토콜)"_ 의 축약어이며 인터넷 프로토콜은 인터넷이나 로컬 네트워크를 통해 보내지는 데이터의 형식을 관리하는 일련의 규칙입니다.

핵심적으로, IP 주소는 네트워크에서 기기 간에 정보를 주고받을 수 있게 해 주는 식별자입니다. IP 주소에는 위치 정보가 있어 기기들이 통신에 접근할 수 있도록 해 줍니다. 인터넷은 서로 다른 컴퓨터와 라우터, 웹사이트들을 구분할 수 있는 방법이 필요합니다. IP 주소는 그런 방법에 대한 해결책이며 인터넷이 동작하는 방식에 있어 매우 중요한 위치를 차지하고 있습니다.

## 버전

이제 IP 주소의 버전들을 알아보겠습니다.

### IPv4

최초의 인터넷 프로토콜은 32bit 숫자로 된 IPv4인데, 숫자.숫자.숫자.숫자 형식을 취하고 40억여개의 IP 주소만을 가질 수 있었습니다. 처음 만들어질 당시에는 충분한 숫자였지만 인터넷 보급률이 늘어남에 따라 더 나은 방법이 필요하게 되었습니다.

_예: `102.22.192.181`_

### IPv6

IPv6은 1998년에 발표된 새 프로토콜입니다. 2000년대 중반에 도입이 시작됐고 인터넷 사용자들이 기하급수적으로 증가한 이후로 지금도 진행중입니다.

IPv6은 128bit를 사용하며 알파벳과 숫자로 이루어진 16진수 표현을 사용합니다. 다시 말해 IPv6는 최대 340e+36개 가량의 IP 주소들을 가질 수 있습니다. 앞으로 수 년간 증가하는 요구사항에도 충분히 대응이 가능한 숫자입니다.

_예: `2001:0db8:85a3:0000:0000:8a2e:0370:7334`_

## 유형

IP 주소의 유형들을 알아보겠습니다.

### 공용(public)

공용 IP 주소는 한 기본 주소가 전체 네트워크와 연관된 주소입니다. 공용 주소에서는 연결된 기기 각각이 같은 IP 주소를 가집니다.

_예: ISP를 통해 라우터가 할당된 IP 주소._

### 사설(private)

사설 IP 주소는 컴퓨터나 태블릿, 스마트폰 같이 집에서 인터넷 네트워크에 연결하는 모든 기기에 할당된 유일한 IP 주소입니다.

_예: 집안 공유기가 각 기기마다 생성해준 IP 주소._

### 정적(static)

정적 IP 주소는 외부에서 자동으로 할당되는 것이 아니라 수동으로 생성하여 바뀌지 않는 주소를 의미합니다. 이 주소들은 대개 더 비싸지만 더 안정적입니다.

_예: 정적 주소는 대개 안정적인 지리 서비스나 원격 접근, 서버 호스팅 등 중요한 것들에 사용됩니다._

### 동적(dynamic)

동적 IP 주소는 시간에 따라 달라질 수 있으며 언제나 같은 값이 아닙니다. [동적 호스트 설정 프로토콜(Dynamic Host Configuration Protocol, DHCP)](https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol) 서버에 의해 할당됩니다. 주소 배포가 간단하고 필요한 경우 IP 주소를 재사용할 수도 있습니다.

_예: 소비자 장비나 개인 용도에서 주로 사용됩니다._

# OSI 계층 모델

OSI 계층 모델은 한 시스템이 다른 시스템과 서로 연결하고 통신하는 방법을 정의한 논리적이고 개념적인 모델입니다. 또한 Open System Interconnection(OSI 모델)은 논리적으로 네트워크를 정의하고 다양한 프로토콜 계층을 사용하여 컴퓨터 패킷 전송을 효율적으로 기술합니다.

OSI 모델은 컴퓨터 네트워킹의 공용어처럼 보일 수 있습니다. 통신 시스템을 일곱 개의 가상 계층으로 쌓아 놓은 개념에 기반하고 있습니다.

## OSI 모델이 왜 중요한가요?

Open System Interconnection (OSI) 모델에서 네트워크와 관련된 논의나 문서에 사용되는 공통 용어들을 정의했습니다. 이 덕분에 매우 복잡한 커뮤니케이션 과정들을 쪼개고 각 컴포넌트 레벨에서 판단할 수 있게 되었습니다.

이 모델이 요즘 가장 많이 쓰이는 TCP/IP 네트워크에서 직접 구현된 것은 아니지만 아래와 같이 더 많은 것들을 할 수 있게 해 줍니다. 

- 문제 해결을 쉽게 해 주며 전체 계층에서 위협을 특정할 수 있게 도와줍니다.
- 하드웨어 제조사들이 네트워크를 통해 서로 통신이 가능한 제품을 생산하도록 독려합니다.
- 보안이 우선이라는 마음을 계발하는 데 필수적입니다.
- 복잡한 기능을 단순한 컴포넌트들로 나눕니다.

## 계층들

OSI 모델의 일곱 가지 추상화 계층들은 위에서 아래까지 이렇게 정의됩니다.

![osi-model](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/osi-model/osi-model.png)

### 어플리케이션

데이터를 통해 유저와 직접 소통하는 계층입니다. 웹 브라우저나 이메일 클라이언트들이 이 어플리케이션 계층을 통해 통신을 시작합니다. 하지만 소프트웨어 어플리케이션 자체가 어플리케이션 레이어가 아니라는 것은 확실히 해 두겠습니다. 오히려 어플리케이션 계층은 의미 있는 데이터를 유저에게 보여줄 수 있도록 소프트웨어가 의존하는 프로토콜이나 데이터 조작에 해당합니다. 어플리케이션 계층 프로토콜에는 HTTP뿐만 아니라 SMTP도 포함됩니다.

### 프레젠테이션

프레젠테이션 계층은 Translation 계층이라고도 부릅니다. 어플리케이션 계층에서 온 데이터를 추출하여 네트워크를 통해 전송될 형식으로 변형됩니다. 프레젠테이션 계층의 기능들에는 번역, 암호화/복호화, 압축이 있습니다.

### 세션

세션 계층은 두 기기 간 통신을 열고 닫는 역할을 합니다. 연결이 맺어지고 닫히는 그 시간을 세션이라고 합니다. 세션 계층은 교환하는 데이터가 전부 전송될 때까지 세션을 열려 있도록 유지하며 자원을 낭비하지 않기 위해 전송이 끝나면 즉시 세션을 닫습니다. 또한 세션 계층은 데이터 체크포인트를 사용하여 전송을 동기화합니다.

### 트랜스포트

트랜스포트 계층(4계층 이라고도 알려진)은 두 기기의 단말 간 통신을 담당합니다. 여기에는 세션 계층의 데이터를 세그먼트라고도 하는 데이터 조각으로 나누어 네트워크 계층(3계층)으로 보냅니다. 또한 수신 기기에서 세그먼트들을 조립하여 세션 계층에서 사용할 수 있게 합니다.

### 네트워크

네트워크 계층은 두 개의 다른 네트워크 간의 데이터 전송을 담당합니다. 보내는 기기에서는 트랜스포트 계층에서 온 세그먼트들을 더 작은 패킷이라는 단위로 나누고, 받는 기기에서는 패킷들을 조립합니다. 또한 네트워크 계층은 라우팅이라고 하는, 목적지까지 가는 최적의 물리 경로를 찾는 일도 합니다. 만약 통신하는 두 기기가 같은 네트워크 안에 있다면 네트워크 계층은 필요하지 않습니다.

### 데이터 링크

데이터 링크 계층은 같은 네트워크 내의 기기들과의 데이터 전송을 다루는 것만 빼면 네트워크 계층와 매우 비슷합니다. 데이터 링크 계층은 네트워크 계층에서 패킷을 받아 프레임이라고 하는 더 작은 조각으로 나눕니다.

### 물리

물리 계층은 케이블이나 스위치 같은 데이터 전송에 필요한 물리 장비들이 포함됩니다. 물리 계층에서 데이터가 실제 0과 1로 구성된 비트 스트림으로 바뀝니다. 물리 계층에서의 두 장비는 반드시 0과 1을 구분하기 위해 시그널 규약을 지켜야 합니다.

# TCP와 UDP

## TCP

전송 제어 프로토콜(Transmission Control Protocol, TCP)는 연결이 성립되고, 데이터가 양방향으로 전송될 수 있다는 점에서 연결 기반입니다. TCP는 에러 체크나 데이터 순차 전송을 보장하는 기능들이 내장되어 있기 때문에 정지 이미지나 데이터 파일, 웹페이지 등을 전송하기에 완벽한 프로토콜입니다.

![tcp](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/tcp-and-udp/tcp.png)

TCP가 직관적으로 안정적이지만 자체 피드백 메커니즘이 더 큰 오버헤드를 가져오기 때문에 더 많은 네트워크 대역폭 사용으로 이어지게 됩니다. 

## UDP

유저 데이터그램 프로토콜(User Datagram Protocol, UDP)는 간단하면서도 비연결적인 인터넷 프로토콜이기 때문에 에러 체크나 복구 시스템이 필요하지 않습니다. UDP에서는 연결을 맺거나 유지하고 종료하는 오버헤드 자체가 없습니다. 데이터가 수신인이 받는지 여부와 상관없이 꾸준히 보내집니다.

![udp](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/tcp-and-udp/udp.png)

UDP는 브로드캐스트나 멀티캐스트와 같은 실시간 통신에 굉장히 자주 사용됩니다. 낮은 지연시간과 최신 데이터가 데이터 유실보다 더 중요하다면 TCP보다는 UDP를 사용하는 것이 좋습니다.

## TCP vs UDP

TCP는 연결 지향 프로토콜이고 UDP는 비연결 프로토콜입니다. TCP와 UDP의 가장 큰 차이는 속도인데, 대개 TCP가 UDP보다 느립니다. 전반적으로 UDP가 훨씬 빠르고 간단하며 효율적인 프로토콜이지만 유실된 데이터 패킷에 대한 재전송은 TCP에서만 가능합니다.

TCP는 유저에서 서버로의 순차적인 데이터 전송을 (또는 반대로) 제공하지만, UDP는 단말 간의 통신만을 지원하는 것도 아니고 수신자의 상태를 확인하지도 않습니다.

| 기능                 | TCP                    | UDP                |
| ------------------- | ----------------------- | ------------------ |
| 연결        | 연결 수립 필요                    | 비연결 프로토콜           |
| 전송 보장   | 데이터 전송 보장                   | 데이터 전송 보장하지 않음  |
| 재전송      | 유실된 데이터에 대한 재전송         | 유실된 데이터에 대한 재전송 없음 |
| 속도        | UDP보다 느림                     | TCP보다 빠름                    |
| 브로드캐스팅 |  지원하지 않음                    | 지원함              |
| 사용예      | HTTPS, HTTP, SMTP, POP, FTP 등등 | 비디오 스트리밍, DNS, VoIP 등등 |

# 도메인 이름 시스템(Domain Name System, DNS)

위에서 다른 기기와 연결하기 위해 모든 기기에 IP 주소가 필요하다고 배웠습니다. 하지만 사람은 숫자보다는 이름이 좀 더 편안하지요. `122.250.192.232`같은 것보다는 `google.com`이라는 이름이 더 기억하기 쉽습니다.

이런 이유로 사람이 읽을 수 있는 도메인 이름을 IP 주소로 바꿔주는 도메인 이름 시스템(DNS)이라는 계층적이고 탈집중화된 이름 시스템이 나타나게 되었습니다.

## DNS의 동작 방식

![how-dns-works](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/domain-name-system/how-dns-works.png)

DNS 룩업은 여덟 단계로 나뉩니다:

1. 클라이언트가 [example.com](http://example.com)을 웹 브라우저에 입력하면 쿼리가 인터넷을 떠돌다가 DNS 리졸버가 수신하게 됩니다.
2. 리졸버는 DNS 루트 네임서버를 재귀적으로 찾습니다.
3. 루트 서버가 리졸버에 탑레벨 도메인(Top-Level Domain, TLD)의 주소를 알려줍니다.
4. 리졸버는 `.com` TLD에 대한 요청을 만듭니다.
5. TLD 서버는 해당 도메인 [example.com](http://example.com)의 네임서버의 주소를 알려줍니다.
6. 재귀 지졸버가 도메인의 네임서버에 쿼리를 보냅니다.
7. 네임서버가 [example.com](http://example.com)의 IP 주소를 리졸버에게 반환합니다.
8. DNS 리졸버는 웹 브라우저에게 처음 요청했던 도메인의 IP 주소를 반환합니다.

IP 주소가 얻어지면 클라이언트는 해당 주소를 통해 콘텐츠를 요청할 수 있어야 합니다. 예를 들어, 얻어진 IP 주소에서 브라우저에 표시할 웹페이지를 리턴할 수도 있습니다.

## 서버 종류

DNS 인프라를 구성하는 네 가지의 핵심 서버군을 알아보겠습니다.

### DNS 리졸버

DNS 리졸버(또는 DNS 재귀 리졸버)는 DNS 쿼리의 가장 첫 단계입니다. 재귀 리졸버는 클라이언트와 DNS 네임서버 사이의 미들웨어 역할을 합니다. 재귀 리졸버가 웹 클라이언트에서 DNS 쿼리를 받으면 캐시된 데이터를 응답하거나 루트 네임서버에 요청을 보낸 다음 TLD 네임서버에 다른 요청을 보내고 마지막으로 권위 있는 네임서버에 요청을 보냅니다. 재귀 리졸버는 권위 있는 네임서버에서 요청한 IP 주소에 대한 응답을 받은 다음 그 응답을 클라이언트에 전달합니다.

### DNS 루트 서버

루트 서버는 재귀 리졸버의 도메인 이름을 포함한 쿼리를 받고 도메인의 끝부분에 따라 (`.com`, `.net`, `.org` 등) TLD 네임서버로 재귀 리졸버를 알려줍니다. 루트 네임서버는 [Internet Corporation for Assigned Names and Numbers (ICANN)](https://www.icann.org)이라고 하는 비영리 단체가 감독합니다.

모든 재귀 리졸버는 13개의 루트 네임서버를 알고 있습니다. 13개의 루트 네임서버가 있다는 말이 실제로 루트 네임서버 시스템에 13개의 머신만 있다는 뜻은 아닙니다. 13종류의 루트 네임서버가 있지만 여러 복제본들이 전세계에 있어서 [Anycast routing](https://en.wikipedia.org/wiki/Anycast)를 통해 빠른 응답을 제공합니다.

### TLD 네임서버

TLD 네임서버는 `.com`이나 `.net`과 같이 URL의 마지막 . 뒤에 달리는 공통 도메인 확장을 공유하는 모든 도메인 이름에 대한 정보를 관리합니다.

TLS 네임서버의 관리는 [ICANN](https://www.icann.org)의 지부인 [Internet Assigned Numbers Authority (IANA)](https://www.iana.org)에서 합니다. IANA는 TLD 서버를 두 개의 메인 그룹으로 나눕니다:

- **일반 최상위 도메인**: `.com`, `.org`, `.net`, `.edu`, `.gov`와 같은 도메인.
- **국가 코드 최상위 도메인**: 특정 국가나 주와 관련된 모든 도메인이 포함됩니다. 예를 들어 `.uk`, `.us`, `.ru`, `.jp`가 있습니다.

### 권위 있는 DNS 서버

권위 있는 네임서버는 대개 IP 주소를 찾기 위한 리졸버의 마지막 단계입니다. 권위 있는 네임서버는 서비스하는 특정 도메인(예를 들어 [google.com](http://google.com))에 대한 정보를 가지고 있어서 DNS A 레코드에 있는 서버의 IP 주소를 재귀 리졸버에게 알려줍니다. 또는 CNAME 레코드가 있다면(별명) 재귀 도메인에게 alias 도메인을 알려주어 재귀 리졸버가 완전히 새로운 DNS 룩업을 시작하게 할 수도 있습니다 (보통 A 레코드가 IP 주소 정보를 가지고 있습니다). 만약 도메인을 찾을 수 없다면 NXDOMAIN 메시지가 반환됩니다.

## 쿼리 종류

DNS 시스템에는 세 종류의 쿼리가 있습니다:

### 재귀(Recursive)

재귀 쿼리는 DNS 클라이언트가 DNS 서버에게(보통 DNS 재귀 리졸버라고 합니다) 요청한 레코드를 알려주거나 찾을 수 없는 경우 에러를 반환하도록 요구합니다.

### 순차(Iterative)

순차 쿼리는 DNS 클라이언트가 hostname을 제공하고 DNS 리졸버는 가능한 가장 최선의 답을 반호나합니다. 만약 DNS 리졸버가 캐시에 연관된 DNS 레코드를 가지고 있으면 그것을 반환합니다. 찾을 수 없다면 DNS 클라이언트에 루트 서버나 요청한 DNS 존에 있는 가장 가까운 권위 있는 네임서버를 알려줍니다. DNS 클라이언트는 응답된 DNS 서버로 같은 쿼리를 보내야 합니다.

### 비재귀(Non-recursive)

비재귀 쿼리는 DNS 리졸버가 이미 답을 알고 있는 경우입니다. 그게 로컬 캐시에 이미 저장했던 DNS 레코드일 수도 있고 해당 서버가 레코드에 대해 권위가 있는 서버인 경우일 수도 있습니다. 후자의 경우 해당 hostname에 대해 올바른 IP를 들고 있는 경우입니다. 두 경우 모두 다 (재귀나 순차 쿼리처럼) 추가 쿼리 단계를 가질 필요가 없습니다. 응답은 즉시 클라이언트에 전달됩니다.

## 레코드 종류

DNS 레코드(또는 존 파일)는 권위 있는 DNS 서버에 있는 정보로서 관련된 IP주소가 무엇인지, 이 요청을 어떻게 처리할지와 같은 방법들을 포함한 해당 도메인의 정보를 제공합니다.

이 레코드들은 _DNS 문법_ 이라고 하는 일련의 텍스트 파일로 이루어져 있습니다. DNS 문법은 그저 DNS 서버가 어떻게 해야 할 지 알려주는 명령이 담긴 문자열입니다. 또한 모든 DNS 레코드들은 time-to-live라는 뜻의 _"TTL"_ 정보가 있어 DNS 서버가 얼마나 자주 정보를 갱신해야 하는지 알려줍니다.

더 많은 종류들이 있지만 가장 자주 사용하는 몇 가지만 보여드리면 아래와 같습니다:

- **A (Address record)**: 도메인의 IP 주소를 담고 있는 레코드입니다.
- **AAAA (IP Version 6 Address record)**: 도메인의 IPv6 주소를 담고 있는 레코드입니다(IPv4 주소를 담고 있는 A 레코드와 별개입니다).
- **CNAME (Canonical Name record)**: 해당 도메인 또는 서브도메인을 다른 도메인으로 보내며, IP 주소 정보가 없습니다.
- **MX (Mail exchanger record)**: 메일을 이메일 서버로 보냅니다.
- **TXT (Text Record)**: 관리자가 메모를 저장할 수 있는 레코드입니다. 대개 이메일 보안을 위해 사용되곤 합니다.
- **NS (Name Server records)**: DNS 엔트리에 네임서버 정보를 저장합니다.
- **SOA (Start of Authority)**: 도메인의 관리자 정보를 저장합니다.
- **SRV (Service Location record)**: 특정 서비스의 포트 번호를 명시합니다.
- **PTR (Reverse-lookup Pointer records)**: 리버스 룩업에서 도메인 이름을 제공합니다.
- **CERT (Certificate record)**: 공개 키 인증서를 저장합니다.

## 서브도메인

서브도메인은 메인 도메인 이름의 추가 부분입니다. 대개 웹사이트를 논리적으로 섹션으로 분리하는 데 사용됩니다. 메인 도메인에서 여러 개의 서브도메인이나 자식 도메인을 만들 수 있습니다.

예를 들어, `blog.example.com`에서 `blog`가 서브도메인이고 `example`가 메인 도메인, `.com`이 탑레벨 도메인(TLD)입니다. 비슷한 예로 `support.example.com`이나 `careers.example.com`이 있습니다.

## DNS 존

DNS 존은 도메인 네임스페이스와는 별개로 DNS 존을 유지보수하는 개인이나 조직, 회사와 같은 법적 존재를 대리하는 부분입니다. 또한 DNS 존은 관리 기능으로서 권위 있는 네임 서버의 DNS 컴포넌트의 세부 제어를 가능하게 합니다.

## DNS 캐싱

DNS 캐시 (또는 DNS 리졸버 캐시라고도 하는)는 컴퓨터의 운영체제가 관리하는 임시 데이터베이스입니다. DNS 캐시는 최근에 방문했거나 방문하려고 시도했던 웹사이트와 다른 인터넷 도메인에 대한 모든 정보를 포함하고 있습니다. 다시 말해서, DNS 캐시는 컴퓨터가 웹사이트를 어떻게 로드해야 할 지 빠르게 알아낼 수 있는 최근의 DNS 룩업에 대한 기억일 뿐입니다.

도메인 이름 시스템은 모든 DNS 레코드에 time-to-live(TTL)을 구현합니다. TTL은 DNS 클라이언트나 서버가 해당 레코드를 몇초나 저장할 지 지정합니다. 레코드가 캐시에 저장되면 딸려오는 TTL 값도 같이 저장됩니다. 서버는 계속하여 캐시에 있는 레코드의 TTL값을 매초마다 갱신합니다. TTL이 0이 되면 레코드가 삭제되거나 캐시에서 제거됩니다. 그 시점에 해당 DNS에 대한 쿼리가 들어오게 되면 DNS 서버는 리졸브 과정을 시작해야 합니다.

## 역방향 DNS

역방향 DNS 룩업은 주어진 IP 주소로 도메인 이름을 찾는 쿼리입니다. 이 기능은 훨씬 자주 사용되는 도메인 이름에서 IP 주소를 반환하는 정방향 DNS 룩업의 반대 작업입니다. IP 주소를 역방향으로 찾는 것은 PTR 레코드를 사용합니다. 만약 서버에 PTR 레코드가 없다면 역방향 룩업을 할 수 없습니다.

역방향 룩업은 대개 이메일 서버가 사용합니다. 이메일 서버는 이메일 메시지를 네트워크에 가져오기 전 적합한 서버에서 온 것인지 확인합니다. 많은 이메일 서버들이 역방향 룩업이 되지 않는 서버나 적절한 서버가 아닌 경우 메시지를 받아들이지 않습니다.

_Note: 역방향 DNS 룩업은 인터넷의 일반 기능에 특히 중요한 역할을 수행하는 것이 아니기 때문에 모두가 지원하지는 않습니다._

## 예제들

주로 사용되는 DNS 솔루션들입니다.

- [Route53](https://aws.amazon.com/route53)
- [Cloudflare DNS](https://www.cloudflare.com/dns)
- [Google Cloud DNS](https://cloud.google.com/dns)
- [Azure DNS](https://azure.microsoft.com/en-in/services/dns)
- [NS1](https://ns1.com/products/managed-dns)

# 로드 밸런싱

로드 밸런싱은 밀려들어오는 네트워크 트래픽을 여러 리소스로 분배하고 연결된 리소스에만 요청을 보내도록 하여 고가용성과 안정성을 확보합니다. 요구하는 정도에 따라 리소스를 추가하거나 제거할 수 있는 유연성을 확보하게 됩니다.

![load-balancing](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/load-balancing/load-balancer.png)

추가적인 확장성이나 복제를 위해 로드 밸런스를 시스템의 각 레이어에 적용해볼 수 있습니다:

![load-balancing-layers](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/load-balancing/load-balancer-layers.png)

## 하지만 왜?

현대의 고 트래픽 웹사이트들은 반드시 유저 또는 클라이언트에게서 오는 수십 수백만의 동시 요청을 처리해야 합니다. 비용 효율적으로 이런 대규모의 용량을 처리하기 위해 현대 컴퓨팅에서는 일반적으로 서버를 추가하는 것이 좋은 방법입니다.

로드 밸런서는 서버 앞에서 클라이언트의 요청들을 모든 서버로 라우팅하여 속도를 최대화하고 여유 리소스를 활용하는 식으로 동작합니다. 어떤 서버 하나라도 과부하되어 성능을 깎아먹지 않게 합니다. 만약 서버 하나가 내려가더라도 로드 밸런서는 연결된 남은 서버들로 트래픽을 돌립니다. 새 서버가 그룹에 추가되면 자동으로 로드밸런서가 새 서버로 요청을 보내게 됩니다.

## 워크로드 분배

로드 밸런서마다 조금씩 다를 수 있지만 제공되는 핵심 기능은 아래와 같습니다:

- **호스트 기반**: 요청된 호스트 이름에 따라 요청을 분산합니다.
- **경로 기반**: 호스트 이름만 사용하지 않고 전체 URL을 기준으로 요청을 분산합니다.
- **내용 기반**: 요청의 메시지 내용을 조사하여 요청을 분산합니다. 파라미터의 값에 따라 분산을 하는 것도 가능합니다.

## 계층

일반적으로 로드 밸런서는 둘 중 하나의 계층 위에서 동작합니다:

### 네트워크 계층

4계층이라고도 알려진 네트워크의 트랜스포트 계층에서 동작하는 로드 밸런서입니다. IP 주소와 같은 네트워킹 정보를 이용하여 분배하며 내용을 기반으로 분배는 할 수 없습니다. 종종 고속으로 처리하는 전용 하드웨어를 사용합니다.

### 어플리케이션 계층

7계층이라고도 알려진 어플리케이션 계층에서 동작하는 로드 밸런서입니다. 이 로드 밸런서는 전체 요청 내용을 읽을 수 있고 내용 기반 분배를 할 수 있습니다. 트래픽 내용을 잘 이해한 상황에서 로드 밸런스를 할 수 있습니다.

## 로드 밸런서 종류

어떤 로드 밸런서들이 있는지 살펴봅시다:

### 소프트웨어

소프트웨어 로드 밸런서들은 하드웨어 로드 밸런서들보다 설치하기 쉽습니다. 또한 비용 효율적이며 기능이 유연한 경향이 있고 소프트웨어 개발 환경과 결합하여 사용하기도 합니다. 소프트웨어 로드밸런서를 사용하면 개발 환경의 특수한 요구사항도 맞출 수 있는 유연함을 줍니다. 유연함에서 오는 추진력은 로드 밸런서를 설치할 때 발생하는 비용과 노력과도 연관이 있습니다. 블랙박스처럼 동작하는 하드웨어 로드밸런서와 비교해서 소프트웨어 로드밸런서는 변경이나 업그레이드에 있어서 더 많은 자유가 있습니다.

소프트웨어 로드 밸런서는 이미 널리 사용되고 있으며 직접 설정하고 설치 가능한 솔루션이나 관리되는 클라우드 솔루션으로도 존재합니다.

### 하드웨어

이름에서 알 수 있듯이 하드웨어 로드 밸런서는 물리적으로 설치된 하드웨어를 사용하여 어플리케이션과 네트워크 트래픽을 분산합니다. 이 기기들은 대용량의 트래픽을 처리할 수 있지만 종종 비용이 비싸고 유연성에 있어 제한된 모습을 보입니다.

하드웨어 로드 밸런서들은 관리와 업데이트가 필요한 자체 펌웨어가 포함되고 보안 패치도 종종 올라옵니다.

### DNS

DNS 로드 밸런싱은 도메인 이름 시스템(DNS)를 설정하여 클라이언트의 도메인 접근을 서버 머신 그룹에 분배하는 방식입니다.

하지만 DNS 로드 밸런싱은 구조적으로 안정성과 효율성이 제약됩니다. 특히나 DNS는 서버 또는 네트워크의 장애나 에러를 확인할 수 없습니다. 서버가 내려가건 말건 도메인의 동일한 IP주소들을 반환할 뿐입니다.

## 라우팅 알고리즘

이제 일반적으로 사용되는 라우팅 알고리즘에 대해 알아봅시다:

- **라운드 로빈**: 서버들마다 돌아가면서 요청을 분배합니다.
- **가중치 라운드 로빈**: 간단한 라운드 로빈 기법에 더해 연산이나 트래픽 처리를 관리자가 DNS 레코드에 할당할 수 있는 가중치를 부여하여 서버마다 다른 특징에 대응할 수 있게 합니다.
- **가장 적은 연결**: 새 요청을 가장 적은 클라이언트 연결을 가지고 있는 서버에 보냅니다. 상대적인 연산 산출량이 가장 적은 연결수로 산정되는 것입니다.
- **가장 적은 응답 시간**: 가장 빠른 응답 시간과 가장 적은 활성 연결을 가진 서버를 공식으로 선택하여 요청을 보냅니다.
- **가장 적은 대역폭**: 이 방법에서는 트래픽을 Mbps로 측정하여 가장 적은 트래픽을 가진 서버에 클라이언트 요청을 보냅니다.
- **해싱**: 클라이언트 IP나 요청 URL과 같은 미리 정의해둔 키로 요청을 분산합니다.

## 장점

다운타임을 방지해준다는 점에서 로드 밸런싱이 핵심적인 역할을 하고 있고, 다른 로드 밸런싱의 이점은 아래와 같습니다:

- 확장성
- 복제
- 유연성
- 효율성

## 복제 로드 밸런서

생각해보셨을지도 모르겠지만 로드 밸런서 자체가 장애 포인트가 될 수도 있습니다. 이를 극복하기 위해 두 번째 또는 `N`개의 로드 밸런서를 클러스터 모드로 동작시킬 수 있습니다.

만약 장애가 있고 _활성화된_ 로드 밸런서가 떨어진다면 또다른 _예비_ 로드 밸런서가 이어받아 전체 시스템이 장애에 더 강한 시스템이 됩니다.

![redundant-load-balancing](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/load-balancing/redundant-load-balancer.png)

## 기능

아래 목록은 로드 밸런서들에게 일반적으로 요구되는 기능입니다:

- **Autoscaling**: 지정한 조건에 따라 리소스를 올리고 내릴 수 있어야 합니다.
- **Sticky sessions**: 같은 유저나 기기를 같은 리소스에 할당하여 리소스의 세션 상태를 유지할 수 있어야 합니다.
- **Healthchecks**: 리소스가 떨어지거나 제대로 동작하지 않는 경우 로드 밸런싱 풀에서 제거할 수 있어야 합니다.
- **Persistence connections**: 서버가 WebSocket과 같은 영구적인 연결을 맺을 수 있어야 합니다.
- **Encryption**: TLS나 SSL과 같은 암호화된 연결을 처리할 수 있어야 합니다.
- **Certificates**: 인증서를 클라이언트에게 노출하거나 클라이언트 인증서의 인증이 가능해야 합니다.
- **Compression**: 응답을 압축할 수 있어야 합니다.
- **Caching**: 어플리케이션 레이어 로드 밸런서는 응답을 캐시할 수도 있습니다.
- **Logging**: 요청이나 응답의 메타데이터를 로그로 남기면 감사 추적이나 분석 데이터의 원데이터가 될 수 있습니다.
- **Request tracing**: 로그나 모니터링, 문제 해결 목적으로 모든 요청에 유일한 id를 부여하여 추적할 수 있어야 합니다.
- **Redirects**: 요청 경로와 같은 인자들로 들어오는 요청을 리디렉션할 수 있어야 합니다.
- **Fixed response**: 에러 메시지와 같은 정적 응답을 줄 수 있어야 합니다.

## Examples

아래 목록은 업계에서 흔히 쓰이는 로드 밸런싱 솔루션들입니다:

- [Amazon Elastic Load Balancing](https://aws.amazon.com/elasticloadbalancing)
- [Azure Load Balancing](https://azure.microsoft.com/en-in/services/load-balancer)
- [GCP Load Balancing](https://cloud.google.com/load-balancing)
- [DigitalOcean Load Balancer](https://www.digitalocean.com/products/load-balancer)
- [Nginx](https://www.nginx.com)
- [HAProxy](http://www.haproxy.org)

# 클러스터링(Clustering)

크게 봤을 때, 컴퓨터 클러스터는 하나의 공통 목표를 달성하기 위해 병렬로 실행되는 둘 이상의 컴퓨터나 노드의 집합을 의미합니다. 클러스터는 수많은 각 병렬 가능한 작업들이 노드들에 분배될 수 있게 합니다. 그 결과로 작업들은 각 컴퓨터의 연산 능력과 메모리들을 끌어내어 전반적인 성능이 향상됩니다.

컴퓨터 클러스터를 만들기 위해서 각 노드들은 노드간 통신을 위해 네트워크에 연결되어 있어야 합니다. 노드들을 하나로 묶어 클러스터를 구성하기 위해 소프트웨어가 사용될 수 있습니다. 각 노드들은 공유되는 저장소나 로컬 저장소를 가질 수도 있습니다.

![cluster](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/clustering/cluster.png)

대개 적어도 노드 하나가 리더 노드로 선출되고 클러스터의 진입점으로 작동합니다. 리더 노드는 자신에게 오는 작업을 다른 노드로 오는 작업들을 대리할 수 있으며 필요하다면 결과를 집계하여 유저에게 응답할 수 있습니다.

클러스터는 이상적으로 하나의 시스템인 것처럼 동작해야 합니다. 클러스터에 접근하는 유저는 해당 시스템이 클러스터인지 개별 머신인지 알 필요가 없습니다. 그에 더해, 클러스터는 노드 간 통신에 지연을 줄이고 병목을 방지하도록 설계되어야 합니다.

## 종류

컴퓨터 클러스터는 일반적으로 세 가지 종류로 구분지어집니다:

- 고가용성 또는 페일오버
- 로드 밸런싱
- 고성능 컴퓨팅

## 설정

가장 많이 사용되는 고가용성(High availability, HA) 클러스터링 설정은 active-active와 active-passive 두 가지가 있습니다.

### Active-Active

![active-active](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/clustering/active-active.png)

Active-Active 클러스터는 대개 두 개 이상의 노드로 만들어지며 동시에 같은 서비스를 실행합니다. Active-Active 클러스터를 사용하는 주 목적은 활성 로드 밸런싱입니다. 로드 밸런서가 워크로드를 모든 노드에 분배하여 어느 노드 하나라도 과부하되지 않도록 합니다. 서비스되는 더 많은 노드들이 준비되어 있기 때문에 처리량과 응답 시간에 개선이 있을 것입니다.

### Active-Passive

![active-passive](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/clustering/active-passive.png)

Active-Passive 클러스터는 active-active 설정처럼 최소 두 개 이상의 노드로 구성됩니다. 하지만 _active-passive_ 라는 이름이 암시하듯이 모든 노드가 활성화되지는 않습니다. 예를 들어 두 개의 노드가 있을 때 첫 번째 노드가 이미 활성화되어 있다면 두 번째 노드는 반드시 수동 또는 준비 상태여야 합니다.

## 장점

클러스터 컴퓨팅을 사용하는 이점은 네 가지가 있습니다:

- 고가용성
- 확장성
- 성능
- 비용 효율적

## 로드 밸런싱 vs 클러스터링

로드 밸런싱은 클러스터링와 어느 정도 속성을 공유하지만 사실 둘은 다른 개념입니다. 클러스터링은 복제를 사용하여 가용량과 가용성을 증가시킵니다. 클러스터를 구성하는 서버는 서로를 의식하며 공동의 목표를 위해 작업을 같이 수행합니다. 하지만 로드 밸런싱의 서버들은 각 서버를 의식하지 않습니다. 그 대신 로드 밸런서로부터 오는 요청들에 반응할 뿐입니다.

클러스터링과 혼합하여 로드 밸런싱을 사용할 수도 있는데, 가령 웹사이트나 비즈니스 어플리케이션, 웹 서비스 또는 다른 IT 리소스를 기동시키기 위함이라는 공통의 목적을 공유하는 독립된 서버를 구성할 수 있습니다.

## 도전

클러스터링을 하는 데 있어 가장 큰 도전은 설치 시의 복잡도와 유지 보수의 복잡도가 증가한다는 것입니다. 운영 체제와 어플리케이션, 종속성들도 전부 각 노드에 같이 설치 및 업데이트되어야 합니다.

심지어 클러스터의 노드가 같은 것이 아니라면 상황은 더 복잡해집니다. 소프트웨어가 제대로 동작하는지 확인하기 위해 각 노드의 리소스 활용도는 반드시 모니터링되어야 하고 로그도 집계하여야 합니다.

그에 더해, 저장소 역시 관리하기 까다로워지는데, 공용 저장소는 다른 노드가 덮어쓰지 않도록 해야 하며 분산 데이터 저장소는 항상 동기화되어 있어야 합니다.

## 예제

클러스터링은 업계에서 공통적으로 사용되며 많은 기법들이 어느 정도의 클러스터링 모드를 제공합니다. 예를 들면 아래와 같습니다:

- 컨테이너 (e.g. [Kubernetes](https://kubernetes.io), [Amazon ECS](https://aws.amazon.com/ecs))
- 데이터베이스 (e.g. [Cassandra](https://cassandra.apache.org/_/index.html), [MongoDB](https://www.mongodb.com))
- 캐시 (e.g. [Redis](https://redis.io/docs/manual/scaling))

# 캐싱

_"컴퓨터 과학에 두 가지 어려운 문제가 있다: 캐시 무효화와 이름 짓기가 그것이다." - Phil Karlton_

![caching](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/caching/caching.png)

캐시의 제일 목표는 느린 하위 저장소 계층의 접근 필요성을 줄여 데이터 접근 성능을 높이는 것입니다. 일반적으로 데이터를 완전하고 지속성 있게 저장하는 데이터베이스와는 달리 캐시는 일반적으로 용량보다는 속도를 트레이드오프하여 데이터를 일시적으로 저장합니다.

캐시는 지역 참조 원칙(Locality of reference principle)의 이점을 활용합니다. _"최근에 요청했던 데이터는 다시 요청될 가능성이 높다."_

## 캐싱과 메모리

캐시는 컴퓨터의 메모리와 비슷하게 데이터를 계층구조에 따라 1계층부터 순차적으로 저장하는 빠른 성능의 메모리입니다. 각각의 계층은 L1, L2, L3과 같이 이름이 붙습니다. 또한 캐시는 요청된 콘텐츠가 업데이트되거나 새로 캐시에 저장되어야 하는 경우 이미 저장되었지만 오래된 콘텐츠를 대체합니다.

캐시가 읽히거나 쓰이거나 작업은 블럭 하나 단위로 이루어집니다. 각 블럭은 캐시의 어느 위치에 데이터가 있는지 가리키는 태그를 가지고 있습니다. 데이터가 캐시에서 요청된 경우 태그를 통해 검색을 수행하며 1계층(L1)에서 필요한 콘텐츠를 찾습니다. 적합한 데이터가 없다면 L2에서의 검색을 추가로 수행합니다.

여전히 데이터를 찾지 못했다면 L3, L4 순으로 데이터를 찾을 때까지 검색을 계속하며 찾았다면 읽고 로드합니다. 데이터가 캐시에서 전혀 발견되지 않았다면 다음 번에 있을 빠른 접근을 위해 캐시에 기록됩니다.

## 캐시 히트와 캐시 미스

### 캐시 히트

캐시 히트란 콘텐츠가 성공적으로 캐시에서 가져온 경우를 의미합니다. 메모리에서 빠르게 태그를 검색하고 데이터를 찾아 읽은 경우 캐시 히트라고 합니다.

**콜드, 웜, 핫 캐시**

캐시 히트는 콜드, 웜, 핫으로 설명되기도 합니다. 각각의 경우에 어느 정도의 속도의 읽기 작업인지 설명해보겠습니다.

핫 캐시(Hot cache)는 _가장 빠른_ 가능성 영역의 메모리에서 읽은 경우입니다. L1에서 데이터를 받아온 경우 일어납니다.

콜드 캐시(Cold cache)는 _가장 느린_ 가능성 영역에서 읽은 경우입니다. 성공적으로 읽은 것이기에 여전히 캐시 히트로 간주합니다. L3나 그 아래와 같은 메모리 계층의 낮은 영역에서 데이터를 찾은 경우입니다.

웜 캐시(Warm cache)는 데이터가 L2 또는 L3에서 발견된 경우입니다. 핫 캐시만큼 빠르지는 않지만 여전히 콜드 캐시보다는 빠릅니다. 일반적으로 웜 캐시로 부르는 것은 대개 핫 캐시보다 느리면서 거의 콜드 캐시급인 경우를 표현합니다.

### 캐시 미스

모든 메모리를 검색했지만 데이터를 찾지 못한 경우를 캐시 미스라고 합니다. 캐시 미스가 발생하면 콘텐츠가 전송되고 캐시에 기록됩니다.

## 캐시 무효화

캐시 무효화는 컴퓨터 시스템이 캐시 엔트리를 부적합으로 선언하고 제거하거나 대체하는 프로세스를 지칭하는 말입니다. 만약 데이터가 바뀌었다면 캐시 무효화되어야 하겠지만 무효화되지 않았다면 어플리케이션이 일관적이지 않은 동작을 유발할 수 있습니다. 캐싱 시스템은 세 종류가 있습니다.

### Write-through 캐시

![write-through-cache](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/caching/write-through-cache.png)

데이터가 캐시에 쓰이고 해당하는 데이터베이스도 동시에 쓰입니다.

**장점**: 빠른 접근, 캐시와 저장소 간 완벽한 데이터 일관성

**단점**: 쓰기 작업에 높은 지연 시간

### Write-around 캐시

![write-around-cache](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/caching/write-around-cache.png)

쓰기 작업이 캐시를 우회하여 직접 데이터베이스나 저장소에 저장됩니다.

**장점**: 지연 시간을 줄일 수도 있다

**단점**: 캐시 미스가 있는 경우 데이터베이이스에서 정보를 읽어와야 하기 때문에 캐시 미스를 증가시킵니다. 그 결과 쓰기 다음 바로 읽기 작업이 있는 경우 높은 읽기 지연 시간이 발생하게 될 것입니다. 느린 백엔드 저장소에서 읽기가 일어나고 높은 지연을 경험하게 될 것입니다.

### Write-back 캐시

![write-back-cache](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/caching/write-back-cache.png)

쓰기 작업이 캐시 계층에서만 일어나고 캐시 계층 쓰기가 완료되면 쓰기 작업이 완료된 것으로 가정합니다. 캐시는 비동기적으로 데이터베이스에 쓰기 작업을 동기화합니다.

**장점**: 쓰기 작업이 많은 어플리케이션의 경우 지연 시간이 적어지고 높은 처리량을 기대할 수 있습니다.

**단점**: 캐시 계층이 충돌하는 경우 데이터 유실의 위험이 있습니다. 캐시의 쓰기 작업을 확인해줄 레플리카를 하나 더 두어 문제를 개선할 수 있습니다.

## 퇴거 정책(Eviction policies)

가장 흔히 사용되는 캐시 퇴거 정책은 아래와 같습니다:

- **First In First Out (FIFO)**: 이전 접근 횟수나 빈도에 상관없이 가장 처음에 접근됐던 캐시를 내보냅니다.
- **Last In First Out (LIFO)**: 이전 접근 횟수나 빈도에 상관없이 가장 최근에 접근됐던 캐시를 내보냅니다.
- **Least Recently Used (LRU)**: 사용한 지 가장 오래된 캐시를 먼저 내보냅니다.
- **Most Recently Used (MRU)**: 가장 최근에 사용한 캐시를 먼저 내보냅니다.
- **Least Frequently Used (LFU)**: 아이템이 얼마나 자주 사용됐는지 셉니다. 가장 덜 사용된 것이 먼저 버려집니다.
- **Random Replacement (RR)**: 필요한 경우 랜덤하게 선택하여 내보냄으로 용량을 확보합니다.

## 분산 캐시(Distributed cache)

![distributed-cache](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/caching/distributed-cache.png)

분산 캐시는 네트워킹된 여러 컴퓨터의 임의 접근 메모리(Random Access Memory, RAM) 풀을 하나의 인메모리 데이터 저장소로 만들어 데이터에 빠르게 접근하는 캐시처럼 사용하는 것입니다. 대부분의 전통적인 캐시가 물리서버 한 대 또는 하드웨어 컴포너넌트 하나였다면 분산 캐시는 여러 컴퓨터들을 연결하기 때문에 한 대의 컴퓨터의 메모리 제한을 훌쩍 뛰어넘을 수 있습니다.

## 전역 캐시(Global cache)

![global-cache](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/caching/global-cache.png)

전역 캐시는 이름에서 알 수 있듯이 모든 어플리케이션 노드들이 사용하는 하나의 공유된 캐시입니다. 캐시에서 데이터를 찾을 수 없는 경우 전역 캐시가 하위 데이터 저장소에서 데이터를 가져와야 합니다.

## 사용 예

실제로 캐시는 아래와 같은 많은 사용 예가 있습니다:

- 데이터베이스 캐싱
- 콘텐츠 배포 네트워크, (Content Delivery Network, CDN)
- 도메인 이름 시스템(DNS) 캐싱
- API 캐싱

**캐시를 사용하면 안 되는 경우**

캐시를 사용해서는 안 되는 경우도 살펴보겠습니다:

- 캐시 접근 속도가 1차 데이터 저장소에 접근하는 속도와 크게 차이나지 않을 때 별로 도움이 되지 않습니다.
- 캐시는 동일한 데이터에 반복되는 메모리 접근에서 오는 성능 향상에서 오기 때문에 반복되지 않는 요청이 많은 경우(더 랜덤한 데이터) 잘 동작하지 않습니다.
- 데이터가 자주 변경되는 경우 캐시된 데이터 동기도 같이 깨지기 때문에 그에 따른 1차 저장소에도 매번 접근하게 되어 캐시가 크게 도움이 되지 않습니다.

_특히 캐시를 영구 데이터 저장소로 사용해서는 안됩니다. 캐시는 빠르기 때문에 휘발성으로 구현되고, 일시적인 것으로 간주해야 합니다._

## 장점

캐싱의 이점은 아래와 같습니다:

- 성능 향상
- 지연 감소
- 데이터베이스 부하 감소
- 네트워크 비용 감소
- 읽기 처리량 증가

## 예제들

캐싱에 사용되는 일반적인 솔루션은 아래와 같습니다:

- [Redis](https://redis.io)
- [Memcached](https://memcached.org)
- [Amazon Elasticache](https://aws.amazon.com/elasticache)
- [Aerospike](https://aerospike.com)

# 콘텐츠 배포 네트워크(CDN)

콘텐츠 배포 네트워크(Content Delivery Network, CDN)은 인터넷 콘텐츠를 빠르게 제공하기 위해 지리적으로 분산된 서버들의 그룹을 말합니다. 일반적으로 HTML/CSS/JS, 사진, 비디오와 같은 정적인 파일들을 CDN으로 제공합니다.

![cdn-map](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/content-delivery-network/cdn-map.png)

## CDN을 사용하는 이유

콘텐츠 배포 네트워크(CDN)은 가용성과 반복성을 증가시키면서도 대역폭을 줄이고 보안을 개선하는 효과가 있습니다. 유저 입장에서 가까운 CDN에서 콘텐츠를 제공하면 극적으로 성능이 향상되며 CDN이 제공하는 콘텐츠에 대해 서버는 관련된 요청을 받지 않게 됩니다.

## CDN이 동작하는 방식

![cdn](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/content-delivery-network/cdn.png)

CDN에서 오리진 서버(origin server)가 콘텐츠 원본을 들고 있고 수많은 엣지 서버(edge server)가 전 세계 곳곳에 다양하게 분산되어 있습니다.

웹사이트 서버와 방문자의 거리를 줄이기 위해 CDN은 캐시된 콘텐츠를 엣지 위치라고도 하는 여러 지리적인 위치들에 저장합니다. 엣지 위치들 각각에는 캐시 서버들이 있어 근처에 있는 방문자들에게 콘텐츠를 제공합니다.

일단 정적 애셋들이 어떤 위치의 CDN에 캐시되면 이후 모든 웹사이트 방문자들이 요청하는 정적 애셋들은 오리진 서버가 아닌 엣지 서버에서 콘텐츠를 받아가기 때문에 오리진 서버의 부하가 줄고 확장성이 증가됩니다.

예를 들어 영국에 있는 누군가가 미국에서 호스팅되는 우리 사이트에 접근했다고 할 때 런던 엣지 위치와 같은 가장 가까운 엣지 위치에서 웹사이트가 제공될 것입니다. 방문자가 오리진 서버에 전체 요청을 보내게 된다면 지연 시간이 증가했을 것입니다.

## 종류

CDN은 일반적으로 두 가지로 분류됩니다:

### 푸시 CDN

푸시 CDN(Push CDNs)은 서버에 변경이 생긴 경우 새 콘텐츠를 수신합니다. 콘텐츠를 제공하고, CDN에 업로드하고, CDN으로 URL을 고치는 것은 우리에게 달려 있습니다. 콘텐츠가 언제 만료되고 언제 업데이트할 지도 다 정할 수 있습니다. 콘텐츠가 새것이거나 바뀌었을 때만 업로드되어 트래픽을 줄일 수 있지만 저장소 용량은 커집니다.

트래픽이 적거나 콘텐츠가 자주 바뀌지 않는 사이트의 경우 푸시 CDN이 잘 동작하지 않습니다. 콘텐츠는 CDN에 일정 주기로 다시 채워지는 것이 아니라 한번에 배치됩니다.

### 풀 CDN

풀 CDN(Pull CDNs)의 경우 요청에 의해 캐시가 업데이트됩니다. 클라이언트가 정적 애셋을 CDN에 요청하고 CDN이 해당 애셋을 가지고 있지 않은 경우 새 애셋을 오리진 서버에서 가져와 캐시에 저장합니다. 그리고 새로 캐시된 애셋을 유저에게 전달합니다.

풀 CDN은 클라이언트의 요청을 오리진 서버로 보내는 요청 기반이기 때문에 푸시 CDN과는 반대로 유지 보수할 일이 상대적으로 적습니다. 트래픽이 최근에 요청된 콘텐츠가 남아 있는 CDN에서 가장 균일하게 분산되기 때문에 높은 트래픽을 가진 사이트에 풀 CDN이 잘 동작합니다.

## 단점

모든 좋은 것이 공짜가 아니듯이, CDN의 단점에 대해서도 이야기해보겠습니다:

- **추가 비용**: 특히 높은 트래픽의 서비스에서 CDN을 사용하는 것은 비용이 많이 들 수 있습니다.
- **규제**: 일부 조직이나 국가에서 유명한 CDN의 IP 주소나 도메인을 차단하는 경우가 있습니다.
- **위치**: CDN 서버가 없는 국가에 위치한 방문자의 경우 데이터가 CDN을 쓰지 않는 것보다 더 먼 거리를 이동할 수 있습니다.

## 예제

널리 알려진 CDN들은 아래와 같습니다:

- [Amazon CloudFront](https://aws.amazon.com/cloudfront)
- [Google Cloud CDN](https://cloud.google.com/cdn)
- [Cloudflare CDN](https://www.cloudflare.com/cdn)
- [Fastly](https://www.fastly.com/products/cdn)

# 프록시

프록시 서버는 클라이언트와 백엔드 서버 사이에 위치하여 중재하는 하드웨어/소프트웨어를 칭합니다. 프록시는 클라이언트에게 요청을 받아 오리진 서버로 요청을 전달합니다. 프록시는 보통 요청을 걸러내거나, 요청 로그를 남기거나, 때때로는 헤더를 더하거나 빼거나, 암호화/복호화를 하거나, 압축하거나 하는 식으로 요청 자체를 변형시키기도 합니다.

## 종류

프록시는 두 종류로 나뉩니다:

### 포워드 프록시

포워드 프록시(Forward Proxy), 보통 프록시 또는 프록시 서버, 웹 프록시 라고 불리는 것들은 클라이언트 그룹 앞에 있습니다. 프록시 서버는 컴퓨터가 인터넷상의 웹사이트와 서비스에 요청을 보내는 경우 요청을 가로채 중간 상인처럼 클라이언트를 대신하여 요청을 처리합니다.

![forward-proxy](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/proxy/forward-proxy.png)

**장점**

포워드 프록시를 사용하는 장점은 아래와 같습니다:

- 특정 콘텐츠로의 접근을 막을 수 있다
- [지역 차단](https://en.wikipedia.org/wiki/Geo-blocking)된 콘텐츠에 접근할 수 있다
- 익명성을 제공한다
- 기타 다른 탐색 규제들을 피할 수 있다

물론 프록시가 익명성을 제공하기는 하지만 여전히 개인 정보를 추적하는 것이 가능합니다. 프록시 서버를 설치하고 운영하는 것은 대개 비용이 적지 않게 들고 설정이 필요합니다.

### 리버스 프록시

리버스 프록시(Reverse proxy)는 하나 이상의 웹 서버 앞에 있어서 클라이언트에서 오는 요청들을 가로챕니다. 클라이언트가 웹사이트의 오리진 서버에 요청을 보내면 리버스 프록시 서버가 가로채게 됩니다.

포워드 프록시와는 미묘하면서도 크게 다릅니다. 간단히 정리하자면 포워드 프록시는 클라이언트 앞에 서서 어떤 오리진 서버와도 직접 통신할 수 없게 만듭니다. 반대로 리버스 프록시는 오리진 서버들 앞에 서서 어떤 클라이언트도 오리진 서버와 직접 통신할 수 없게 합니다.

![reverse-proxy](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/proxy/reverse-proxy.png)

리버스 프록시를 도입하면 복잡도가 증가하게 됩니다. 리버스 프록시 하나만 있으면 단일 실패 지점이 되고, 여러개의 리버스 프록시를 구성하면(페일오버와 같이) 더욱 복잡도가 증가하게 됩니다.

**장점**

리버스 프록시를 사용할 경우의 장점은 아래와 같습니다:

- 보안성 향상
- 캐싱
- SSL 암호화
- 로드 밸런싱
- 확장성 및 유연성

## 로드 밸런서 vs 리버스 프록시

잠시 생각해보면 리버스 프록시는 로드 밸런서와 비슷하지 않나요? 음, 로드 밸런서는 서버가 여러 대 있을 때 유용합니다. 종종 로드 밸런서는 같은 기능을 하는 서버들에 트래픽을 라우팅하지만 리버스 프록시는 단 하나의 웹 서버나 어플리케이션 서버가 있을 때에도 유용합니다. 리버스 프록시도 로드 밸런서로 동작할 수 있지만 그 반대는 그렇지 않습니다.

## 예제

일반적으로 사용되는 프록시들은 아래와 같습니다:

- [Nginx](https://www.nginx.com)
- [HAProxy](http://www.haproxy.org)
- [Traefik](https://doc.traefik.io/traefik)
- [Envoy](https://www.envoyproxy.io)

# 가용성

가용성(Availability)은 지정된 기간동안 시스템이 필요한 기능을 수행할 수 있음을 나타내는 시간입니다. 일반적인 환경에서 시스템이나 서비스, 머신이 작동하고 있는 시간을 퍼센트로 나타낸 지표입니다.

## 9의 가용성(The Nine's of availability)

가용성은 종종 서비스가 동작하는 시간 동안 대비 업타임(또는 다운타임)의 퍼센트로 정량화됩니다. 보통 숫자 9를 이용하여 측정됩니다.

$$
Availability = \frac{Uptime}{(Uptime + Downtime)}
$$

만약 가용성이 99.00%라면 "2 nines"의 가용성이라고 합니다. 99.9% 가용성이라면 "3 nines", 그리고 그렇게 계속됩니다.

| 가용성 (%)   | 다운타임 (연간)    | 다운타임 (월간)  | 다운타임 (주간)    |
| ------------------------ | ------------------ | ----------------- | ------------------ |
| 90% (one nine)           | 36.53일         | 72시간          | 16.8시간         |
| 99% (two nines)          | 3.65일          | 7.20시간        | 1.68시간         |
| 99.9% (three nines)      | 8.77시간         | 43.8분      | 10.1분       |
| 99.99% (four nines)      | 52.6분       | 4.32분      | 1.01분       |
| 99.999% (five nines)     | 5.25분       | 25.9초      | 6.05초       |
| 99.9999% (six nines)     | 31.56초      | 2.59초      | 604.8ms |
| 99.99999% (seven nines)  | 3.15초       | 263ms  | 60.5ms  |
| 99.999999% (eight nines) | 315.6ms | 26.3ms | 6ms     |
| 99.9999999% (nine nines) | 31.6ms  | 2.6ms  | 0.6ms   |

## 연속 가용성 vs 병렬 가용성

서비스가 다운되기 쉬운 여러 컴포넌트로 구성되어 있다면 전반적인 서비스의 가용성은 컴포넌트가 연속적으로 구성되어 있는지 병렬로 구성되어 있는지에 따라 달라집니다.

### 연속

컴포넌트가 연속적인 구조인 경우 전체 가용성은 감소합니다.

$$
Availability \space (Total) = Availability \space (Foo) * Availability \space (Bar)
$$

예를 들어 `Foo`와 `Bar`가 각각 99.9%의 가용성을 가진다면 전체 연속 가용성은 99.8%가 됩니다.

### 병렬

두 컴포넌트가 병렬로 구성된 경우 전체 가용성이 증가합니다.

$$
Availability \space (Total) = 1 - (1 - Availability \space (Foo)) * (1 - Availability \space (Bar))
$$

예를 들어 `Foo`와 `Bar`가 각각 99.9%의 가용성을 가진다면 전체 병렬 가용성은 99.9999%가 됩니다.

## 가용성 vs 안정성

시스템이 안정적이라면 가용성이 있다고 보지만 가용성이 있다고 해서 반드시 안정적인 것은 아닙니다. 다시 말해, 높은 안정성이 높은 가용성에 영향을 미치지만 안정적이지 않음에도 높은 가용성인 시스템이 있을 수 있습니다.

## 고가용성 vs 내고장성

높은 가용성과(High availability) 낮은 고장성(Fault tolerance) 모두 높은 업타임의 지표로 사용됩니다. 하지만 달성하고자 하는 목적은 서로 다릅니다.

낮은 고장성을 가진 시스템은 서비스 장애가 없지만 많은 비용이 들고, 높은 가용성을 가진 시스템은 최소한의 서비스 장애를 가지고 있습니다. 낮은 고장성을 위해서는 메인 시스템이 떨어지더라도 업타임 손실 없이 다른 시스템이 넘겨받을 수 있는 수준의 전체 하드웨어 복제가 요구됩니다.

# 확장성

확장성(Scalability)은 필요시에 리소스를 추가하거나 줄였을 때 시스템이 얼마나 변화에 반응하는지에 대한 지표입니다.

![scalability](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-I/scalability/scalability.png)

확장의 종류에 대해 알아보겠습니다:

## 수직 확장

수직 확장(스케일 업이라고도 합니다)은 현재 있는 머신의 능력을 더하는 더하는 식으로 시스템의 확장성을 넓힙니다. 쉽게 말해서, 수직 확장은 하드웨어 용량을 키워 어플리케이션의 가용량을 개선하는 것입니다.

### 장점

- 구현하기 쉽다
- 관리하기 쉽다
- 데이터가 일관성 있다

### 단점

- 높은 다운타임의 위험성
- 업그레이드가 힘들다
- 단일 장애점이 될 수 있다

## 수평 확장

수평 확장(스케일 아웃이라고도 합니다)은 더 많은 머신을 추가하여 시스템의 규모를 키웁니다. 현재 있는 서버 풀에 더 많은 인스턴스를 추가하여 부하를 더욱 고르게 분산할 수 있게 만들어 성능을 개선합니다.

### 장점

- 복제성이 증가한다
- 더 나은 내고장성
- 유연하여 효율적이다
- 업그레이드가 더 쉽다

### 단점

- 복잡도가 증가한다
- 데이터가 일관적이지 않다
- 하위 서비스의 부하가 커진다

# 저장

저장(Storage)는 시스템이 데이터를 일시적으로나 영구적으로 보관하는 메커니즘입니다. 저장은 시스템 디자인에서 대충 넘어가는 주제이긴 하지만 공통적인 저장 기법에 대한 기본적인 이해가 있다면 저장 컴포넌트를 세밀하게 조절하는 데 도움이 될 것입니다. 중요한 주제 몇개를 좀 짚어보겠습니다:

## RAID

RAID (Redundant Array of Independent Disks, 독립 디스크 복제 배열)는 같은 데이터를 여러 하드 디스크나 SSD에 저장하여 드라이브가 실패하더라도 데이터를 보호할 수 있는 저장 기법입니다.

여러 가지 RAID 레벨이 있지만 모든 레벨이 중복성을 달성하기 위한 것은 아닙니다. 공통적으로 사용되는 RAID 레벨에 대해 알아보겠습니다:

- **RAID 0**: 스트리핑이라고도 하는데 데이터가 모든 드라이브에 골고루 분산됩니다.
- **RAID 1**: 미러링이라고도 하며, 적어도 두 개 이상의 드라이브가 같은 데이터셋을 복제합니다. 한 드라이브가 실패하더라도 나머지 드라이브가 잘 동작할 것입니다.
- **RAID 5**: 패리티를 사용해 스트리핑합니다. 적어도 3개 이상의 드라이브가 필요하며 RAID 0처럼 데이터를 스트리핑하면서 패리티 또한 드라이브에 분산됩니다.
- **RAID 6**: 더블 패리티를 사용해 스트리핑합니다. RAID 6은 RAID 5와 같지만 패리티 자체는 두 개의 드라이브에 쓰여집니다.
- **RAID 10**: RAID 0의 스트리핑과 RAID 1의 미러링을 혼합합니다. 모든 데이터를 2차 드라이브에 복제하면서도 각 드라이브 세트에 스트리핑을 수행해 데이터 전송 속도를 증가시킵니다.

### 비교

서로 다른 RAID 레벨들의 기능들을 비교해 보겠습니다:

| 기능             | RAID 0   | RAID 1               | RAID 5               | RAID 6                      | RAID 10                                  |
| -------------------- | -------- | -------------------- | -------------------- | --------------------------- | ---------------------------------------- |
| 설명          | 스트리핑 | 미러링 | 스트리핑 + 패리티 | 스트리핑 + 더블 패리티 | 스트리핑 + 미러링
| 최소 디스크 숫자        | 2        | 2                    | 3                    | 4                           | 4                                        |
| 읽기 성능 | 높음     | 높음                 | 높음                 | 높음                        | 높음                                     |
| 쓰기 성능 | 높음     | 중간               | 높음                 | 높음                        | 중간                                   |
| 비용                 | 낮음      | 높음                 | 낮음                  | 낮음                         | 높음                                     |
| 내고장성 | None     | 드라이브 1개 실패 | 드라이브 1개 실패 | 드라이브 2개 실패           | 각 서브 배열당 최대 디스크 1개 실패 |
| 용량 활용도 | 100%     | 50%                  | 67%-94%              | 50%-80%                     | 50%                                      |

## 볼륨

볼륨(Volume)은 디스크 또는 테이프의 고정된 용량입니다. 볼륨이란 용어 자체는 대개 저장소와 동일한 의미로 많이 쓰이지만 디스크 하나가 여러 볼륨을 가지거나 한 볼륨이 여러 디스크로 되어 있는 경우도 있습니다.

## 파일 저장소

파일 저장소는 데이터를 파일로 저장하여 최종 사용자에게 계층적 디렉토리 구조로 보여주는 솔루션을 말합니다. 파일 저장소를 사용하는 가장 큰 이점은 파일을 저장하고 받는 데 있어 사용자 친화적인 솔루션이라는 점입니다. 파일 저장소에 있는 파일 하나의 위치를 알기 위해서는 파일의 전체 경로가 필요합니다. 간단하고 경제적인 구조이며 주로 하드 드라이브에서 발견됩니다. 그 구조가 유저에게 보이는 식이나 실제 하드 드라이브에 저장되는 방식이나 동일하다는 것입니다.

예: [Amazon EFS](https://aws.amazon.com/efs), [Azure files](https://azure.microsoft.com/en-in/services/storage/files), [Google Cloud Filestore](https://cloud.google.com/filestore) 등등.

## 블록 저장소

블록 저장소는 데이터를 블록(청크)로 나뉜 조각으로 저장합니다.각 데이터 블록은 유일한 식별자가 있어 저장소 시스템이 가장 편리한 위치에 데이터 조각을 배치합니다.

또한 블록 저장소는 데이터를 유저 환경과 분리하여 여러 환경에 데이터가 분산되게끔 할 수 있습니다. 그리하여 데이터로의 여러 경로가 생길 수 있으며 유저가 빠르게 받을 수 있게 됩니다. 유저 또는 어플리케이션이 블록 저장소에서 데이터를 요청하면 하위 저장소 시스템이 데이터 블록을 재조합하여 유저나 어플리케이션에 데이터를 보여주게 됩니다.

예: [Amazon EBS](https://aws.amazon.com/ebs).

## 오브젝트 저장소

오브젝트 기반 저장소라고도 하는 오브젝트 저장소는 오브젝트라고 하는 조각으로 데이터를 쪼갭니다. 이 오브젝트들을 저장소에 저장하여 여러 네트워크 시스템에 퍼져나갈 수 있게 합니다.

예: [Amazon S3](https://aws.amazon.com/s3), [Azure Blob Storage](https://azure.microsoft.com/en-in/services/storage/blobs), [Google Cloud Storage](https://cloud.google.com/storage) 등등.

## NAS

NAS(Network Attached Storage)는 인증된 네트워크 유저들이 데이터를 저장하고 받아올 수 있는 네트워크에 연결된 중앙 저장소입니다. NAS 기기는 저장소가 더 필요할 때 가지고 있는 만큼 더 추가할 수 있기 때문에 유연합니다. 또한 더 빠르고, 상대적으로 저렴하고, 공개 클라우드의 이점들을 설치한 곳에서 누릴 수 있으며 우리 손으로 완전히 제어 가능합니다.

## HDFS

하둡 분산 파일 시스템(Hadoop Distributed File System, HDFS)는 보급형 하드웨어에서 돌아가도록 설계된 분산 파일 시스템입니다. HDFS는 내고장성이 매우 높고 저가 하드웨어에 배포되도록 설계되었습니다. HDFS는 어플리케이션 데이터에 높은 처리량을 제공하고 거대한 데이터셋을 가진 어플리케이션에 적합합니다. 또한 기존 분산 파일 시스템과 많은 공통점을 가집니다.

HDFS는 거대한 클러스터에서 머신들 간에 매우 큰 파일들을 안정적으로 저장하기 위해 고안되었습니다. HDFS는 각 파일들을 순차적인 블록들로 저장하고 파일의 모든 블록은 마지막 블록만 빼고 전부 같은 크기를 갖비니다. 파일 블록들은 내고장성을 위해 복제됩니다.

# 데이터베이스와 DBMS

## 데이터베이스란?

데이터베이스는 대개 컴퓨터 시스템에서 전기적으로 저장되는 구조화된 정보나 데이터들의 조직된 묶음입니다. 데이터베이스는 보통 데이터베이스 관리 시스템(Database Management System, DBMS)이 제어합니다. 데이터와 DBMS, 이와 연관된 어플리케이션 모두를 통틀어 데이터베이스 시스템이라고 부르며 때로는 그냥 데이터베이스라고 부르기도 합니다.

## DBMS란?

데이터베이스에는 보통 데이터베이스 관리 시스템(DBMS)이라고도 하는 통합 데이터베이스 프로그램이 필요합니다. DBMS는 데이터베이스와 유저/프로그램 사이의 인터페이스로 동작하여 유저들이 데이터를 받거나 갱신하고 정보가 조직되거나 최적화되는 방식을 정할 수 있게 합니다. 또한 DBMS는 데이터베이스를 감시하고 제어하는 기능이 있어서 성능 모니터링이나 튜닝, 백업, 복구와 같은 관리 작업들을 할 수 있습니다.

## 요소

데이터베이스들 간에 발견되는 공통 요소들은 다음과 같습니다:

### 스키마

스키마(Schema)의 역할은 데이터 구조의 모양을 정의하고 어떤 종류의 데이터를 보낼 수 있는지 정합니다. 스키마는 전체 데이터베이스에 엄격하게 강제할 수도 있고 일부분에 약하게 강제할 수도 있고, 아예 없을 수도 있습니다.

### 테이블

각 테이블(Table)은 스프레드시트와 같이 다양한 컬럼들을 가지고 있습니다. 어떤 종류의 정보를 넣느냐에 따라 두 개의 컬럼만 가진 빈약한 테이블에서부터 100개 이상의 컬럼을 가지게 만들 수도 있습니다.

### 컬럼

컬럼(Column)은 특정 타입의 데이터 값들을 담는 곳이며 로우 하나마다 하나의 값을 가집니다. 컬럼에는 텍스트나 숫자, 열거형, 타임스탬프 등의 값들이 저장될 수 있습니다.

### 로우

테이블의 데이터는 로우(Row) 단위로 기록됩니다. 테이블에는 정보를 가지는 수천 또는 수백만 개의 로우가 있을 수 있습니다.

## 유형

![database-types](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/databases-and-dbms/database-types.png)

데이터베이스의 유형은 아래와 같습니다:

- **[SQL](https://karanpratapsingh.com/courses/system-design/sql-databases)**
- **[NoSQL](https://karanpratapsingh.com/courses/system-design/nosql-databases)**
  - Document
  - Key-value
  - Graph
  - Timeseries
  - Wide column
  - Multi-model

SQL과 NoSQL 데이터베이스는 넓은 주제이기 때문에 [SQL 데이터베이스](#sql-데이터베이스)와 [NoSQL 데이터베이스](#nosql-데이터베이스)에서 따로 이야기하겠습니다. 이 둘이 어떻게 다른지는 [SQL vs NoSQL 데이터베이스](#sql-vs-nosql-데이터베이스)에서 비교해보겠습니다.

## 도전

데이터베이스를 큰 규모로 운영할 때 생기는 도전들은 보통 이렇습니다:

- **급격히 증가하는 데이터 크기**: 센서나 연결된 머신들이나 그 외 수많은 곳에서 오는 폭발적인 데이터들이 있습니다.
- **데이터 보안**: 오늘날 데이터 침입은 어디에서나 일어납니다. 데이터가 안전히 보관되면서 유저들에게는 쉽게 접근 가능할 수 있게 만드는 것이 어느 때보다 중요합니다.
- **필요할 때 접근 가능해야 함**: 회사에서의 재빠른 의사결정을 지원하여 새로운 기회들에서 이점을 얻기 위해 실시간으로 데이터에 접근할 수 있어야 합니다.
- **데이터베이스와 인프라를 유지보수하기**: 데이터베이스가 점점 복잡해지고 데이터량이 늘어남에 따라 회사는 데이터베이스를 관리하는 인재를 추가로 채용하기 위한 비용을 들여야 합니다.
- **확장성으로 한계를 제거하기**: 사업이 살아남기 위해 성장이 필요하다면 데이터 관리 체계 역시 그에 맞추어 성장해야 합니다. 하지만 회사가 얼마나 많은 용량을 필요로 할 지, 특히 온프레미스 데이터베이스인 경우 그 용량에 대한 추정은 매우 어렵습니다.
- **데이터 현지화, 데이터 주권, 지연 요구사항을 만족하기**: 일부 조직은 온프레미스로 운영하기에 최적인 사용 사례를 가지고 있습니다. 그런 경우에 데이터베이스 운영에 대비해 미리 설정되고 최적화된 시스템이 가장 이상적입니다.

# SQL 데이터베이스

SQL(또는 관계형) 데이터베이스는 미리 정의된 관계를 가진 데이터들의 집합입니다. 이 데이터들은 컬럼들과 로우들로 구성된 일련의 테이블로 조직되어 있습니다. 테이블은 개체들이 어떻게 데이터베이스에 나타날지에 대한 정보를 담는 데 사용됩니다. 테이블의 각 컬럼들은 특정 종류의 데이터를 가지며 필드는 속성의 실제 값들을 저장합니다. 테이블의 로우는 개체 또는 엔티티 하나와 관련된 값들의 컬렉션입니다.

테이블의 각 로우는 기본키(primary key)라고 하는 유일한 식별자로 표시할 수 있고 여러 테이블 간의 로우들은 외래키(foreign key)로 관계를 맺을 수 있습니다. 이런 데이터는 데이터베이스 테이블 자체를 재조직하지 않더라도 다양한 방법으로 가져올 수 있습니다. 대개 SQL 데이터베이스는 [ACID 일관성 모델](#acid와-base-일관성-모델)을 따릅니다.

## 구체화 뷰

구체화 뷰(materialized view)는 쿼리 명세에서 파생된 미리 계산되고 나중 사용을 결과를 저장하는 위해 데이터셋입니다. 데이터가 미리 계산되기 때문에 구체화 뷰에 쿼리를 하는 것이 원래 테이블에 같은 쿼리를 하는 것보다 빠릅니다. 특히 쿼리가 자주 실행되거나 충분히 복잡한 경우 성능 차이가 극명하게 보입니다.

또한 데이터 서브세팅을(data subsetting) 하게 만들어 네트워크 로드가 줄어들기 때문에 거대한 데이터셋 위에서 실행하는 복잡한 쿼리들에 대한 성능도 향상시킵니다. 구체화 뷰의 다른 사용례도 있지만 대부분 성능과 복제를 위해 사용됩니다.

## N+1 쿼리 문제

N+1 쿼리 문제는 데이터 접근 계층에서 1차 SQL 쿼리로도 가져올 수 있는 동일한 데이터를 가져오는 N개의 추가 SQL 질의를 실행하는 경우 발생합니다. N이 커질수록 더 많은 쿼리들이 실행되어 성능에 더 많은 영향을 미치게 될 것입니다.

이것은 GraphQL과 ORM(객체-관계 매핑) 툴에서 흔히 보이는 문제이며 SQL 쿼리를 최적화하거나 일련의 데이터 요청을 하나씩 요청하는 식으로 일괄 처리하는 데이터 로더를 만들어 해결할 수 있습니다.

## 장점

관계형 데이터베이스의 장점을 알아봅시다:

- 간단하고 정확하다
- 접근성이 좋다
- 데이터 일관성
- 유연성

## 단점

관계형 데이터베이스의 단점입니다:

- 관리하는 비용이 많이 든다
- 스키마 변경이 어렵다
- 성능 (조인, 비정규화 등)
- 수평 확장성이 매우 좋지 않아 확장이 어렵다

## 예제들

자주 사용되는 관계형 데이터베이스들은 아래와 같습니다:

- [PostgreSQL](https://www.postgresql.org)
- [MySQL](https://www.mysql.com)
- [MariaDB](https://mariadb.org)
- [Amazon Aurora](https://aws.amazon.com/rds/aurora)

# NoSQL 데이터베이스

NoSQL은 SQL을 기본 데이터 접근 언어로 사용하지 않는 모든 데이터베이스를 지칭하는 넓은 범위의 분류입니다. 또한 이런 종류의 데이터베이스는 종종 비관계형 데이터베이스라고 부르기도 합니다. NoSQL 데이터베이스는 관계형 데이터베이스와는 달리 미리 정의한 스키마를 준수할 필요가 없습니다. NoSQL 데이터베이스는 [BASE 일관성 모델](#acid와-base-일관성-이론)을 따릅니다.

NoSQL의 종류들은 아래와 같습니다:

### 도큐먼트

도큐먼트 데이터베이스(도큐먼트 지향 데이터베이스 또는 도큐먼트 스토어라고도 합니다)는 도큐먼트라는 것에 정보를 저장합니다. 트랜잭션이나 분석 어플리케이션과 같이 다양한 사용례에 사용되는 범용 데이터베이스입니다.

**장점**

- 직관적이고 유연하다
- 수평 확장이 쉽다
- 스키마 없음

**단점**

- 스키마 없음
- 관계형이 아니다

**예**

- [MongoDB](https://www.mongodb.com)
- [Amazon DocumentDB](https://aws.amazon.com/documentdb)
- [CouchDB](https://couchdb.apache.org)

### 키-밸류

가장 단순한 형태의 NoSQL 데이터베이스 중 하나인 키-밸류 데이터베이스는 각각의 데이터를 키-밸류 두 개의 데이터 쌍으로 표현한 것들의 집합입니다. 또한 키-밸류 저장소라고도 부릅니다.

**장점**

- 간결하고 성능이 좋다
- 높은 양의 트래픽에도 확장성이 좋다
- 세션 관리
- 최적화된 룩업

**단점**

- 기본 CRUD
- 값들을 필터링할 수 없음
- 인덱싱과 스캔 기능이 없음
- 복잡한 쿼리에 최적화되지 않음

**예**

- [Redis](https://redis.io)
- [Memcached](https://memcached.org)
- [Amazon DynamoDB](https://aws.amazon.com/dynamodb)
- [Aerospike](https://aerospike.com)

### 그래프

그래프 데이터베이스는 테이블이나 도큐먼트를 사용하여 데이터를 표현하거나 저장하는 대신 노드나 엣지, 프로퍼티들을 사용한 그래프 구조로 의미 쿼리를 하는 NoSQL 데이터베이스를 말합니다.

그래프가 저장소의 데이터들을 노드와 엣지의 모음으로 연관짓는데, 엣지가 노드들 간의 관계를 나타냅니다. 이 관계들이 저장된 데이터를 직접 연결된 것처럼 만들며 한 번의 작업으로 받아올 수 있게 됩니다.

**장점**

- 쿼리 속도
- 변화에 잘 적응하며(agile) 유연하다
- 명시적 데이터 표현

**단점**

- 복잡하다
- 표준화된 쿼리 언어가 없다

**사용예**

- 허위 탐지
- 추천 엔진
- 소셜 네트워크
- 네트워크 매핑

**예**

- [Neo4j](https://neo4j.com)
- [ArangoDB](https://www.arangodb.com)
- [Amazon Neptune](https://aws.amazon.com/neptune)
- [JanusGraph](https://janusgraph.org)

### 시계열

시계열(time-series) 데이터베이스는 타임스탬프가 찍히거나 시간 순서대로 저장되는 데이터에 최적화된 데이터베이스입니다.

**장점**

- 삽입과 조회가 빠르다
- 효율적인 데이터 저장

**사용예**

- IoT 데이터
- 지표 분석
- 어플리케이션 모니터링
- 금융 트렌드 이해

**예**

- [InfluxDB](https://www.influxdata.com)
- [Apache Druid](https://druid.apache.org)

### 와이드 컬럼

컬럼 저장소라고도 하는 와이드 컬럼(wide column) 데이터베이스는 스키마에 구애받지 않습니다. 데이터는 로우나 컬럼 단위가 아니나 컬럼 패밀리라고 하는 단위로 저장됩니다.

**장점**

- 확장성이 매우 좋아 페타바이트급의 데이터도 다룰 수 있다
- 현실의 빅데이터 어플리케이션에 최적이다

**단점**

- 비용이 비싸다
- 쓰기 시간이 증가

**사용예**

- 사업 분석
- 속성(attribute) 기반 데이터 저장소

**예**

- [BigTable](https://cloud.google.com/bigtable)
- [Apache Cassandra](https://cassandra.apache.org)
- [ScyllaDB](https://www.scylladb.com)

### 다중 모델

다중 모델(multi-model) 데이터베이스는 다른 데이터베이스 모델들(관계형, 그래프, 키-밸류, 도큐먼트 등)을 혼합하여 하나의 통합된 백엔드로 만든 데이터베이스입니다. 하나의 모델을 사용한 것보다 더 다양한 데이터 타입과 인덱스, 쿼리, 저장이 가능합니다.

**장점**

- 유연성
- 복잡한 프로젝트에 적합
- 데이터 일관성

**단점**

- 복잡하다
- 덜 성숙하다

**예**

- [ArangoDB](https://www.arangodb.com)
- [Azure Cosmos DB](https://azure.microsoft.com/en-in/services/cosmos-db)
- [Couchbase](https://www.couchbase.com)

# SQL vs NoSQL 데이터베이스

데이터베이스의 세계에서는 SQL(관계형)과 NoSQL(비관계형)이라는 두 종류의 솔루션이 있습니다. 만들어지는 방식이나 저장되는 데이터의 종류나 저장되는 방식 모두 다릅니다. 관계형 데이터베이스는 미리 정의된 스키마가 있고 구조화된 반면 비관계형 데이터베이스는 구조화되지 않고 분산되며 동적인 스키마를 사용합니다.

## 높은 수준에서의 차이

SQL과 NoSQL을 높은 관점에서 비교하면 이렇게 다릅니다:

### 저장소

SQL은 엔티티를 나타내는 로우와 엔티티의 데이터 포인트를 나타내는 컬럼을 사용하는 테이블로 데이터를 저장합니다.

NoSQL 데이터베이스는 키-밸류나 그래프, 도큐먼트와 같은 다른 데이터 저장 모델을 가지고 있습니다.

### 스키마

SQL에서 각 레코드는 고정된 스키마에 맞춰져야 하는데, 다시 말해 데이터가 들어오기 전에 컬럼들이 미리 선택 및 결정되어야 하며 각 로우는 각 컬럼에 해당하는 데이터를 가져야 합니다. 스키마는 나중에 변경할 수 있지만 여기에는 마이그레이션을 이용하여 데이터베이스를 수정하는 일이 포함됩니다.

스키마는 NoSQL의 경우 유동적입니다. 운영중에 필드가 얼마든지 바뀔 수 있고 각 _레코드_(또는 동일한 레코드)는 각 _필드_ 에 데이터를 가지고 있어야 할 필요가 없습니다.

### 쿼리

SQL 데이터베이스는 SQL(Structed Query Language)이라는 강력한 도구를 사용하여 데이터를 정의하거나 수정합니다.

NoSQL 데이터베이스에서는 쿼리가 도큐먼트 콜렉션에 초점이 맞춰져 있습니다. 데이터베이스들이 쿼리를 위한 문법이 각각 다릅니다.

### 확장성

대부분의 경우 SQL 데이터베이스는 비용이 많이 드는 방식인 수직 확장이 됩니다. 관계형 데이터베이스를 여러 서버들로 확장하는 것도 가능하지만 대개 도전적이고 시간이 많이 드는 작업입니다.

반대로 NoSQL 데이터베이스는 수평 확장이 가능해서 많은 트래픽을 다루기 위해 NoSQL 데이터베이스 인프라에 더 많은 서버를 쉽게 추가할 수 있습니다. 저렴한 범용 하드웨어나 클라우드 인스턴스 어느 것이든지 NoSQL 데이터베이스를 호스팅할 수 있기 때문에 수직 확장보다 훨씬 비용 효율적인 확장입니다. 또한 많은 NoSQL 기술들이 자동으로 데이터를 서버들에 분산합니다.

### 안정성

관계형 데이터베이스의 절대 다수는 ACID를 준수합니다. 그렇기 때문에 데이터 안정성과 트랜잭션 안전 보증에 있어서 SQL 데이터베이스가 여전히 가장 좋은 선택입니다.

대부분의 NoSQL 솔루션들은 성능과 확장성을 위해 ACID 준수를 희생합니다.

## 이유

언제나처럼 우리는 항상 요구사항에 가장 잘 맞는 기술을 선택해야 합니다. 그렇기 때문에 SQL이나 NoSQL을 선택하게 되는 이유들을 알아보도록 합시다:

**SQL을 사용해야 하는 경우**

- 엄격한 스키마가 있는 구조화된 데이터
- 관계형 데이터
- 복잡한 조인이 필요한 경우
- 트랜잭션
- 인덱스를 통한 룩업이 매우 빠름

**NoSQL을 사용해야 하는 경우**

- 유동적이거나 유연한 스키마
- 비관계적인 데이터
- 복잡한 조인이 필요없는 경우
- 데이터 집중적인 워크로드
- IOPS 대비 높은 처리량

# 데이터베이스 복제

데이터베이스 복제는 다중 데이터베이스와 같은 리소스 복제를 통해 정보를 공유하여 일관성을 유지하고, 안정성과 내고장성, 가용성을 향상시키는 과정입니다.

## 마스터-슬레이브 복제

마스터가 읽기와 쓰기를 서비스하며 쓰기 작업을 슬레이브에 복제합니다. 슬레이브는 읽기 작업만 서비스합니다. 또한 슬레이브는 트리 구조와 비슷하게 다른 슬레이브를 복제할 수 있습니다. 시스템은 마스터가 오프라인이 되는 경우 슬레이브 하나가 마스터로 승격하거나 새로운 마스터가 실행될 때까지 읽기 전용 모드로 동작할 수 있습니다.

![master-slave-replication](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/database-replication/master-slave-replication.png)

### 장점

- 전체 데이터베이스 백업이 상대적으로 마스터에 영향을 덜 미친다
- 어플리케이션이 마스터에 영향을 주지 않고 슬레이브에서 읽어올 수 있다
- 다운타임 없이도 슬레이브가 오프라인이 되었다가 나중에 마스터와 동기화될 수 있다

### 단점

- 복제 과정이 추가적인 하드웨어와 복잡도를 필요로 한다
- 마스터가 내려간 경우 다운타임 및 데이터 유실 가능성이 있다
- 마스터-슬레이브 구조에서는 모든 쓰기 작업이 마스터에 일어나야 한다
- 슬레이브 읽기가 많을 수록 더 복제가 많아야 하며 복제할 것들이 많아지기 때문에 복제 렉을 유발하게 됩니다

## 마스터-마스터 복제

두 대의 마스터가 읽기와 쓰기를 서비스하며 서로 협력하는 방식입니다. 둘 중 하나의 마스터가 내려가더라도 읽기와 쓰기 둘 다 사용 가능합니다.

![master-master-replication](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/database-replication/master-master-replication.png)

### 장점

- 어플리케이션이 두 마스터 모두에서 읽을 수 있다
- 쓰기 부하가 두 마스터 노드로 분산된다
- 간단하고, 자동으로 동작하며 빠른 페일오버가 된다

### 단점

- 설정 및 배포가 마스터-슬레이브만큼 간단하지 않다
- 일관성이 약하거나 동기화 때문에 쓰기 지연이 늘어날 수 있다.
- 더 많은 쓰기가 발생하면 지연 시간이 더욱 늘어나기 때문에 충돌 해결이 중요한 문제가 된다

## 동기 vs 비동기 복제

동기 복제와 비동기 복제 간의 기본적인 차이는 바로 레플리카에 데이터가 어떻게 기록되느냐 하는 것입니다. 동기 복제를 사용하면 데이터가 기본 저장소와 레플리카 동시에 기록됩니다. 그런 식으로 원본과 레플리카가 언제나 동기화되어 있게 됩니다.

반대로 비동기 복제는 기본 저장소에 데이터가 기록된 다음에 레플리카에 기록하는 방식입니다. 복제가 완전히 실시간은 아니지만 스케쥴 기반으로 복제가 동작하며 좀 더 비용 효율적입니다.

# 색인

데이터베이스에서 색인(Indexes)은 매우 잘 알려진 주제입니다. 데이터 저장소에서 데이터를 꺼내올 때 색인을 사용하여 속도를 향상시킵니다. 색인은 빠른 읽기를 제공하는 대신 (데이터뿐만 아니라 색인도 업데이트해야 하기 때문에) 저장소 오버헤드를 늘리고 쓰기 작업을 느리게 만드는 트레이드 오프가 있습니다. 색인 덕분에 데이터베이스 테이블의 모든 로우를 뒤져보지 않아도 빠르게 찾는 데이터의 위치를 알 수 있습니다. 색인은 데이터베이스 테이블에서 한 개 이상의 컬럼을 지정하여 생성할 수 있으며 빠른 임의 접근과 정렬된 레코드에 대한 접근의 기초가 됩니다.

![indexes](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/indexes/indexes.png)

색인은 실제 데이터가 어디 있는지 가리키는 내용의 테이블로 볼 수 있는 데이터 구조입니다. 테이블 컬럼들로 인덱스를 구성할 때 해당 컬럼과 전체 로우에 대한 포인터를 저장합니다. 같은 데이터에 대한 다른 뷰를 만들 때에도 색인이 사용됩니다. 데이터셋이 거대한 경우 색인은 재정렬이나 여러 카피를 만들지 않고도 다른 필터나 정렬을 할 수 있는 훌륭한 방법입니다.

데이터베이스 색인은 **촘촘하거나(dense)** **듬성듬성하거나(sparse)** 둘 중에 하나의 품질을 가질 수 있습니다. 각 품질은 각기 장단점이 있습니다. 인덱스 종류들이 각기 어떻게 동작하는지 알아보겠습니다:

## 촘촘한 색인

촘촘한 색인(Dense Index)은 테이블의 모든 로우로 색인을 만듭니다. 색인의 모든 레코드가 검색 키값과 실제 레코드의 포인터를 가지고 있기 때문에 바로 레코드의 위치를 확인할 수 있습니다.

![dense-index](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/indexes/dense-index.png)

촘촘한 색인은 쓰기 시간에 듬성듬성한 색인보다 더 많은 유지보수를 필요로 합니다. 모든 로우에 대한 접근이 되어야 하기 때문에 데이터베이스가 삽입, 갱신, 삭제 시에 색인을 관리해야 합니다. 또한 모든 로우에 대한 접근이 된다는 것은 촘촘한 색인이 더 많은 메모리를 요구한다는 의미이기도 합니다. 촘촘한 색인의 장점은 이진 탐색만으로도 값을 빠르게 찾을 수 있다는 점입니다. 촘촘한 색인에서는 데이터가 정렬되어야 할 필요는 없습니다.

## 듬성듬성한 색인

듬성듬성한 색인에서는 일부 레코드들에 대해서만 색인 레코드가 생성됩니다.

![sparse-index](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/indexes/sparse-index.png)

듬성듬성한 색인은 값들의 일부분만 담고 있기 때문에 촘촘한 색인보다 쓰기 시간에 적은 관리만이 필요합니다. 관리 부담이 가볍기 때문에 삽입, 갱신, 삭제가 빠릅니다. 또한 적은 수의 레코드는 색인이 더 적은 메모리를 사용한다는 뜻이기도 합니다. 데이터 탐색은 대개 이진 탐색 이후에 페이지들 사이에 스캔을 하기 때문에 더 느린 편입니다. 듬성듬성한 색인은 정렬된 데이터에서는 선택적으로 사용됩니다.

# 정규화와 비정규화

## 용어

정규화와 비정규화에서 공통적으로 사용되는 용어들을 먼저 알아보겠습니다:

### 키

**기본키(Primary Key)**: 테이블의 각 로우를 유일하게 식별할 수 있는 컬럼 또는 컬럼 그룹.

**합성키(Composite Key)**: 여러 컬럼들로 만들어진 기본키.

**슈퍼키(Super Key)**: 테이블에 있는 모든 로우들을 각자 식별할 수 있는 전체 키들의 집합.

**후보키(Candidate Key)**: 테이블에서 로우를 유일하게 식별할 수 있는 속성들.

**외래키(Foreign Key)**: 다른 테이블의 기본키에 대한 참조입니다.

**대체키(Alternate Key)**: 기본키가 아닌 키들을 대체키라고 합니다.

**대리키(Surrogate Key)**: 다른 컬럼들로 기본키를 만들 수 없는 경우에도 테이블의 엔트리들을 구분할 수 있도록 시스템에서 생성한 값.

### 종속성 

**부분 종속성(Partial Dependency)**: 기본키로 일부 속성을 정의할 때 발생합니다.

**함수적 종속성(Functional Dependency)**: 테이블에서 대개 기본키와 키가 아닌 속성 간에 관계가 있는 경우.

**이행적 함수 종속성(Transitive functional dependency)**: 키가 아닌 속성으로 다른 일부 속성을 표현할 때 발생합니다.

### 이상

데이터베이스 이상(Anomaly)은 플래닝을 잘못 하거나 모든 정보를 평평한 데이터베이스에 저장하는 등으로 데이터베이스에 결함이 있는 경우 발생합니다. 일반적으로 정규화로 해결 가능합니다.

세 가지 데이터베이스 이상이 있습니다.

**삽입 이상**: 다른 속성 없이 특정 속성들을 데이터베이스에 삽입할 수 없는 경우

**갱신 이상**: 데이터 복제를 하거나 부분 갱신을 할 때 발생합니다. 다시 말해, 정상적인 데이터베이스 갱신을 하기 위해 추가나 삭제와 같은 다른 작업이 필요한 경우입니다.

**삭제 이상**: 어떤 데이터 삭제를 하기 위해서 다른 데이터 삭제가 선행되어야 하는 경우

**예**

아래에 정규화되지 않은 테이블이 있습니다:

| ID  | Name   | Role              | Team |
| --- | ------ | ----------------- | ---- |
| 1   | Peter  | Software Engineer | A    |
| 2   | Brian  | DevOps Engineer   | B    |
| 3   | Hailey | Product Manager   | C    |
| 4   | Hailey | Product Manager   | C    |
| 5   | Steve  | Frontend Engineer | D    |

새로 "John"이라는 사람을 채용했지만 팀에 바로 할당하지는 않은 경우를 생각해 보겠습니다. 이 경우 팀 속성이 나타나지 않기 때문에 _삽입 이상_ 입니다.

그 다음으로, C팀의 Hailey가 승진한 경우 이 변화를 반영하기 위해서는 두 개의 로우를 갱신해야 일관성이 유지되는데, 이를 _갱신 이상_ 이라고 합니다.

마지막으로 B팀을 없애고 싶은 경우 B팀의 Role이나 Name 같은 것에 대한 정보도 없어져야 합니다. 이는 _삭제 이상_ 의 좋은 예입니다.

## 정규화

정규화(Normalization)는 데이터베이스에 있는 데이터를 조직하는 과정입니다. 테이블을 생성하고 규칙에 따라 테이블들 간에 관계를 맺어 데이터를 보호하고, 중복과 일관적이지 않은 종속성들을 제거하여 데이터베이스가 더 유연하게 되도록 합니다.

### 정규화가 필요한 이유

정규화의 목적은 데이터 중복을 없애 데이터를 일관적으로 만드는 것입니다. 완전히 정규화된 데이터베이스는 새로운 종류의 데이터가 추가되어도 기존 구조를 과도하게 고치지 않게 합니다. 그 결과 데이터베이스와 상호작용하는 어플리케이션 역시 최소한의 영향만 받게 됩니다.

### 기본형

기본형은 데이터베이스가 정규화되었는지 확인하는 일련의 가이드라인들입니다. 필수 기본형들에 대해 알아보겠습니다:

**1차 정규화**

1차 정규화(1NF)된 테이블은 아래 규칙을 따라야 합니다:

- 그룹 반복이 허용되지 않음
- 관련된 데이터 세트를 기본키로 식별한다
- 관련된 데이터 세트들은 별도의 테이블이 있어야 한다
- 동일 컬럼에 데이터 유형들을 섞는 것은 허용되지 않음

**2차 정규화**

2차 정규화(2NF)된 테이블은 아래 규칙을 따라야 합니다:

- 1NF를 만족한다
- 부분 종속성이 없어야 한다

**3차 정규화**

3차 정규화(3NF)된 테이블은 아래 규칙을 따라야 합니다:

- 2NF를 만족한다
- 이행적 함수 종속성(Transitive functional dependency)를 허용하지 않는다

**BCNF**

Boyce-Codd 정규화(BCNF라고도 합니다)는 3차 정규화(3NF)을 약간 강화한 버전입니다. BCNF는 3NF에서 정의하지 않았던 특정 이상들을 해결합니다. 때로 3.5차 정규화(3.5NF)로 알려져 있기 도 합니다.

BCNF를 만족하는 테이블은 아래 규칙을 따라야 합니다:

- 3NF를 만족한다
- 모든 X → Y로의 함수적 종속성에서 X가 슈퍼키여야 한다

_4NF나 5NF, 6NF와 같은 정규화 유형도 있지만 여기서 다루지 않겠습니다. [이 엄청난 영상](https://www.youtube.com/watch?v=GFQaEYEc8_8)에서 더 자세히 설명하니 참고하시면 됩니다._

관계형 데이터베이스에서 3차 정규화 조건을 만족하는 경우 종종 관계가 _"정규화되었다"_ 라고 합니다. 대부분의 3차 정규화 관계는 삽입, 갱신, 삭제가 자유롭습니다.

많은 규칙과 명세가 있더라도 실전에서는 완벽하게 따라할 수 없습니다. 정규화의 처음 세 규칙들을 따르지 않기로 한다면 여러분의 어플리케이션에서 중복 데이터나 일관적이지 않은 종속성과 같은 문제들이 발생할 것임을 감안해야 합니다.

### 장점

정규화에서 오는 장점들은 이렇습니다:

- 데이터 중복을 줄인다
- 더 좋은 데이터 설계를 하게 된다
- 데이터 일관성이 증가한다
- 참조 무결성을 달성하게 만든다

### 단점

정규화의 단점도 알아보겠습니다:

- 복잡한 데이터 설계
- 느린 성능
- 유지보수 오버헤드
- 더 많은 조인이 필요하다

## 비정규화

비정규화는 하나 이상의 테이블에 중복 데이터를 넣는 데이터베이스 최적화 기법입니다. 관계형 데이터베이스에서 비용이 많이 드는 조인을 피할 수 있게 합니다. 비정규화는 약간의 쓰기 비용을 들여 읽기 성능을 향상시킵니다. 여러 테이블에 중복 데이터를 카피를 저장하면 비싼 조인 연산을 하지 않아도 됩니다.

조인을 관리하는 입장에서 연합 또는 샤딩과 같은 기법으로 데이터를 분산하는 것이 복잡도를 증가시킵니다. 비정규화는 그런 복잡한 조인에 대한 필요를 우회합니다.

_참고: 비정규화는 정규화를 반대로 한다는 의미가 아닙니다._

### 장점

비정규화의 장점은 이렇습니다:

- 데이터 수신이 더 빠르다
- 쿼리 작성이 쉬워진다
- 테이블 숫자가 줄어든다
- 관리하기 편하다

### 단점

비정규화의 단점은 이렇습니다:

- 삽입 및 갱신 비용이 더 든다
- 데이터베이스 설계 복잡도가 늘어난다
- 데이터 중복이 늘어난다
- 데이터 비일관성 가능성이 높아진다

# ACID와 BASE 일관성 모델

ACID와 BASE 일관성 모델에 대해 알아보겠습니다.

## ACID

ACID라는 단어는 각각 원자성(Atomicity), 일관성(Consistency), 독립성(Isolation), 지속성(Durability)을 의미합니다. ACID 속성은 트랜잭션 처리를 하는 동안 데이터 무결성을 유지하는 데 사용됩니다.

관계형 데이터베이스는 트랜잭션 전후로 일관성을 유지하기 위해 ACID 속성을 따릅니다. 각각의 속성을 알아봅시다:

### 원자성(Atomicity)

트랜잭션의 모든 작업들은 성공하거나 전부 롤백되거나 둘 중 하나입니다.

### 일관성(Consistency)

트랜잭션이 완료되는 때 데이터베이스는 구조적으로 안정됩니다.

### 독립성(Isolation)

트랜잭션들은 서로를 침범하지 않습니다. 데이터로의 경쟁적인 접근은 데이터베이스에 의해 제어되며 바깥에서 보기에 트랜잭션이 순차적으로 처리되는 것처럼 보입니다.

### 지속성(Durability)

트랜잭션이 완료되고 쓰기와 갱신 작업들이 디스크에 기록되면 시스템 실패가 일어나도 시스템 안에 남아 있습니다.

## BASE

데이터의 양과 고가용성 요구가 점점 증가함에 따라 데이터베이스 디자인도 극적으로 변했습니다. 가용성이 높은 상태에서 확장 능력까지 증가시키기 위해 데이터베이스 로직을 나누어진 서버들로 이동시켰습니다. 이런 방법으로 데이터베이스가 더욱 독립적이 되어 데이터를 저장하는 실제 작업에 더 집중할 수 있게 되었습니다.

NoSQL 데이터베이스 세계에서 일부 데이터베이스들이 확장성이나 복원력과 같은 이득을 얻기 위해 즉시적인 일관성이나 데이터 신선도(?)와 정확도를 느슨하게 만듦에 따라 ACID 트랜잭션은 더 이상 공통적인 것이 아닙니다.

BASE 속성은 ACID가 보장하는 것보다 더 느슨하지만 두 개의 일관성 모델을 1:1로 비교할 수는 없습니다. 아래 용어들에 대해 알아봅시다:

### 기본 가용성(Base Availiability)

데이터베이스가 대부분의 시간동안 정상 동작합니다.

### 부드러운 상태(Soft State)

저장소가 더 이상 쓰기 작업에 일관적이지 않고, 레플리카들이 항상 일관된 데이터를 가지고 있을 필요도 없습니다.

### 결과론적 일관성(Eventual Consistency)

데이터가 일시적으로 일관적이지 않을 수는 있지만 결국에는 일관적이게 됩니다. 시스템에 대한 읽기 작업은 일관적이지 않은 속성 때문에 올바른 응답을 주지 않을 가능성도 있지만 동작 자체는 문제없습니다.

## ACID vs BASE 트레이드오프

어떤 어플리케이션이 ACID나 BASE 일관성 모델을 사용해야 할 지에 대해서는 정답이 없습니다. 두 모델은 서로 다른 요구사항을 만족시키기 위해 존재해 왔습니다. 데이터베이스를 선택할 때 두 모델의 속성들과 개발하는 어플리케이션의 요구사항을 항상 마음에 두고 있어야 합니다.

BASE의 느슨한 일관성 아래 개발자가 어플리케이션 BASE 저장소를 사용하기로 결정했다면 일관적인 데이터라는 것에 대해 더 지식을 많이 쌓고 엄밀해야 합니다. 선택한 데이터베이스에 대한 BASE적 행동에 대해 익숙해지고 그 제한 안에서 작업하는 것이 필수입니다.

다시 말해, BASE 제약 아래 계획을 세우는 것이 ACID 트랜잭션의 간단명료함에 비교해봤을 때 커다란 단점이 될 수 있습니다. 데이터 안정성과 일관성이 필수적인 경우 완전 ACID 데이터베이스가 완벽한 선택입니다.

# CAP 이론

CAP 이론에서는 어떤 분산 시스템이 갖추길 바라는 세 가지 속성인 일관성(Consistency), 가용성(Availiability), 분할 내구성(Partition Tolerance) 중 두 가지만 가질 수 있다고 말합니다.

![cap-theorem](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/cap-theorem/cap-theorem.png)

세 가지 분산 시스템의 특징들을 자세히 살펴보면서 CAP 이론이 말하고자 하는 바를 알아봅시다.

### 일관성

일관성(Consistency)은 모든 클라이언트가 어느 노드에 연결했던 상관없이 같은 시간에 같은 데이터를 볼 수 있는 것입니다. 그렇게 되기 위해서는 쓰기 작업이 "성공"이라고 처리되기 전에 데이터가 한 노드에 기록되는 때 즉시 시스템의 다른 모든 노드로 복제되어야 합니다.

### 가용성

가용성(Availiability)은 하나 혹은 그 이상의 노드가 내려가더라도 클라이언트가 데이터를 요청하면 응답을 받을 수 있는 속성입니다.

### 분할 내구성

분할 내구성(Partition Tolerance)는 메시지 유실이 발생하거나 부분적으로 장애가 있어도 전체 시스템이 동작해야 하는 것을 의미합니다. 분할 내구성이 있는 시스템은 전체 네트워크의 장애로 이어지지 않는 류의 네트워크 장애를 버틸 수 있습니다. 간헐적인 다운에도 데이터가 충분히 노드들과 네트워크의 조합에 충분히 복제되어 시스템이 내려가지 않습니다.

## 일관성-가용성 트레이도오프

우리는 현실 세계에 살고 있기 때문에 언제나 네트워크의 안정성을 장담할 수 없으므로 분산 데이터베이스는 반드시 분할 내구성(P)을 지녀야 합니다. 이 말은 여기에는 일관성(C)와 가용성(A) 사이에 트레이드오프가 있다는 말을 뜻합니다.

### CA 데이터베이스

CA 데이터베이스는 모든 노드에 일관성과 가용성을 제공합니다. 시스템의 어떤 두 노드 간에 단절이 생기면 더 이상 동작할 수 없기 때문에 분할 내구성을 제공하지는 못합니다.

**예**: [PostgreSQL](https://www.postgresql.org), [MariaDB](https://mariadb.org).

### CP 데이터베이스

CP 데이터베이스는 가용성을 희생하여 일관성과 분할 내구성을 달성합니다. 어느 두 노드 사이에 단절이 발생하는 경우 단절이 해결될 때까지 일관적이지 않은 노드를 내려야 합니다.

**예**: [MongoDB](https://www.mongodb.com), [Apache HBase](https://hbase.apache.org).

### AP 데이터베이스

AP 데이터베이스는 일관성을 희생하여 가용성과 분할 내구성을 달성합니다. 단절이 발생하면 모든 노드가 사용 가능상태로 남지만 잘못된 부분의 경우 다른 노드에 비해 오래된 버전의 데이터를 반환할 수 있습니다. 일반적으로 AP 데이터베이스는 단절이 복구되면 노드들을 다시 동기화하여 모든 비일관성을 해결합니다.

**예**: [Apache Cassandra](https://cassandra.apache.org), [CouchDB](https://couchdb.apache.org).

# PACELC 이론

PACELC 이론은 CAP 이론의 확장입니다. CAP 이론에서는 분산 시스템에서 네트워크 단절(P)이 발생하면 가용성(A)과 일관성(C) 중 하나를 선택해야 한다고 말합니다.

PACELC는 CAP 이론을 확장하여 분산 시스템에 지연(L)이라는 속성을 추가합니다. 이론에서는 else(E)를 사용하여 심지어 시스템이 단절이 일어나지 않는 정상적인 상황에서도 지연(L)과 일관성(C) 중에 하나를 선택해야 한다고 합니다.

_PACELC 이론은 [Daniel J. Abadi](https://scholar.google.com/citations?user=zxeEF2gAAAAJ)에서 처음 소개되었습니다._

![pacelc-theorem](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/pacelc-theorem/pacelc-theorem.png)

PACELC 이론은 CAP이론이 성능이나 지연에 대한 이야기가 없다는 핵심적인 한계를 해결하기 위해 만들어졌습니다.

예를 들어 CAP 이론에 따르면 데이터베이스는 쿼리가 30일 뒤에 반환되어도 가용성이 있다고 간주할 수 있습니다. 실제 어플리케이션에서는 분명히 받아들일 수 없는 수준의 지연입니다.

# 트랜잭션

트랜잭션(Transaction)은 일련의 데이터베이스 작업들이지만 _"작업 단위 하나"_ 처럼 간주하는 것입니다. 트랜잭션의 작업들은 모두 성공하거나 모두 실패해야 합니다. 이런 방식으로 트랜잭션은 시스템 장애에도 데이터 무결성을 지원합니다. 일부 데이터베이스에서는 함께 구현하는 것이 어렵거나 이론상 불가능한 다른 최적화를 우선하는 경우도 있기 때문에 모든 데이터베이스들이 ACID 트랜잭션을 지원하지는 않습니다.

_대개 관계형 데이터베이스들은 ACID 트랜잭션을 지원하고 비관계형 데이터베이스는 지원하지 않습니다(예외는 있습니다)._

## 상태

데이터베이스의 트랜잭션은 아래 상태들 중 하나에 있을 수 있습니다:

![transaction-states](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/transactions/transaction-states.png)

### 활성(Active)

활성 상태는 트랜잭션이 일어나려고 하는 상태입니다. 모든 트랜잭션의 최초 상태입니다.

### 부분적으로 커밋됨(Partially Committed)

트랜잭션이 마지막 작업을 실행하면 부분적으로 커밋된 상태라고 합니다.

### 커밋(Committed)

트랜잭션이 모든 작업들을 성공적으로 실행하면 커밋된 상태라고 합니다. 이 작업으로 인한 모든 효과들은 이 데이터베이스 시스템에 영구적으로 반영됩니다.

### 실패(Failed)

어느 데이터베이스 복구 시스템이라도 실패하게 되면 실패 상태라고 하는 상태가 됩니다. 실패한 트랜잭션은 더 이상 진행되지 않습니다.

### 중지됨(Aborted)

어떤 체크가 실패하고 실패 상태에 도달하면 복구 매니저가 데이터베이스의 쓰기 작업들을 되돌려 트랜잭션을 실행하기 이전 상태로 돌아갑니다. 이 상태의 트랜잭션은 중지된 상태입니다.

데이터베이스 복구 모듈은 트랜잭션이 중지된 후 두 작업 중 하나를 선택할 수 있습니다:

- 트랜잭션 재실행
- 트랜잭션 종료

### 종료(Terminated)

더 이상 트랜잭션이 롤백할 것이 없거나 _커밋 상태_ 에서 오는 경우 시스템은 일관적이며 새로운 트랜잭션을 받을 수 있으며 오래된 트랜잭션은 종료됩니다.

# 분산 트랜잭션

분산 트랜잭션은 둘 혹은 그 이상의 데이터베이스를 통해 일어나는 일련의 작업들입니다. 대개 네트워크에 분산된 노드들 간에 조직되지만 서버 하나에 여러 데이터베이스가 있는 경우에도 확장할 수 있습니다.

## 분산 트랜잭션이 필요한 이유

분산 트랜잭션은 단일 데이터베이스의 ACID 트랜잭션과는 달리 여러 데이터베이스의 데이터를 교체하는 작업입니다. 따라서 분산 트랜잭션 처리는 각 데이터베이스가 트랜잭션을 자체 포함된 단위처럼 커밋과 롤백을 조직해야 하기 때문에 더욱 복잡합니다.

다시 말해 모든 노드가 모두 커밋하거나 모두 취소하여 트랜잭션이 롤백되게 해야 합니다. 이것이 분산 트랜잭션이 필요한 이유입니다.

분산 트랜잭션에 사용되는 일부 유명한 솔루션을 보겠습니다:

## 2단계 커밋

![two-phase-commit](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/distributed-transactions/two-phase-commit.png)

2단계 커밋(Two-phase commit, 2PC) 프로토콜은 분산 트랜잭션에 참여하는 모든 프로세스에게 트랜잭션을 커밋하거나 취소(롤백)하도록 조직하는 분산 알고리즘입니다.

이 프로토콜은 일시적으로 시스템에 장애가 있는 경우를 포함한 많은 경우에도 목표를 달성하기 때문에 자주 쓰입니다. 하지만 모든 장애 상황에 대응하기는 어렵고 일부 드문 경우에는 데이터를 수동으로 고쳐야 할 필요도 있습니다.

이 프로토콜에는 여러 노드를 아울러 일어나는 트랜잭션을 관장하는 코디네이터 노드가 필요합니다. 코디네이터라는 이름은 이 노드가 두 단계에 일어나는 일들에 대한 협의를 담당하고 있기 때문에 이렇게 이름이 붙었습니다.

### 단계들

2PC는 아래와 같은 단계들이 있습니다:

**준비 단계**

준비 단계(prepare phase)는 코디네이터 노드가 각 참여하는 노드에서 합의를 수집하는 작업이 포함됩니다. 모든 노드가 _준비_ 상태로 응답하지 않는다면 트랜잭션이 취소됩니다.

**커밋 단계**

만약 모든 참여자들이 _준비_ 라고 코디네이터에가 응답한다면 코디네이터는 모든 노드들에게 트랜잭션을 커밋할 것을 요청합니다. 실패가 일어난다면 트랜잭션이 롤백될 것입니다.

### 문제

2PC 프로토콜에서 다음과 같은 문제가 생길 수 있습니다:

- 노드 하나가 충돌한다면?
- 코디네이터 자체가 충돌한다면?
- 블로킹 프로토콜이다

## 3단계 커밋

![three-phase-commit](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/distributed-transactions/three-phase-commit.png)

3단계 커밋(Three-phase commit, 3PC)는 2PC의 확장으로 커밋 단계가 또다른 두 단계로 나뉩니다. 2PC에서 발생하는 블로킹 문제를 해결합니다.

### 단계들

3PC는 아래와 같은 단계들이 있습니다:

**준비 단계**

2PC의 준비 단계와 같습니다.

**프리커밋 단계**

코디네이터가 프리커밋 메시지를 발생시키고 참여하는 모든 노드들은 반드시 응답해야 합니다. 참여 노드 하나가 제 시간에 메시지를 받지 못하면 트랜잭션이 취소됩니다.

**커밋 단계**

2PC의 커밋 단계와 비슷합니다.

### 프리커밋 단계가 필요한 이유

프리커밋 단계가 달성하는 바는 아래와 같습니다:

- 참여 노드가 이 단계에서 찾아진다면 _모든_ 참여 노드가 첫 단계를 완료했다는 의미입니다. 준비 단계의 완료가 보장됩니다.
- 모든 단계에서 타임아웃을 발생시킬 수 있으므로 무한 대기를 피할 수 있습니다.

## 사가

![sagas](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/distributed-transactions/sagas.png)

사가(Saga)는 일련의 로컬 트랜잭션들입니다. 각각의 로컬 트랜잭션은 데이터베이스를 갱신하고 메시지나 이벤트를 통해 사가에 있는 다음 로컬 트랜잭션을 발동시킵니다. 만약 로컬 트랜잭션 하나가 비즈니스 로직 침범으로 인해 실패한다면 변경사항을 되돌리는 일련의 보상 트랜잭션을 실행합니다. 이 보상 트랜잭션은 앞서 실행했던 로컬 트랜잭션에 의해 만들어집니다.

### 조정

두 가지 일반적인 구현 방법이 있습니다:

- **Choreography**: 각 로컬 트랜잭션이 다른 서비스의 트랜잭션을 발동하는 도메인 이벤트를 발행합니다.
- **오케스트레이션(Orchestration)**: 오케스트레이터가 참여자들에게 어떤 로컬 트랜잭션을 실행할 지 알려줍니다.

### 문제

- 사가 패턴은 디버깅하기가 특히 어렵습니다.
- 사가 참여자들 간에 의존성 참조가 발생할 위험이 있습니다
- 참여 데이터 고립이 없기 때문에 지속성에 문제가 있을 수 있습니다.
- 트랜잭션을 실행하기 위해 모든 서비스가 동작하고 있어야 하기 때문에 테스트하기가 어렵습니다.

# 샤딩

샤딩에 대해 알아보기 전에 데이터 분할에 대해 이야기해보겠습니다.

## 데이터 분할

데이터 분할(Data partitioning)은 데이터베이스를 작은 부분 여러개로 나누는 기법입니다. 관리성이나 성능, 데이터베이스의 가용성을 늘리기 위해 데이터베이스나 테이블을 여러 머신으로 나누는 과정입니다.

### 방법

어플리케이션 데이터베이스를 작은 데이터베이스 여러개로 나누는 방법에는 여러 가지 다른 방법들이 있습니다. 다양한 거대 규모 어플리케이션들에서 사용하는 방법 세 가지를 소개해드리겠습니다:

**수평 분할(또는 샤딩)**

이 전략에서는 _파티션 키_ 라고 하는 값들의 범위를 기준으로 테이블 데이터를 수평으로 나눕니다. 이를 **_데이터베이스 샤딩_** 이라고도 부릅니다.

**수직 분할**

수직 분할에서는 컬럼에 기반하여 데이터를 수직으로 나눕니다. 테이블을 상대적으로 더 적은 컬럼을 가진 작은 테이블로 나누고 각 파티션에 부분들이 존재합니다.

이 튜토리얼에서는 특히 샤딩에 대해 초점을 맞출 것입니다.

## 샤딩이란?

샤딩은 _수평 분할_ 과 관련된 데이터베이스 설계 패턴으로, 테이블의 로우를 _파티션_ 또는 _샤드_ 라고 하는 다른 여러 테이블로 분산하는 것입니다. 각 파티션은 동일한 스키마와 컬럼들을 가지며 공유된 데이터의 서브셋이 됩니다. 비슷하게 각 파티션에 있는 데이터는 유일하며 다른 파티션이 가지고 있는 데이터와는 독립적입니다.

![sharding](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/sharding/sharding.png)

샤딩을 하는 이유는 머신을 더 추가하는 식으로 수평 확장하는 것이 더 성능이 좋은 서버를 추가하는 수직 확장보다 더 저렴하고 가능성 있는 확장 방법이기 때문입니다. 샤딩은 어플리케이션 레벨이나 데이터베이스 레벨 모두에서 구현할 수 있습니다.

## 분할 기준(partition criteria)

데이터 분할을 위한 많은 기준들이 있습니다. 가장 자주 사용되는 것들은 아래와 같습니다:

### 해시 기반

연속적인 인덱스에 기반하는 것이 아니라 해시 알고리즘을 통해 다른 파티션에 로우를 분배합니다.

이 방법의 단점은 데이터베이스 서버를 추가하거나 제거하는 것에 드는 비용이 크다는 점입니다.

### 리스트 기반

리스트 기반 분할에서 각 파티션은 연속된 범위의 값으로 데이터를 선택하는 대신 지정된 일련의 컬럼값들을 기반으로 데이터를 선택합니다.

### 범위 기반

범위 분할은 파티션 키의 값의 범위에 따라 데이터를 다양한 파티션으로 분할합니다. 다시 말해, 각 파티션에는 파티션 키의 지정한 범위의 로우들이 저장되어 있습니다.

각 범위는 포함되지 않는 최저값와 최대값을 지정하므로 범위는 연속적이어야 하며 겹치지 않아야 합니다. 파티션 키 값이 범위의 최대값 이상이라면 다음 파티션에 추가됩니다.

### 복합

복합 분할 파티션은 이름에서 알 수 있듯이 두 개 혹은 더 많은 분할 기법들을 사용하여 데이터를 나눕니다. 우선 기법 하나로 데이터를 나눈 뒤 다시 나눌 때에는 같은 방법 혹은 다른 방법으로 나눕니다.

## 장점

하지만 샤딩이 필요한 이유가 무엇일까요? 샤딩을 했을 때의 장점은 아래와 같습니다:

- **가용성**: 나뉘어진 데이터베이스에 논리적인 독립성을 제공하여 어플리케이션이 고가용성이 되도록 합니다. 각 파티션들은 개별적으로 관리될 수 있습니다.
- **확장성**: 데이터를 여러 파티션으로 분산하여 확장성이 증가합니다.
- **보안성**: 민감한 데이터와 그렇지 않은 데이터를 다른 파티션에 보관하여 시스템의 보안성을 증가시킬 수 있습니다. 민감한 데이터에 대해 더 나은 관리 기능과 보안을 제공할 수 있습니다.
- **쿼리 성능**: 시스템의 성능을 증가시킵니다. 이제 시스템이 전체 데이터베이스에 쿼리하는 대신 더 작은 부분에만 쿼리해야 합니다.
- **데이터 관리성**: 테이블과 인덱스를 더 작게 나누어 관리에 더욱 용이한 크기가 됩니다.

## 단점

- **복잡도**: 샤딩은 일반적으로 시스템의 복잡도를 증가시킵니다.
- **샤드 간 조인**: 데이터베이스가 한번 파티션되어 여러 머신에 나뉘어지면 여러 데이터베이스 샤드 간에 조인을 수행하는 것이 그다지 좋지 않습니다. 데이터가 여러 서버들에서 받아져야 하기 때문에 성능 효율적이지 않게 됩니다.
- **리밸런싱**: 데이터 분포가 동일하지 않고 샤드 하나에 많은 부하가 걸리는 경우 요청들이 가능한 한 모든 샤드들에 동일하게 분배되도록 리밸런스해야 합니다.

## 샤딩을 해야 하는 시점

샤딩이 필요하게 되는 몇몇 경우를 아래에 소개해보겠습니다:

- 하이엔드 머신 대신 기존 하드웨어의 성능을 끌어내려는 경우
- 지리적으로 떨어진 지역 간에 데이터를 유지하는 경우
- 더 많은 샤드를 추가하여 빠르게 확장을 해야 하는 경우
- 각 머신에 부하가 덜 걸리면 더 좋은 성능이 나오는 경우
- 동시 연결이 더 많이 필요한 경우

# 일관된 해싱

해결하고자 하는 문제에 대해 알아봅시다.

## 일관된 해싱이 필요한 이유

기존의 해시 기반 분산 기법들에서는 해시 함수를 사용하여 (요청 ID나 IP와 같은) 파티션 키를 해시했었습니다. 그 다음 전체 (서버 또는 데이터베이스) 노드 수로 나머지 함수를 사용합니다. 결과로 얻어지는 값이 요청이 갈 노드입니다.

![simple-hashing](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/consistent-hashing/simple-hashing.png)

$$
\begin{align*}
& Hash(key_1) \to H_1 \bmod N = Node_0 \\
& Hash(key_2) \to H_2 \bmod N = Node_1 \\
& Hash(key_3) \to H_3 \bmod N = Node_2 \\
& ... \\
& Hash(key_n) \to H_n \bmod N = Node_{n-1}
\end{align*}
$$

여기서,

`key`: 요청 ID 또는 IP 주소.

`H`: 해시 함수 결과.

`N`: 전체 노드 개수.

`Node`: 요청이 라우팅될 노드.

만약 노드를 추가하거나 제거하는 경우 `N`이 바뀌는 것이기 때문에 기존의 매핑 전략이 깨져 같은 요청이 다른 서버에게로 가게 될 것입니다. 그 결과 대다수의 요청들이 다시 분산되어야 하기 때문에 매우 비효율적이게 됩니다.

다른 노드들에 균등하게 요청을 분산시키고 적은 노력으로도 노드를 더하거나 빼는게 가능하게 만들고 싶을 것입니다. 그래서 노드(또는 서버)의 수에 의존하지 않는 분산 기법이 필요합니다. 그래야 노드를 추가하거나 제거할 때 재배치되어야 하는 키의 수가 최소화될 것입니다.

일관된 해싱은 스케일 업 또는 다운이 일어날 때 모든 서버를 건드리거나 모든 키를 재배열할 필요가 없도록 하여 수평 확장 문제를 해결합니다.

문제가 무엇인지 알았으니 일관된 해싱에 대해 더 자세히 알아보도록 하겠습니다.

## 작동 방식

일관된 해싱은 분산 해시 테이블에서 노드의 개수와 관계없이 동작하는 해시 기법으로, 가상의 원 또는 해시 링이라고도 하는 것에서의 위치를 할당하는 방식으로 동작합니다. 이렇게 전체 시스템에 영향을 미치지 않으면서 서버나 객체들이 확장될 수 있도록 합니다.

![consistent-hashing](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/consistent-hashing/consistent-hashing.png)

일관된 해싱을 사용하면 `K/N`만큼의 데이터만 재배치하면 됩니다.

$$
R = K/N
$$

여기에서,

`R`: 재배치가 필요한 데이터 수

`K`: 파티션 키 개수

`N`: 노드의 숫자

해시 함수는 범위로 나타날 수 있는데 예를 들어 `0...m-1`로 표현하여 해시 링으로 나타낼 수 있습니다. 요청을 해시하고 그 출력에 따라 요청을 링 위에 배분합니다. 비슷하게 노드를 해시하여 같은 링 위에 배분합니다.

$$
\begin{align*}
& Hash(key_1) = P_1 \\
& Hash(key_2) = P_2 \\
& Hash(key_3) = P_3 \\
& ... \\
& Hash(key_n) = P_{m-1}
\end{align*}
$$

여기서,

`key`: 요청/노드 ID 또는 IP.

`P`: 해시 링에서의 위치.

`m`: 전체 해시 링의 범위.

이제 요청이 새로 들어오면 시계 방향으로(또는 시계 반대방향으로) 가장 가까이 있는 노드에 라우팅하면 됩니다. 다시 말해 노드가 새로 추가되거나 제거되더라도 가장 가까운 노드를 사용하면 되고 요청의 _일부분만_ 다시 라우팅할 필요가 있습니다.

일관된 해싱은 이론적으로 부하를 균등하게 나눠야 하지만 실제로 균등하게 나뉘지는 않습니다. 대개 부하 분산은 균일하지 않고 서버 하나가 요청 대부분을 처리하는 _핫스팟_ 이 되어 시스템의 병목이 되기도 합니다. 추가 노드들을 더해 해결할 수 있지만 비용이 많이 들 수 있습니다.

어떻게 이 문제를 해결할 수 있는지 알아보겠습니다.

## 가상 노드

더 균등한 부하 분산을 위해 가상 노드, 또는 VNode라고도 불리는 아이디어를 도입할 수 있습니다.

노드 하나에 자리 하나를 할당하는 대신 해시 범위를 더 많고 작은 범위로 나누고 각 물리 노드들을 몇몇 작은 범위에 할당합니다. 각 하위 범위들을 VNode라고 생각하면 됩니다. 따라서 가상 노드들은 실제로는 노드의 할당 범위 변경을 최소화하기 위해 해시 링에 여러 가상 노드가 할당된 물리 노드입니다.

![virtual-nodes](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/consistent-hashing/virtual-nodes.png)

여기 해시 함수에 숫자 `k`를 사용할 수 있습니다.

$$
\begin{align*}
& Hash_1(key_1) = P_1 \\
& Hash_2(key_2) = P_2 \\
& Hash_3(key_3) = P_3 \\
& . . . \\
& Hash_k(key_n) = P_{m-1}
\end{align*}
$$

여기서,

`key`: 요청/노드 ID 또는 IP.

`k`: 해시 함수의 개수.

`P`: 해시 링에서의 위치.

`m`: 해시 링의 전체 범위.

가상 노드가 해시 범위를 더 작은 범위로 나누어 실제 노드로 가는 부하를 더욱 균등하게 분산되도록 도와주기 때문에 노드를 추가하거나 제거한 뒤 일어나는 리밸런싱 속도를 빠르게 합니다. 또한 핫스팟이 생길 가능성도 줄여줍니다.

## 데이터 복제

일관된 해싱은 고가용성과 지속성을 확보하기 위해 각 데이터를 시스템의 `N`개 노드에 복제하는데 여기서 `N`은 _복제 상수(replication factor)_ 입니다.

복제 상수는 동일한 데이터 복제본을 받을 노드의 수를 말합니다. 이 작업은 결과론적 일관성 시스템에서 비동기적으로 이루어집니다.

## 장점

일관된 해싱의 장점들을 좀 알아보겠습니다.

- 재빠른 스케일 업/다운이 더 예측 가능해집니다.
- 노드 간 분할과 복제를 자동화합니다.
- 확장성과 가용성을 확장시킵니다.
- 핫스팟을 줄여줍니다.

## 단점

일관된 해싱의 단점을 일부 나열해보면 이렇습니다:

- 복잡도가 증가합니다.
- 실패가 생기는 경우 줄이어 이어집니다.
- 여전히 부하 분산이 균일하지 않을 수 있습니다.
- 노드가 일시적으로 실패하는 경우 키 관리에 드는 비용이 클 수 있습니다.

## 예제들

일관된 해싱이 사용되는 예들이 여기 있습니다:

- [Apache Cassandra](https://cassandra.apache.org)에서의 데이터 분할.
- [Amazon DynamoDB](https://aws.amazon.com/dynamodb)의 다중 저장 호스트 간의 부하 분산.

# 연합 데이터베이스

연합(Federation) 또는 기능적 분할은 데이터베이스를 기능에 따라 나눕니다. 연합 구조는 엔드 유저에게 몇몇 다른 물리 데이터베이스가 마치 하나의 데이터베이스인 것처럼 보이게 합니다.

연합의 모든 컴포넌트들은 연합의 데이터 전체를 관통하는 일반성을 표현하는 하나 이상의 연합 스키마로 묶입니다. 연합된 스키마들은 연합 컴포넌트에 의해 공유되는 정보를 명시하고 컴포넌트들 간 통신을 할 때 공통 기초를 제공하는 데 사용할 수 있습니다.

![database-federation](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-II/database-federation/database-federation.png)

또한 연합은 여러 데이터 소스에서 온 결합되고 통합된 데이터 뷰를 제공합니다. 연합 시스템의 데이터 소스에는 데이터베이스를 포함한 다양한 구조화되고 비구조화된 데이터 형태가 있습니다.

## 특징

연합 데이터베이스의 핵심 특징들을 몇 개 보겠습니다:

- **투명성(transparency)**: 연합 데이터베이스는 하위 데이터 소스의 유저 다름이나 구현을 지웁니다. 따라서 유저는 데이터가 어디에 저장되는지 알 필요가 없습니다.
- **이질성(heterogeneity)**: 데이터 소스는 서로 다를 수 있습니다. 연합 데이터베이스는 다른 하드웨어, 다른 네트워크 프로토콜, 다른 데이터 모델 등을 처리할 수 있습니다.
- **확장성(extensibility)**: 사업의 바뀐 요구사항에 맞추어 새로운 데이터 소스가 필요할 수 있습니다. 좋은 연합 데이터베이스는 새로운 소스를 쉽게 추가할 수 있어야 합니다.
- **자치(autonomy)**: 연합 데이터베이스는 기존 데이터 소스나 인터페이스를 바꾸지 않고 항상 같은 상태로 두어야 합니다.
- **데이터 통합(data integration)**: 연합 데이터베이스는 다양한 프로토콜이나 DBMS 등등에서 온 데이터를 통합할 수 있습니다.

## 장점

연합 데이터베이스의 장점은 아래와 같습니다:

- 유연한 데이터 공유.
- 데이터베이스 컴포넌트들 간의 자치.
- 서로 다른 형태의 데이터를 통합된 방법으로 접근할 수 있습니다.
- 어플리케이션이 레거시 데이터베이스와 강한 커플링을 가지지 않습니다.

## 단점

연합 데이터베이스의 단점은 아래와 같습니다:

- 추가 하드웨어와 추가 복잡도가 더해집니다.
- 두 데이터베이스에서의 데이터를 조인하는 것은 복잡합니다.
- 자발적인 데이터 소스에의 의존성.
- 쿼리 성능 및 확장성.

# N-티어 구조

N-티어 구조는 어플리케이션 하나를 논리적인 계층(레이어, layer)과 물리적인 단계(티어, tier)로 나눕니다. 계층을 사용하여 책임을 분산하고 의존성을 관리합니다. 각 계층에는 해당하는 특정 책임이 있습니다. 높은 계층은 낮은 계층의 서비스를 사용할 수 있고 다른 방법은 없습니다.

![n-tier-architecture](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/n-tier-architecture/n-tier-architecture.png)

티어는 물리적으로 분리되어 별도의 머신에서 돌아갑니다. 한 티어가 다른 티어를 직접 호출하거나 비동기 메시징으로 호출할 수 있습니다. 물론 각 계층이 고유 티어에서 돌아갈 수도 있지만 필수 사항은 아닙니다. 몇몇 계층들은 같은 티어에서 돌아가는 것도 가능합니다. 물리적으로 티어를 나누는 것은 확장성과 회복력을을 개선하고 추가 네트워크 통신으로 인해 지연 시간이 증가하게 됩니다.

N-티어 구조는 두 가지 유형이 있습니다:

- 닫힌 계층 구조, 한 계층은 바로 아래 있는 레이어만 호출할 수 있습니다.
- 열린 계층 구조, 한 계층은 아래에 있는 어느 계층이건 호출할 수 있습니다.

닫힌 레이어 구조는 레이어 간의 의존성을 제한합니다. 하지만 단순히 한 계층이 다음 계층으로 메시지를 전달하기만 하는 것이라면 불필요한 네트워크 트래픽이 만들어지게 되는 셈입니다.

## N-티어 구조 유형

N-티어 구조의 예들을 보겠습니다.

### 3-티어 구조

3-티어는 자주 사용되고 있으며 아래와 같은 다른 계층을 사용합니다:

- **표현 계층(Presentation Layer)**: 어플리케이션과 유저 간의 상호작용을 처리합니다.
- **비즈니스 로직 계층(Business Logic Layer)**: 어플리케이션 계층에서 데이터를 받아 비즈니스 로직에 따라 데이터를 검증한 뒤 데이터 계층으로 전달합니다.
- **데이터 접근 계층(Data Access Layer)**: 비즈니스 계층에서 데이터를 받아 데이터베이스에 필요한 작업을 수행합니다.

### 2-티어 구조

표현 계층이 이 구조에서는 클라이언트에서 동작하며 데이터 저장소와 통신합니다. 여기에는 클라이언트와 서버 사이에 비즈니스 로직 계층이나 즉시 처리 레이어 같은 것이 없습니다.

### 단일 티어 또는 1-티어 구조

어플리케이션이 개인용 컴퓨터에서 돌아가는 것과 같은 가장 간단한 구조입니다. 어플리케이션에 필요한 모든 컴포넌트들이 단일 어플리케이션이나 단일 서버에서 실행됩니다.

## 장점

N-티어 구조의 장점은 다음과 같습니다:

- 가용성을 개선할 수 있다.
- 각 계층이 방화벽처럼 동작할 수 있기 때문에 더 나은 보안성을 제공한다.
- 티어를 분리하면 필요할 때 확장이 가능하다.
- 여러 사람이 서로 다른 티어를 관리할 수 있기 때문에 관리가 용이하다.

## 단점

N-티어 구조의 단점은 다음과 같습니다:

- 시스템 전체적인 관점에서 복잡도가 증가한다.
- 티어 개수가 늘어날수록 네트워크 지연이 증가한다.
- 모든 각 티어가 하드웨어 비용이 들기 때문에 전체적으로 비용이 많이 든다.
- 네트워크 보안을 관리하기 어렵다.

# 메시지 브로커

메시지 브로커는 어플리케이션이나 시스템, 서비스들이 서로 통신하고 정보를 교환할 수 있게 해 주는 소프트웨어입니다. 메시지 브로커는 일반적인 메시징 프로토콜들 간에 메시지들을 해석하여 그런 작업을 합니다. 메시지 브로커는 독립적인 서비스들이 다른 서비스에게 심지어 다른 언어로 만들어지거나 다른 플랫폼에서 구현됐다 하더라도 직접 _"통신"_ 할 수 있게 만들어줍니다.

![message-broker](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/message-brokers/message-broker.png)

메시지 브로커는 목적지로 가는 메시지들을 검증, 저장, 라우팅, 전달할 수 있습니다. 메시지 브로커는 다른 어플리케이션들 사이에서 중개자로 동작하여 송신자들이 수신자가 누구고 어디 있는지, 활성화된 상태인지, 얼마나 많이 있는지 알 필요 없이 메시지를 생성할 수 있게 합니다. 이런 시설들이 시스템 내 프로세스와 서비스를 디커플링합니다.

## 모델

메시지 브로커는 두 가지 기본 메시지 분배 패턴 또는 메시징 스타일을 가지고 있습니다:

- **[점대점 메시징(Point-to-Point messaging)](https://karanpratapsingh.com/courses/system-design/message-queues)**: 메시지 큐에 활용되는 분배 패턴으로 송신자와 수신자 간에 1:1 관계를 맺게 됩니다.
- **[발행-구독 메시징(Publish-subscribe messaging)](https://karanpratapsingh.com/courses/system-design/publish-subscribe)**: 보통 _"pub/sub"_ 으로 일컬어지는 이 메시지 분배 패턴에서 각 메시지의 생성자는 메시지를 주제(topic)로 발행하고 여러 메시지 소비자들이 받기 원하는 주제를 구독합니다(subscribe).

_나중 튜토리얼에서 이 메시징 패턴들을 자세히 다를 예정입니다._

## 메시지 브로커 vs 이벤트 스트리밍

메시지 브로커는 메시지 큐와 pub/sub과 같은 두 가지 이상의 메시지 패턴을 지원하지만 이벤트 스트리밍은 pub/sub 스타일의 분배 패턴만을 지원합니다. 높은 메시지 볼륨을 위해 설계된 이벤트 스트리밍 플랫폼들은 손쉽게 확장됩니다. 레코드 스트림들을 _주제(topic)_ 라고 하는 카테고리로 정렬하고 미리 지정한 시간만큼 저장하는 것이 가능합니다. 하지만 메시지 브로커와 달리 메시지가 전달되었는지 보증하거나 어떤 소비자가 메시지를 받았는지 추적할 수 없습니다.

이벤트 스트리밍 플랫폼들은 메시지 브로커보다 더 확장성이 높다고 하지만 내고장성을 보장하기 위한 메시지 재전송이나 한정 메시지 라우팅이나 큐잉과 같은 기능은 적습니다.

## 메시지 브로커 vs 엔터프라이즈 서비스 버스(ESB)

[엔터프라이즈 서비스 버스(ESB)](https://karanpratapsingh.com/courses/system-design/enterprise-service-bus) 인프라는 복잡하고 통합하기 어렵고 관리 비용이 많이 듭니다. 운영 환경에서 문제가 발생할 경우 해결하기가 어렵고 확장이 쉽지 않고 업데이트는 지루합니다.

메시지 브로커는 서비스 간 통신과 같은 ESB와 비슷한 기능을 적은 비용으로 제공하는 _"가벼운"_ 대체재입니다. ESB가 점점 퇴출됨에 따라 [마이크로서비스 구조](https://karanpratapsingh.com/courses/system-design/monoliths-microservices#microservices)에서 더 널리 퍼지고 있습니다.

## 예제들

자주 사용되는 메시지 브로커들은 아래와 같습니다:

- [NATS](https://nats.io)
- [Apache Kafka](https://kafka.apache.org)
- [RabbitMQ](https://www.rabbitmq.com)
- [ActiveMQ](https://activemq.apache.org)

# 메시지 큐

메시지 큐는 비동기 통신을 가능하게 해주는 서비스 간 통신의 형태입니다. 메시지 생성자에게서 비동기적으로 메시지를 받아 소비자에게 전달합니다.

큐는 대규모 분산 시스템에서 효율적으로 요청들을 관리하는 데 사용됩니다. 작은 데이터베이스와 적은 연산을 하는 소규모 시스템에서는 쓰기가 빠르지만 더 복잡하고 거대한 시스템에서는 생각보다 시간이 오래 거릴 수 있습니다.

![message-queue](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/message-queues/message-queue.png)

## 작동 방식

메시지는 처리되고 제거되기 전까지 큐에 저장됩니다. 각 메시지는 소비자 하나가 한번만 처리합니다. 더 자세히 설명하면 아래와 같습니다:

- 생성자가 큐에 작업을 생성하고 유저들에게 작업 상황을 알려줍니다.
- 소비자가 큐에서 작업을 하나 꺼내와 처리한 뒤 작업이 왼료됐다고 신호합니다.

## 장점

메시지 큐의 장점을 알아보겠습니다:

- **확장성**: 메시지 큐는 필요할 때 정교하게 확장이 가능하게 만듭니다. 워크로드가 한계에 도달했을 때에도 어플리케이션이 충돌 걱정 없이 모든 요청을 큐에 담을 수 있습니다.
- **디커플링**: 메시지 큐는 컴포넌트 간의 의존성을 없애 디커플링된 어플리케이션들의 구현을 획기적으로 간단하게 합니다.
- **성능**: 메시지 큐는 비동기 통신을 가능하게 만드는데, 양 말단에 있는 생성자와 소비자들이 서로 직접 통신하는 것이 아니라 큐와 통신하게 됩니다. 생성자는 소비자가 작업을 완료할 때까지 기다릴 필요 없이 요청을 추가할 수 있습니다.
- **안정성**: 큐는 시스템의 다른 부분이 내려가는 경우에도 데이터를 영구 보존하고 에러를 줄입니다.

## 기능

메시지 큐가 구현하는 기능들에 대해 알아보겠습니다:

### 푸시 혹은 풀 전달

대부분의 메시지 큐는 메시지를 수신하기 위해 푸시와 풀 두 옵션 모두 제공합니다. 풀(pull)은 큐에 계속적으로 메시지를 요청하는 것을 의미합니다. 푸시(push)는 메시지가 준비됐을 때 소비자가 알림을 받습니다. 또한 롱폴링(long-polling)을 사용하여 새 메시지가 올 때까지 일정 시간동안 풀을 할 수도 있습니다.

### FIFO (First-In-First-Out) 큐

이 큐에서 가장 오래된(먼저 들어온) 엔트리, 또는 큐의 _"헤드"_ 가 가장 먼저 처리됩니다.

### 스케쥴 또는 지연 전달

많은 메시지 큐들은 메시지 하나에 특정 전달 시간을 지정할 수 있습니다. 모든 메시지에 공통 지연 시간이 필요하다면 지연 큐를 설정할 수 있습니다.

### 적어도 한 번 전달(At-Least-Once Delivery)

메시지 큐는 데이터 복제성과 고가용성을 위해 메시지의 여러 복제본을 저장하고 나중에 통신이 실패하거나 에러가 발생했을 때 메시지를 재전송하여 최소 한번은 메시지가 전달되게 합니다.

### 정확히 한 번 전달(Exactly-Once Delivery)

중복을 허용할 수 없다면 FIFO 메시지 큐는 자동으로 중복을 제거하여 각 메시지가 정확히 한 번(만) 전달되게끔 합니다.

### 데드레터 큐(Dead-letter Queues)

데드레터 큐는 다른 큐들이 성공적으로 처리할 수 없는 메시지들을 보내는 큐입니다. 추후 조사를 위해 보관하는 용도로 사용하면 제대로 소비할 수 없는 메시지 때문에 큐를 블로킹하거나 CPU 사이클을 허비하는 일 없이 간편하게 설정할 수 있습니다.

### 정렬

대부분의 메시지 큐들은 일반적으로 동일한 순서로 전달되고 적어도 한 번 이상 전달되도록 하는 최선형 정렬을 제공합니다.

### 포이즌필 메시지(Poison-pill Messages)

포이즌필은 받을 수 있지만 처리되지 않는 특별한 메시지입니다. 소비자에게 작업을 멈추고 새 입력을 더 이상 기다리지 않도록 신호를 주는 방식인데, 클라이언트-서버 모델에서 소켓을 닫는 것과 비슷합니다.

### 보안

메시지 큐는 큐에 접근하려는 어플리케이션들을 인증하며 큐 자신뿐만 아니라 네트워크의 모든 메시지들을 암호화할 수 있게 됩니다.

### 작업 큐(Task Queues)

작업 큐는 작업과 그와 관련된 데이터를 받아 작업을 실행하고 결과를 전달합니다. 스케줄링이 가능하고 연산량이 많은 작업을 백그라운드에서 돌리는 데 사용할 수 있습니다.

## 백프레셔(Backpressure)

큐가 급격하게 커지기 시작하면 큐의 크기가 메모리보다 커져 캐시 미스와 디스크 읽기, 더 느린 성능을 유발하게 됩니다. 백프레셔(배압)를 사용하여 큐 사이즈를 제한시켜 이미 큐에 있는 작업들에 대해 높은 처리율과 좋은 응답 시간을 유지할 수 있습니다. 큐가 가득 차게 되면 클라이언트는 server busy나 HTTP 503코드를 받아 나중에 재시도할 수 있습니다. 클라이언트는 요청을 나중에 다시 보낼 수 있으며 아마도 [exponentail backoff](https://en.wikipedia.org/wiki/Exponential_backoff)를 사용하게 될 것입니다.

## 예제들

많이 사용되는 메시지 큐는 아래와 같습니다:

- [Amazon SQS](https://aws.amazon.com/sqs)
- [RabbitMQ](https://www.rabbitmq.com)
- [ActiveMQ](https://activemq.apache.org)
- [ZeroMQ](https://zeromq.org)

# 발행-구독

발행-구독(publish-subscribe)은 메시지 큐와 비슷하게 비동기 통신을 가능하게 하는 서비스 간 통신의 형태입니다. 발행/구독 모델에서 특정 주제로 발행되는 어떤 메시지든 해당 주제를 구독하는 모두에게 즉시 푸시됩니다.

![publish-subscribe](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/publish-subscribe/publish-subscribe.png)

메시지의 구독자들은 대개 다른 기능을 수행하며 같은 메시지로 여러 작업들을 병렬로 수행할 수 있습니다. 발행자는 브로드캐스팅되는 정보를 사용하는 이가 누구인지 알 필요가 없고 구독자 역시 브로드캐스팅되는 정보가 누구에게서 온 것이지 알 필요가 없습니다. 이런 류의 메시징은 대개 메시지를 어디로 보내는지 그 목적지를 아는 메시지 큐와는 살짝 다릅니다.

## 작동 방식

받은 메시지를 배치로 처리하는 메시지 큐와 다르게 메시지 주제는 메시지들을 큐잉을 조금만 하거나 거의 하지 않고 모든 구독자들에게 즉시 푸시합니다. 바로 이렇게 동작합니다:

- 메시지 주제는 비동기 이벤트 알림을 브로드캐스트하는 가벼운 방법과 소프트웨어 컴포넌트들이 메시지들을 보내고 받기 위해 주제에 접속하기 위한 엔드포인트를 제공합니다.
- _발행자_ 라고 하는 컴포넌트가 간단히 주제로 메시지를 푸시하여 메시지를 브로드캐스팅합니다.
- 주제를 구독하는 (_구독자_ 라고도 하는) 모든 컴포넌트들은 브로드캐스팅된 모든 메시지를 받게 됩니다.

## 장점

발행-구독 모델의 장점들은 이렇습니다:

- **폴링 제거**: 메시지 주제는 즉발적이고 푸시에 기반한 전달을 사용하기 때문에 메시지 소비자들이 주기적으로 확인하거나 새로운 정보나 업데이트를 물어봐야 할 필요는 완전히 제겋바니다. 이 방식은 특히 지연 시간이 큰 문제가 되는 시스템에서 더 빠른 응답 시간과 낮은 전달 지연 시간을 촉진시킵니다.
- **동적 타겟팅**: 발행-구독 모델에서는 서비스 발견을 더욱 쉽고 자연스러우며 에러가 덜 발생합니다. 어플리케이션이 메시지를 보낼 수 있는 피어들의 목록을 관리하는 대신 발행자가 그냥 주제로 메시지를 보내기만 하면 됩니다. 그리고 관심이 있는 그룹들은 해당 주제의 엔드포인트에 구독을 하여 메시지들을 받기 시작합니다. 구독자들은 언제든 바뀌거나 업그레이드되고 늘어나거나 줄어들 수 있지만 시스템이 자율적으로 적응합니다.
- **디커플되며 독립적으로 스케일된다**: 발행자와 구독자들은 서로 디커플링되며 각각 별도로 동작하기 때문에 독립적으로 개발하거나 규모를 조절할 수 있습니다.
- **통신 간소화**: 발행-구독 모델은 모든 점대점 연결들을 메시지 주제에 대한 단일 연결로 대체하여 복잡도를 줄이고 메시지 토픽은 구독자를 관리하며 어떤 메시지를 어느 엔드포인트로 전달할 지 결정합니다.

## 기능들

발행-구독 모델에서 요구되는 기능들을 다뤄보겠습니다:

### 푸시 전달

발행-구독 메시징은 메시지가 메시지 주제로 발행됐을 때 즉시 비동기 이벤트 알림을 푸시합니다. 구독자들은 메시지가 사용 가능할 때 알림을 받습니다.

### 다중 전달 프로토콜

주제는 일반적으로 발행-구독 모델에서 메시지 큐나 서버리스 함수, HTTP 서버 등 여러 종류의 엔드포인트를 가질 수 있습니다.

### 팬아웃(Fanout)

이 시나리오는 메시지가 주제로 보내지고 복제되어 여러 엔드포인트에 푸시되는 경우입니다. 팬아웃은 비동기 이벤트 알림을 제공하여 병렬 처리를 가능하게 합니다.

### 필터링(Filtering)

필터링은 구독자가 메시지 필터링 정책을 만들도록 하여 주제의 모든 메시지를 받는 것이 아니라 구독자가 관심있어하는 메시지에 대해서만 알림을 받을 수 있게 합니다.

### 지속성(Durability)

발행-구독 메시징 서비스는 종종 초고가용성을 제공하며 적어도 한번 전달은 여러 서버에 같은 메시지를 저장함으로 달성할 수 있습니다.

### 보안

메시지 주제는 콘텐츠를 발행하려고 하는 어플리케이션들을 인증하여 암호화된 엔드포인트와 암호화된 메시지를 네트워크로 보내는 것이 가능하게 합니다.

## 예제들

흔히 사용되는 발행-구독 모델의 구현체는 아래와 같습니다:

- [Amazon SNS](https://aws.amazon.com/sns)
- [Google Pub/Sub](https://cloud.google.com/pubsub)

# 엔터프라이즈 서비스 버스 (ESB)

엔터프라이즈 서비스 버스 (Enterprise Service Bus, ESB) 중앙 집중화된 소프트웨어 컴포넌트가 어플리케이션들 간의 통합을 수행하는 설계 패턴입니다. 데이터 모델의 트랜스포메이션, 연결을 처리, 메시지 라우팅, 통신 프로토콜 변경, 여러 요청들의 조합을 잠재적으로 관리하는 일까지 하게 됩니다. ESB는 이런 통합과 트랜스포메이션들을 새로운 어플리케이션에서도 재사용할 수 있도록 서비스 인터페이스의 형태로 제공합니다.

![enterprise-service-bus](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/enterprise-service-bus/enterprise-service-bus.png)

## 장점

이론적으로 중앙 집중화된 ESB는 표준화가 가능한 잠재력이 있고 통신과 메시징, 엔터프라이즈 전체에 걸친 서비스 간의 통합까지 극적으로 간단하게 만들 수 있습니다. ESB를 사용하는 이점들은 아래와 같습니다:

- **개발자 생산성 향상**: 어플리케이션의 다른 부분들을 건드리는 일 없이도 개발자들이 새로운 기술들을 도입할 수 있습니다.
- **간단하며 더 비용 효율적인 확장성**: 컴포넌트들이 다른 컴포넌트에 상관없이 확장할 수 있습니다.
- **더 나은 회복력**: 한 컴포넌트가 실패하더라도 다른 컴포넌트에 영향을 주지 않으며 각 마이크로서비스가 시스템의 다른 컴포넌트에 위험을 주지 않으면서 각자의 가용성 요구사항을 준수할 수 있습니다.

## 단점

ESB가 여러 조직에 성공적으로 도입되기도 하지만 동시에 다른 많은 조직에서는 ESB가 병목이 되기도 합니다. ESB를 사용하는데 따른 단점을 아래와 같습니다:

- 통합 하나에 변경이나 개선을 일으키는 경우 같은 통합을 사용하는 다른 컴포넌트의 안정성을 저해할 수 있습니다.
- 단일 실패 지점이 모든 통신을 중단시킬 수 있습니다.
- ESB의 업데이트가 종종 기존 통합에 영향을 미치기 때문에 어떤 업데이트를 하더라도 상당한 양의 테스트를 해야 합니다.
- ESB를 한 곳에서 관리하기 때문에 팀 간의 협업이 쉽지 않습니다.
- 설정할 것이 많고 유지보수가 복잡합니다.

## 예제들

많이 사용되는 ESB들은 아래와 같습니다:

- [Azure Service Bus](https://azure.microsoft.com/en-in/services/service-bus)
- [IBM App Connect](https://www.ibm.com/in-en/cloud/app-connect)
- [Apache Camel](https://camel.apache.org)
- [Fuse ESB](https://www.redhat.com/en/technologies/jboss-middleware/fuse)

# 모놀리스와 마이크로서비스

## 모놀리스

모놀리스는 모든 기능이 들어있는 독립적인 어플리케이션입니다. 단위 하나로 빌드되며 하나의 작업만 하는 것이 아니라 비즈니스에서 필요한 모든 단계를 만족하는 모든 작업을 수행합니다.

![monolith](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/monoliths-microservices/monolith.png)

### 장점

모놀리스의 장점은 다음과 같습니다:

- 개발 및 디버그가 간편합니다.
- 통신이 빠르고 안정적입니다.
- 모니터링과 테스트가 쉽습니다.
- ACID 트랜잭션을 지원합니다.

### 단점

모놀리스의 일반적인 단점은 다음과 같습니다:

- 코드베이스가 커짐에 따라 유지보수가 어려워집니다.
- 어플리케이션이 굉장히 커플링되어 확장이 어렵습니다.
- 특정 기술 스택에 대한 전문 지식이 필요합니다.
- 매 업데이트마다 전체 어플리케이션이 배포됩니다.
- 버그 하나가 전체 시스템을 다운시키는 것과 같은 상황이 있어 안정성이 떨어집니다.
- 확장이나 새로운 기술을 접목시키기 어렵습니다.

## 모듈화된 모놀리스

모듈화된 모놀리스는 어플리케이션 하나를(_모노리스_ 부분입니다) 빌드하고 배포하는 방법 중 하나이지만 코드 자체는 어플리케이션의 필요한 기능에 따라 독립적인 모듈들로 나누어 빌드합니다.

이 방법은 다른 모듈에 영향을 미치지 않고 해당 모듈을 개선하거나 바꿀 수 있어 모듈 간의 의존성을 줄여줍니다. 제대로 되기만 한다면 시스템이 커지면서 복잡해지는 모노리스를 유지하는 데 있어 복잡도을 줄임으로 장기적으로 매우 유용합니다.

## 마이크로서비스

마이크로서비스 구조는 작고 자발적인 서비스들로 구성되어 있는데 각 서비스는 제한된 영역(bounded context)에서 하나의 비즈니스 작업 처리를 스스로 처리하도록 구현되어야 합니다. 제한된 영역이란 도메인 모델 안에서 비즈니스 로직을 자연스럽게 직관적으로 분리하여 명확한 경계를 나눈 것입니다.

![microservices](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/monoliths-microservices/microservices.png)

각 서비스는 분리된 코드베이스를 가지기 때문에 작은 개발 팀이 관리할 수 있습니다. 서비스는 독립적으로 배포할 수 있고 전체 어플리케이션을 빌드하거나 전체 배포 없이도 기존 서비스를 업데이트할 수 있습니다.

서비스는 자체 데이터나 외부 상태를 보존해야 할 책임이 있습니다(서비스마다 데이터베이스). 분리된 데이터 계층이 데이터 보관을 담당하는 것이 전통적인 방법과 다른 부분입니다.

### 특성

마이크로서비스 구조는 이런 특성을 가지고 있습니다:

- **느슨한 커플링**: 서비스들이 독립적으로 배포되고 확장되기 위해서는 느슨하게 커플링되어야 합니다. 그래야 개발 팀들이 탈집중화되며 최소한의 제약과 운영 의존성으로 빠르게 개발할 수 있게 됩니다.
- **작지만 집중된**: 크기가 아닌 범위와 책임과 관련된 내용입니다. 서비스는 특정 문제를 해결하는 데 초점을 맞추어야 합니다. 기본적으로 _"하나만 잘 하면 됩니다"_. 전체적인 아키텍쳐와는 독립적인 것이 이상적입니다.
- **비즈니스에 맞춰진**: 마이크로서비스 구조는 대개 비즈니스의 범위와 우선순위에 따라 조직됩니다.
- **탄력적 & 내고장성**: 서비스는 서비스 실패나 에러가 있는 상황에서도 동작할 수 있도록 설계되어야 합니다. 내고장성은 독립적으로 서비스가 배포되는 환경에서 가장 중요합니다.
- **관리하기 쉬운**: 관리되지 않는 서비스는 다시 쓰여져야 하기 때문에,서비스는 관리 및 테스트가 쉬어야 합니다.

### 장점

마이크로서비스 구조의 장점들 중 몇개는 아래와 같습니다:

- 서비스 간 커플링이 느슨합니다.
- 서비스를 독립적으로 배포할 수 있습니다.
- 여러 개발 팀들이 민첩하게 움직일 수 있습니다.
- 내고장성과 데이터 독립성이 개선됩니다.
- 각자 스케일링이 가능하기 때문에 확장성이 더 좋습니다.
- 특정 기술 스택으로 장기적인 집중을 할 필요가 없습니다.

### 단점

마이크로서비스 구조에서는 아래와 같은 도전도 있습니다:

- 분산 시스템의 복잡도
- 테스트가 더 힘듭니다.
- (개별 서버나 데이터 베이스 등을) 관리하는 비용이 비쌉니다.
- 서비스 간 통신도 해결해야 할 문제가 있을 수 있습니다.
- 데이터 무결성과 일관성
- 네트워크 혼잡과 지연

### 사례

마이크로서비스를 잘 사용하는 방법에 대해 알아봅시다:

- 비즈니스 도메인에서의 서비스 모델링
- 서비스는 커플링이 느슨하고 기능적 응집도가 좋아야 합니다.
- 실패를 격리하고 탄력적인 전략으로 단계적인 서비스 실패를 예방합니다.
- 서비스들은 잘 설계된 API만을 통해서 통신해야 합니다. 구현 세부사항을 노출시키지 않아야 합니다.
- 서비스가 가지고 있는 데이터에 대한 저장소는 전용이어야 합니다.
- 서비스 간의 커플링을 피합니다. 커플링이 생기는 원인들 중에는 공유 데이터베이스 스키마와 경직된 통신 프로토콜이 있습니다.
- 모든것을 탈집중화합니다. 각 팀들은 서비스를 설계하고 빌드할 책임이 있습니다. 코드나 데이터 스키마를 공유하지 않습니다.
- [서킷 브레이커](https://karanpratapsingh.com/courses/system-design/circuit-breaker)를 사용하여 빠르게 실패함으로 내고장성을 갖도록 합니다.
- API 변경이 하위 호환되도록 합니다.

### 함정

마이크로서비스 구조에서 겪을 수 있는 일반적인 함정들은 아래와 같습니다:

- 서비스 경계가 비즈니스 도메인에 기초해 있지 않음.
- 분산 시스템을 만드는 것이 얼마나 힘든지 과소평가한다.
- 서비스 간에 데이터베이스를 공유하거나 공통 의존성을 가진다.
- 사업 조정이 되어있지 않음.
- 오너십이 명확하지 않음.
- [BASE 대신 ACID](https://karanpratapsingh.com/courses/system-design/acid-and-base-consistency-models)로 모든 것을 해결하려 한다.
- 내고장성을 고려하지 않은 설계는 단계적으로 실패하는 결과를 만들 수 있습니다.

## 분산된 모놀리스를 조심하십시오

분산 모놀리스 시스템은 마이크로서비스 구조를 닮았지만 내부적으로는 모놀리스 어플리케이션처럼 강하게 커플링되어 있습니다. 마이크로서비스 구조를 적용하면 많은 이점이 있습니다. 하지만 하나를 만들다가 분산된 모놀리스로 마무리하게 되는 경우도 있습니다.

마이크로서비스 중에 아래 어느 하나라도 해당한다면 분산된 모놀리스일 뿐이라고 볼 수 있습니다.

- 낮은 지연의 통신이 필요하다
- 서비스 확장이 쉽지 않다
- 서비스 간 의존성이 있다
- 데이터베이스와 같은 리소스를 공유한다
- 시스템이 강하게 결합됐다

어플리케이션에 마이크로서비스 구조를 사용하는 이유는 확장성을 가지기 위해서입니다. 그래서 마이크로서비스는 모든 서비스들이 독립적이 되기 위해 느슨하게 결합된 서비스들로 이루어져 있어야 합니다. 분산 모놀리스 구조는 느슨한 구조를 던져버리고 대부분의 컴포넌트들이 서로 의존하게 만듦으로 설계를 더욱 복잡하게 만듭니다.

## 마이크로서비스 vs 서비스 지향 아키텍쳐(SOA)

인터넷에서 _서비스 지향 아키텍쳐(Service-oriented architecture, SOA)_ 에 대해 말하는 것을 들어봤을 지 모릅니다. 때로는 마이크로서비스로 대체할 수 있다고도 하지만 이 둘은 다른 개념이며 두 접근접의 가장 큰 차이는 _스코프_ 입니다.

서비스 지향 아키텍쳐 (SOA)는 소프트웨어 컴포넌트들을 서비스 인터페이스라는 것을 통해 재사용 가능하게 만듭니다. 이 인터페이스들은 공통 통신 표준을 사용하고 어플리케이션의 재사용성을 극대화하는 데 초점을 두지만 마이크로서비스는 가장 작은 독립적인 서비스 단위들으 집합으로 만들어져 팀의 자주권과 디커플링에 초점을 맞춥니다.

## 마이크로서비스가 필요하지 않은 이유

![설계 범위](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/monoliths-microservices/architecture-range.png)

이쯤 되면 모놀리스가 시작하기 좋지 않아 보이는데 왜 사용할까 생각할지 모르겠습니다.

음, 상황에 따라 다릅니다. 두 방법이 서로 장단점이 명확하기도 하지만 새로운 시스템을 구축할 때는 모놀리스로 시작하는 것을 권장합니다. 마이크로서비스가 은탄환이 아니라는 것을 이해하는 것이 중요합니다. 오히려 조직적인 문제를 해결하는 방법입니다. 마이크로서비스 구조는 기술뿐만 아니라 팀과 조직의 우선순위도 같이 고려합니다.

마이크로서비스 구조로 넘어가기 위한 결정을 하기 전에 아래와 같은 질문을 해 보기 바랍니다:

- _"코드베이스를 공유하며 효율적으로 일하기에 팀이 너무 큽니까?"_
- _"다른 팀에 의해 일이 막힌 팀이 있습니까?"_
- _"마이크로서비스가 우리에게 명확한 사업 가치를 제공합니까?"_
- _"사업이 마이크로서비스를 사용할 만큼 성숙합니까?"_
- _"현재 아키텍쳐가 많은 의사소통으로 과부화를 일으킵니까?"_

어플리케이션이 마이크로서비스로 나뉠 요구사항이 없다면 필요하지 않은 것입니다. 모든 어플리케이션을 마이크로서비스로 옮겨야 할 절대적인 기준은 존재하지 않습니다.

저희도 주로 넷플릭스와 같은 회사에서 마이크로서비스를 사용하는 방법에서 영감을 얻지만 우리가 넷플릭스가 아니라는 사실을 간과했었습니다. 넷플릭스는 프로덕션급 솔루션을 만들기 전에도 수많은 이터레이션과 모델들을 거쳤고 마이크로서비스가 저 회사에서 해결하려고 했던 문제에 쓸만한 구조가 됐습니다.

이것이 여러분의 사업이 _실제로_ 마이크로 서비스가 필요한지 심도 있게 이해해야 하는 이유입니다. 제가 말하려고 하는 것은 마이크로서비스가 복잡한 고민들에 대한 해결책이며 사업에 복잡한 문제가 없다면 마이크로서비스가 필요하지 않습니다.

# 이벤트 중심 아키텍쳐 (EDA)

이벤트 중심 아키텍쳐(Event-driven architecture, EDA)는 시스템 내에서 이벤트를 사용하여 통신을 하는 것입니다. 일반적으로 메시지 브로커를 활용하여 비동기적으로 이벤트를 발행하고 소비합니다. 발행자(publisher)는 누가 이벤트를 소비하는지 알 수 없고 소비자(consumer) 역시 서로를 알지 못합니다. 이벤트 중심 아키텍쳐는 시스템 내에서 느슨한 커플링을 만드는 간단한 방법입니다.

## 이벤트란

이벤트는 시스템의 상태 변경을 표현하는 데이터 포인트입니다. 이벤트는 어떤 사건이 발생해야 한다거나 시스템이 어떻게 바뀌어야 하는가를 명시하지 않고 단순히 시스템의 상태 변경만을 공지할 뿐입니다. 유저가 액션을 취하면 이벤트를 발생시킵니다.

## 컴포넌트

이벤트 중심 아키텍쳐는 세 가지 컴포넌트로 구성됩니다:

- **이벤트 발행자**: 이벤트를 라우터에 발행합니다.
- **이벤트 라우터**: 이벤트를 필터링하고 소비자에게 전송합니다.
- **이벤트 소비자**: 이벤트를 사용하여 시스템의 상태 변화를 반영합니다.

![이벤트-중심-아키텍쳐](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/event-driven-architecture/event-driven-architecture.png)

_참고: 다이어그램의 점들은 시스템에서의 다른 이벤트를 나타냅니다._

## 유형

이벤트 중심 아키텍쳐를 구현하는 몇 가지 방법이 있는데 어떤 방법을 사용할지는 사용례에 따라 다르겠지만 아래 일반적인 예들이 있습니다:

- [사가](https://karanpratapsingh.com/courses/system-design/distributed-transactions#sagas)
- [발행-구독](https://karanpratapsingh.com/courses/system-design/publish-subscribe)
- [이벤트 소싱](https://karanpratapsingh.com/courses/system-design/event-sourcing)
- [명령과 조회의 책임 분리(CQRS)](https://karanpratapsingh.com/courses/system-design/command-and-query-responsibility-segregation)

_참고: 각 방법들은 따로 설명하겠습니다._

## 장점

장점들은 아래와 같습니다:

- 발행자와 소비자가 디커플링됨
- 고도로 확장 및 분산 가능하다
- 새 소비자를 더하기 쉬움
- 민첩성(agility)을 향상

## 챌린지

이벤트 중심 아키텍쳐에서의 챌린지는 아래와 같습니다:

- 전달 보증
- 에러 처리가 까다로움
- 대개 이벤트 중심 시스템은 복잡함
- 이벤트들을 단 한번, 순서대로 처리하기

## 사용례

이벤트 중심 아키텍쳐를 유용하게 사용할 수 있는 공통 사용례입니다:

- 메타데이터 및 메트릭
- 서버 및 보안 로그
- 이질적인 시스템 통합
- 팬아웃 및 병렬 처리

## 예

이벤트 중심 아키텍쳐를 구현하는 데 널리 사용되는 기술은 아래와 같습니다:

- [NATS](https://nats.io)
- [Apache Kafka](https://kafka.apache.org)
- [Amazon EventBridge](https://aws.amazon.com/eventbridge)
- [Amazon SNS](https://aws.amazon.com/sns)
- [Google PubSub](https://cloud.google.com/pubsub)

# 이벤트 소싱(Event Sourcing)

현재 데이터의 상태만을 도메인에 저장하는 대신 덧붙이기만 하는(append-only) 저장소를 사용하여 데이터에 어떤 액션들이 일어났는지의 전체 목록을 기록합니다. 저장소는 기록 시스템처럼 동작하며 도메인 객체를 구체화하는 데 사용됩니다. 

![event-sourcing](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/event-sourcing/event-sourcing.png)

이벤트 소싱은 데이터 모델과 사업 도메인의 동기화를 회피하면서 성능과 확장성, 응답성까지 개선하며 복잡한 도메인에서의 작업을 단순하게 만듭니다. 또한 트랜잭션 데이터에 일관성을 부여하고 전체 수정내역 및 히스토리를 관리하여 보상 액션도 가능하게 됩니다.

## 이벤트 소싱 vs 이벤트 중심 아키텍쳐 (EDA)

이벤트 소싱은 [이벤트 중심 아키텍쳐(EDA)](https://karanpratapsingh.com/courses/system-design/event-driven-architecture)와 혼동될 수 있습니다. 이벤트 중심 아키텍쳐는 서비스 경계 사이에 이벤트를 사용하여 통신하는 것에 대한 내용입니다. 일반적으로 메시지 브로커의 기능을 활용하여 다른 경계에서 비동기적으로 이벤트를 발행하고 소비합니다.

한편 이벤트 소싱은 이벤트를 하나의 상태로 사용하는 방식이기 때문에 데이터를 저장하는 방법부터 다른 접근방식을 취하고 있습니다. 현재 상태를 저장하는 대신 이벤트들을 저장하는 것입니다. 또한 이벤트 소싱은 이벤트 중심 아키텍쳐를 구현하는 여러 패턴들 중 하나입니다.

## 장점

이벤트 소싱의 장점에 대해 알아보겠습니다:

- 실시간 데이터 보고에 매우 적합합니다.
- 실패를 안전하게 처리하는 데 좋으며 이벤트 저장소에서 데이터를 복원하는 것이 가능합니다.
- 매우 유연하고 어느 유형에 메시지건 저장할 수 있습니다.
- 높은 컴플라이언스 시스템에서 감사 로그 기능을 만들 때 선호되는 방법입니다.

## 단점

아래는 이벤트 소싱의 단점입니다:

- 극단적으로 효율적인 네트워크 인프라가 요구됩니다.
- 스키마 레지스트리와 같은 메시지 포맷을 제어할 수 있는 안정적인 방법이 필요합니다.
- 다른 이벤트가 다른 페이로드를 포함합니다.

# 명령과 조회의 책임 분리 (CQRS)

명령과 조회의 책임 분리(CQRS)는 시스템의 활동을 명령과 질의로 분리하는 구조적 패턴입니다. 이 용어는 [Greg Young](https://twitter.com/gregyoung)이 처음 사용했습니다.

CQRS에서 _명령_ 은 특정 작업을 수행하게 만드는 지시입니다. 무언가를 바꾸지만 값을 반환하지는 않음을 암시하며 오직 성공 또는 실패만 알려줍니다. 그리고 _조회_ 는 시스템의 상태를 바꾸지 않거나 사이드 이펙트를 만들지 않으면서 정보를 요청하는 것입니다.

![command-and-query-responsibility-segregation](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/command-and-query-responsibility-segregation/command-and-query-responsibility-segregation.png)

CQRS의 핵심 원칙은 명령과 조회를 구분하는 것입니다. 이 둘은 시스템에서 기초적으로 다른 역할을 수행하며 이 둘을 분리했다는 것은 각각이 필요한 경우 최적화가 가능하기 때문에 분산 시스템이 실제로 이점을 가질 수 있습니다.

## CQRS와 이벤트 소싱

CQRS 패턴은 종종 이벤트 소싱 패턴과 함께 사용됩니다. CQRS 기반 시스템은 데이터 모델에 읽기와 쓰기를 분리하며 각각 연관된 작업들이 물리적으로 분리된 저장소에 위치합니다.

이벤트 소싱 패턴과 함께 사용됐을 때, 이 이벤트 저장소는 쓰기 모델이며 정보의 공식 원천이 됩니다. CQRS 기반 시스템의 실제 모델은 데이터의 구체화된 뷰로써 제공되는데 이런 뷰는 대개 매우 비정규화되어 있습니다.

## 장점

CQRS의 장점에 대해 알아보겠습니다:

- 읽기와 쓰기 워크로드의 독립적인 확장이 가능합니다.
- 확장, 최적화, 구조적 변경이 좀 더 쉬워집니다.
- 비즈니스 로직과 가까우면서 낮은 커플링을 가집니다.
- 어플리케이션이 쿼리를 할 때 복잡한 조인을 피할 수 있습니다.
- 시스템 역할 간에 명확한 경계를 제공합니다.

## 단점

CQRS의 단점은 아래와 같습니다:

- 어플리케이션 디자인이 더 복잡해집니다.
- 메시지 실패나 메시지 중복이 발생할 수 있습니다.
- 결과론적 일관성을 만들기 어렵습니다.
- 시스템 유지보수에 더 많은 노력이 듭니다.

## 사용 사례

CQRS가 도움이 되는 경우들을 아래에 적었습니다:

- 데이터 쓰기의 성능과는 별개로 데이터 읽기 성능이 반드시 미세 조정되어야 하는 경우.
- 시스템이 시간에 따라 진화하며 여러 버전의 모델을 가질 수 있거나 사업 역할이 주기적으로 바뀌는 경우.
- 다른 시스템들, 특히 이벤트 소싱과 가티 하위 시스템의 일시적인 실패 하나가 다른 요소의 가용성에 영향을 미치지 않는 시스템과 통합하는 경우. 
- 올바른 도메인 개체만이 데이터 쓰기를 할 수 있게 하여 더 나은 보안을 제공.

# API 게이트웨이

API 게이트웨이는 클라이언트와 백엔드 서비스 사이에 있는 API 관리 도구입니다. 시스템으로의 단 하나의 진입점으로 내부 시스템 구조를 캡슐화하고 각 클라이언트에게 맞춰진 API를 제공합니다. 또한 인증, 모니터링, 부하 분산, 캐싱, 스로틀링, 로깅 등을 할 책임이 있습니다.

![api-gateway](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/api-gateway/api-gateway.png)

## API 게이트웨이가 필요한 이유

마이크로서비스가 제공하는 API들의 세분화된 정도는 종종 클라이언트의 요구사항과 다릅니다. 대개 마이크로서비스는 세분화된 API를 제공하기 때문에 클라이언트는 여러 서비스와 상호작용해야 합니다. 그렇기 때문에 API 게이트웨이가 모든 클라이언트를 위한 단일 진입점으로 동작하여 좀 더 많은 기능과 더 나은 관리를 제공할 수 있습니다.

## 기능

API 게이트웨이가 가지면 좋은 기능들은 아래와 같습니다:

- 인증 및 승인
- [서비스 디스커버리](https://karanpratapsingh.com/courses/system-design/service-discovery)
- [리버스 프록시](https://karanpratapsingh.com/courses/system-design/proxy#reverse-proxy)
- [캐싱](https://karanpratapsingh.com/courses/system-design/caching)
- 보안
- 재시도 및 [서킷 브레이킹](https://karanpratapsingh.com/courses/system-design/circuit-breaker)
- [부하 분산](https://karanpratapsingh.com/courses/system-design/load-balancing)
- API 조합
- [속도 제한](https://karanpratapsingh.com/courses/system-design/rate-limiting) 및 스로틀링
- 버전 관리
- 라우팅
- IP 화이트리스트 및 블랙리스트

## 장점

API 게이트웨이의 장점에 대해 알아보겠습니다:

- API의 내부 구조를 캡슐화합니다.
- API의 중심화된 뷰를 제공합니다.
- 클라이언트 코드가 단순해집니다.
- 모니터링, 분석, 추적과 같은 기능들을 넣을 수 있습니다.

## 단점

API 게이트웨이에 대해 가능한 단점은 아래와 같습니다:

- 단일 실패 지점이 될 수 있습니다.
- 성능에 영향을 미칠 수 있습니다.
- 적절히 확장되지 않으면 병목이 될 수 있습니다.
- 설정하는 것이 쉽지 않을 수 있습니다.

## 백엔드를 위한 프론트엔드(BFF) 패턴

백엔드를 위한 프론트엔드 (Backend For Frontend, BFF) 패턴에서는 특정 프론트엔드 어플리케이션이나 인터페이스를 위해 분리된 백엔드 서비스를 만듭니다. 이 패턴은 단일 백엔드를 여러 인터페이스로 커스터마이징하는 것을 피하고 싶을 때 유용합니다. 이 패턴은 [Sam Newman](https://samnewman.io)가 처음 제안하였습니다.

또한 마이크로서비스에서 프론트엔드로 반환하는 데이터 출력이 실제 프론트엔드에서 필요한 포맷이 아니거나 필터링이 되지 않은 경우가 있습니다. 이런 문제를 해결하기 위해 프론트엔드에 데이터를 포맷을 다시 만드는 규칙이 있어야 합니다. BFF를 사용하여 이런 규칙들 일부를 중간 레이어로 이동시킬 수 있습니다.

![backend-for-frontend](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/api-gateway/backend-for-frontend.png)

BFF의 핵심 기능은 필요한 데이터를 적절한 서비스에서 가져오고, 데이터 규격을 맞추고, 프론트엔드로 보내는 것입니다.

_[GraphQL](https://karanpratapsingh.com/courses/system-design/rest-graphql-grpc#graphql)이 BFF로서의 기능을 정말 잘 합니다._

### 이 패턴을 사용해야 할 때

이런 경우에 BFF를 사용해야 합니다:

- 공유 또는 범용 백엔드 서비스 유지보수에 많은 개발 오버헤드가 발생할 때
- 백엔드를 특정 클라이언트의 요구사항에 최적화하고 싶을 때
- 여러 인터페이스에 맞도록 범용 백엔드를 커스터마이징해야 할 때

## 예제

자주 사용되는 게이트웨이 기술들은 아래와 같습니다:

- [Amazon API Gateway](https://aws.amazon.com/api-gateway)
- [Apigee API Gateway](https://cloud.google.com/apigee)
- [Azure API Gateway](https://azure.microsoft.com/en-in/services/api-management)
- [Kong API Gateway](https://konghq.com/kong)

# REST, GraphQL, gRPC

어느 시스템에서 좋은 API 설계는 언제나 중대한 영역입니다. 또한 올바른 API 기술을 선택하는 것 역시 중요합니다. 이 튜토리얼에서는 REST나 GraphQL, gRPC 같이 다른 API 기술들을 간단히 알아보겠습니다. 

## API란

API 기술들을 알아보기 전에 우선 API가 무엇인지 이해해 봅시다.

API는 어플리케이션 프로그래밍 인터페이스(Application Programming Interface)의 약자입니다. API는 어플리케이션 소프트웨어를 만들고 통합하기 위한 정의들과 프로토콜들의 집합입니다. 때때로 생산자가 필요한 콘텐츠와 소비자가 필요한 콘텐츠를 설정하는 정보 제공자와 정보 사용자 간의 계약으로 표현되기도 합니다.

다시 말해, 컴퓨터나 시스템을 통해 정보를 가져오거나 기능을 수행할 때 API가 당신이 시스템에 무엇을 필요로 하는지 통신하는 데 도움을 주어 시스템이 요청을 이해하고 완료할 수 있게 도와줍니다.

## REST

(RESTful API라고도 하는) [REST API](https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm) 는 REST 설계 스타일로 제한한 API이며 RESTful 웹 서비스와의 상호작용을 가능하게 해 줍니다. REST는 표현적 상태 전송(Represenational State Transfer)의 약자이며 [Roy Fielding](https://roy.gbiv.com)가 2000년도에 처음 제안했습니다.

_REST API에서 기초 단위는 리소스입니다._

### 개념

RESTful API의 개념들을 좀 알아보겠습니다.

**제약 사항**

API가 _RESTful_ 이기 위해서는 아래 구조적 제약들을 준수해야 합니다:

- **동일 인터페이스**: 서버와 상호작용하기 위한 동일한 방법이 있어야 합니다.
- **클라이언트-서버**: HTTP를 통해 관리되는 클라이언트 서버 구조여야 합니다.
- **Stateless**: 어떤 클라이언트 컨텍스트도 요청 중에 서버에 저장되지 않아야 합니다.
- **캐시 가능**: 모든 응답은 캐시 가능 여부와 클라이언트에 얼마나 오래 캐시되어야 하는지에 대한 정보를 포함해야 합니다.
- **계층적 시스템**: 어플리케이션 아키텍쳐가 여러 계층으로 구성되어 있어야 합니다.
- **주문형 코드**: 어플리케이션의 일부로써 실행 가능한 코드를 반환해야 합니다. _(선택)_

**HTTP 동사**

HTTP는 주어진 리소스에 사용할 수 있는 액션들을 나타내는 요청 메소드들을 정의합니다. 명사이기도 하지만 이런 요청 메소드들은 종종 _HTTP 동사_ 라고도 합니다. 각 동사는 다른 의미를 구현하지만 일부 공통적인 기능들은 단체로 공유되기도 합니다.

일반적으로 사용되는 HTTP 동사들은 아래와 같습니다:

- **GET**: 지정한 리소스의 표현을 요청한다.
- **HEAD**: `GET` 요청과 동일하지만 응답 바디가 없습니다.
- **POST**: 지정한 리소스에 개체를 전송하며 종종 상태를 바꾸거나 서버에 사이드 이펙트를 만듭니다.
- **PUT**: 목표가 되는 리소스의 모든 현재 표현을 요청 페이로드로 대체합니다.
- **DELETE**: 지정한 리소스를 제거합니다.
- **PATCH**: 리소스에 부분적인 수정을 반영합니다.

**HTTP 응답 코드**

[HTTP 응답 상태 코드](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)는 특정 HTTP 요청이 성공적으로 완료됐는지 가리킵니다.

표준에 따르면 다섯 종류가 정의되어 있습니다:

- 1xx - 정보 응답
- 2xx - 성공 응답
- 3xx - 리다이렉션 응답
- 4xx - 클라이언트 에러 응답
- 5xx - 서버 에러 응답

예를 들어 HTTP 200은 요청이 성공했음을 의미합니다.

### 장점

REST API의 장점에 대해 알아보겠습니다:

- 간단하며 이해하기 쉽다.
- 유연하며 이식 가능하다.
- 캐싱 지원이 좋다.
- 클라이언트와 서버가 커플링되지 않는다.

### 단점

REST API의 단점에 대해 알아보겠습니다:

- 데이터를 과도하게 가져온다.
- 때때로 여러 번의 서버 통신이 필요하다.

### 사용 사례

REST API는 세계적으로 정말 많이 사용되며 API를 설계할 때 기본 표준입니다. REST API는 전반적으로 매우 유연하며 거의 모든 시나리오에 잘 맞습니다.

### 예제

**users** 리소스에 동작하는 REST API의 예제입니다.

| URI           | HTTP verb | Description         |
| ------------- | --------- | ------------------- |
| /users        | GET       | Get all users       |
| /users/\{id\} | GET       | Get a user by id    |
| /users        | POST      | Add a new user      |
| /users/\{id\} | PATCH     | Update a user by id |
| /users/\{id\} | DELETE    | Delete a user by id |

_REST API에 대해 더 공부해야 할 것들이 많기 때문에 [어플리케이션 상태 엔진을 위한 하이퍼미디어(Hypermedia as the Engine of Application State, HATEOAS)](https://en.wikipedia.org/wiki/HATEOAS)를 살펴보는 것을 강력 추천합니다._

## GraphQL

[GraphQL](https://graphql.org)은 쿼리 언어이면서 요청한 데이터만을 전달하는 것을 우선하는 API를 위한 서버사이드 런타임이기도 합니다. [페이스북](https://engineering.fb.com)에서 개발했으며 2015년에 오픈소스로 공개되었습니다.

GraphQL은 빠르고 유연하며 개발자 친화적으로 API를 만들 수 있게 설계되었습니다. 그에 더해 GraphQL은 API를 유지보수하면서 기존 쿼리에 영향을 주지 않으면서 유연하게 필드를 추가 또는 비활성화할 수 있도록 부여합니다. 개발자들이 각자가 좋아하는 방법으로 API를 만들기만 하면 GraphQL 스펙이 해당 기능이 클라이언트에 잘 동작할 것을 보장해줄 겁니다.

_GraphQL에서 가장 작은 단위는 쿼리입니다._

### 개념

GraphQL의 핵심 개념들을 간단히 짚어보자면 이렇습니다:

**스키마**

GraphQL 스키마는 클라이언트가 GraphQL 서버에 접속하여 사용할 수 있는 기능들을 설명합니다.

**쿼리**

쿼리는 클라이언트가 만드는 요청입니다. 쿼리는 필드와 인자들로 구성할 수 있습니다. 쿼리의 작업 타입에는 [조작(mutation)](https://graphql.org/learn/queries/#mutations)이라고도 하는 서버 측 데이터를 수정할 수 있는 방법을 제공하는 유형도 있습니다.

**리졸버(Resolvers)**

리졸버는 GraphQL 쿼리에 대한 응답을 만드는 함수들의 집합입니다. 간단히 말해 리졸버는 GraphQL 쿼리 핸들러로 동작합니다.

### 장점

GraphQL의 장점을 알아봅시다:

- 데이터 오버페칭을(over-fetching) 없앱니다.
- 강하게 정의된 스키마
- 코드 생성 지원
- 페이로드 최적화

### 단점

GraphQL의 단점을 알아봅시다:

- 복잡도를 서버단으로 이동시킴
- 캐싱이 어려움
- 버전 관리가 모호합니다.
- N+1 문제

### 사용 사례

GraphQL은 아래와 같은 시나리오에 필수적인 것으로 증명되었습니다:

- 여러 리소스들을 쿼리 하나로 만들어 앱 대역폭을 줄이는 경우
- 복잡한 시스템에서 재빠른 프로토타이핑을 하는 경우
- 그래프와 같은 유형의 데이터 모델을 작업하는 경우

### 예제

`User`와 `Query` 타입을 정의하는 GraphQL 스키마입니다.

```graphql
type Query {
  getUser: User
}

type User {
  id: ID
  name: String
  city: String
  state: String
}
```

클라이언트는 위의 스키마를 활용하여 전체 리소스를 받아오거나 API가 어떤 값을 리턴할 지 예상할 필요 없이 필요한 필드만 간편하게 요청할 수 있습니다.

```graphql
{
  getUser {
    id
    name
    city
  }
}
```

위 쿼리는 클라이언트에 아래와 같은 응답을 합니다.

```json
{
  "getUser": {
    "id": 123,
    "name": "Karan",
    "city": "San Francisco"
  }
}
```

_[graphql.org](https://graphql.org)에서 GraphQL에 더 많이 알아볼 수 있습니다._

## gRPC

[gRPC](https://grpc.io)는 어느 환경에서든 돌아갈 수 있는 현대적인 고성능 [원격 프로시저 호출(RPC)](https://en.wikipedia.org/wiki/Remote_procedure_call) 프레임워크입니다. gRPC는 로드 밸런싱과 추적, 헬스 체크, 인증 등 다양한 선택 가능한 기능들과 함께 효율적으로 서비스와 데이터 센터에 연결할 수 있습니다.

### 개념

gRPC의 핵심 개념들을 알아보겠습니다.

**프로토콜 버퍼**

프로토콜 버퍼는 구조화된 데이터를 직렬화할 때 상위/하위 호환 가능하며 언어나 플랫폼 상관없이 확장되는 방법을 제공합니다. JSON과 비슷하면서도 더 작고 빠르며 네이티브 언어 바인딩도 만들어줍니다.

**서비스 정의**

gRPC는 다른 많은 RPC 시스템처럼 서비스와 원격으로 호출하고 반환되는 메소드들을 정의한다는 기본 아이디어에 기초해 있습니다. gRPC는 [인터페이스 정의 언어(Interface Description Language, IDL)](https://en.wikipedia.org/wiki/Interface_description_language)로 프로토콜 버퍼를 사용하여 서비스 인터페이스와 페이로드 메시지 구조를 정의합니다.

### 장점

gRPC의 장점을 알아봅시다:

- 가볍고 효율적이다.
- 고성능
- 코드 생성 지원 내장
- 양방향 스트리밍(bi-directional streaming)

### 단점

gRPC의 단점을 알아봅시다:

- REST나 GraphQL보다 상대적으로 젊다.
- 브라우저 지원이 적다.
- 학습 곡선이 가파르다
- 사람이 읽기 힘들다

### 사용 사례

gRPC를 사용하는 좋은 사례들을 아래 정리했습니다:

- 양방향 스트리밍을 사용한 실시간 통신
- 마이크로서비스에서 효율적인 서비스 간 통신
- 지연이 적고 높은 처리량을 요구하는 통신
- 폴리글랏(여러 프로그래밍 언어를 사용하는) 환경

### 예제

`*.proto` 파일에 정의된 gRPC 서비스 예제입니다. 이 정의를 사용하여 간단하게 여러분이 원하는 언어로 `HelloService` 서비스 코드를 생성할 수 있습니다.

```protobuf
service HelloService {
  rpc SayHello (HelloRequest) returns (HelloResponse);
}

message HelloRequest {
  string greeting = 1;
}

message HelloResponse {
  string reply = 1;
}
```

## REST vs GraphQL vs gRPC

앞선 API들이 어떻게 동작하는지 알아보았으니 아래 기준에 따라 비교해 보도록 하겠습니다.

- Coupling: 강한 커플링을 만드는가?
- Chattieness: API 호출이 얼마나 _수다스러운가?_ (정보를 얻기 위해 호출해야 하는 API 호출 수)
- Performance: 성능이 좋은가?
- Complexity: 통합하기 복잡한가?
- Caching: 캐싱이 잘 동작하는가?
- Codegen: 빌트인 툴이나 코드 생성이 지원되는가?
- Discoverability: API 디스커버리가 괜찮은가?
- Versioning: API 버전 관리가 쉬운가?

| Type    | Coupling | Chattiness | Performance | Complexity | Caching | Codegen | Discoverability | Versioning |
| ------- | -------- | ---------- | ----------- | ---------- | ------- | ------- | --------------- | ---------- |
| REST    | Low      | High       | Good        | Medium     | Great   | Bad     | Good            | Easy       |
| GraphQL | Medium   | Low        | Good        | High       | Custom  | Good    | Good            | Custom     |
| gRPC    | High     | Medium     | Great       | Low        | Custom  | Great   | Bad             | Hard       |

### 어느 API 기법이 더 좋을까?

음, 하나도 없다고 할 수 있겠습니다. 각 기법들이 장단점이 있기 때문에 은탄환 같은 건 없습니다. 사용자들은 일관된 방법으로 API를 사용하는 것에만 관심이 있기 때문에 API를 설계할 때 여러분의 도메인과 요구사항에 초점을 맞추는 것이 좋습니다.

# 롱폴링, 웹소켓, SSE

초기 웹 어플리케이션들은 클라이언트-서버 모델로 개발되었고 클라이언트는 언제나 서버에 데이터를 요청하는 것과 같이 트랜잭션의 시작점이 되었습니다. 클라이언트가 먼저 요청을 만들지 않는 이상 서버가 독립적으로 클라이언트에 데이터를 전송하거나 푸시할 수 있는 방법이 없었습니다. 이 문제를 극복하기 위한 방법들을 알아보겠습니다.

## 롱폴링 (Long Polling)

HTTP 롱폴링은 클라이언트에 필요한 정보가 서버에 생겼을 때 푸시하는 기법입니다. 서버는 클라이언트가 요청을 보낼 때까지 기다릴 필요가 없습니다.

롱폴링을 사용하면 클라이언트의 요청을 받은 서버는 연결을 닫지 않습니다. 그 대신 새로운 메시지가 오거나 타임아웃 제한이 걸렸을 때만 응답합니다.

![long-polling](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/long-polling-websockets-server-sent-events/long-polling.png)

클라이언트가 응답을 받으면 즉시 서버에 새로운 요청을 보내 데이터를 받을 새로운 연결을 만듭니다. 그리고 반복합니다. 이렇게 실시간 서버 푸시 기능을 흉내낼 수 있습니다.

### 동작 원리

롱폴링이 동작하는 방법을 이해해 봅시다:

1. 클라이언트가 최초 요청을 보내고 응답을 기다립니다.
2. 서버가 요청을 받고 무언가 업데이트가 있을 때까지 전송을 지연합니다.
3. 업데이트가 생기면 응답이 클라이언트에 전송됩니다.
4. 클라이언트는 응답을 받고 즉시 새로운 요청을 보내거나 미리 지정된 시간 뒤에 연결을 다시 맺습니다.

### 장점

롱폴링의 장점은 이렇습니다:

- 구현하기 간단하며 소규모 프로젝트에 적합합니다.
- 거의 모든 환경에서 지원됩니다.

### 단점

롱폴링의 가장 큰 단점은 대개 확장이 불가능하다는 점입니다. 다른 단점들은 아래와 같습니다:

- 매번 새 연결을 맺는 것이 서버 입장에서 바쁠 수 있습니다.
- 여러 요청이 있을 때 메시지 순서 보장이 문제가 될 수 있습니다.
- 서버가 새 요청을 기다려야 하기 때문에 지연이 더 생길 수 있습니다.

## 웹소켓

웹소켓은 TCP 통신 하나에서 이루어지는 전이중 통신(full-duplex communication) 채널입니다. 클라이언트와 서버 모두 어느 때나 메시지를 보낼 수 있는 지속 연결입니다.

클라이언트는 웹소켓 핸드셰이크라고 알려진 방법으로 웹소켓 연결을 맺습니다. 이 프로세스가 성공하면 서버와 클라이언트는 어느 때건 데이터를 교환할 수 있게 됩니다. 웹소켓 프로토콜은 적은 오버헤드로 서버와 클라이언트 간 통신을 가능하게 해 주어 서버롸의 실시간 데이터 통신을 촉진합니다.

![websockets](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/long-polling-websockets-server-sent-events/websockets.png)

연결이 유지되는 동안에는 서버가 클라이언트 메시지를 요청하거나 할 필요 없이 데이터를 클라이언트에 전송할 수 있는 표준화된 방법을 제공함으로 가능해졌습니다.

### 동작 원리

웹소켓이 동작하는 방법을 이해해 봅시다:

1. 클라이언트가 요청을 보내 웹소켓 핸드셰이크를 시작합니다.
2. 요청에는 [HTTP Upgrade](https://en.wikipedia.org/wiki/HTTP/1.1_Upgrade_header) 헤더가 있어 요청을 웹소켓 프로토콜로(`ws://`) 전환할 수 있게 합니다.
3. 서버는 클라이언트에 응답을 보내 웹소켓 핸드셰이크 요청을 확인합니다.
4. 클라이언트가 성공한 핸드셰이크 응답을 받는 순간 웹스켓 통신이 열립니다.
5. 클라이언트와 서버가 양방향으로 통신하기 시작해 실시간 통신이 가능해집니다.
6. 서버 또는 클라이언트가 연결을 닫으면 웹소켓 연결이 닫힙니다.

### 장점

웹소켓의 장점은 아래와 같습니다:

- 비동기 전이중 메시징
- 더 나은 origin 기반 보안 모델
- 클라이언트, 서버 모두에 가볍습니다.

### 단점

웹소켓의 단점을 알아보겠습니다:

- 종료된 연결은 자동으로 복구되지 않습니다.
- 오래된 브라우저들은 웹소켓을 지원하지 않습니다.

## 서버 전송 이벤트 (SSE)

서버 전송 이벤트(Server-Sent Events, SSE) 는 클라이언트와 서버 간 장시간 통신을 맺는 방법이며 서버가 선제적으로 클라이언트에 데이터를 전송할 수 있게 해 줍니다.

![server-sent-events](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-III/long-polling-websockets-server-sent-events/server-sent-events.png)

SSE는 단방향으로써, 일단 클라이언트가 요청을 보내기만 하면 같은 연결 안에서 새로운 요청을 보내지 않아도 여러 응답을 받을 수 있습니다.

### 동작 원리

SSE가 동작하는 방법을 이해해 봅시다:

1. 클라이언트가 서버로 요청을 보냅니다.
2. 클라이언트와 서버 사이의 연결이 맺어지고 계속 열어둡니다.
3. 새로운 데이터가 생기면 클라이언트에 응답이나 이벤트를 전송합니다.

### 장점

- 클라이언트와 서버 모두 구현하기 쉽고 사용이 간편합니다.
- 대부분의 브라우저가 지원합니다.
- 방화벽 문제가 없습니다.

### 단점

- 단방향이라는 속성이 제한적일 수 있습니다.
- 연결 최대 수에 따른 제한이 있을 수 있습니다.
- 바이너리 데이터를 지원하지 않습니다.

# 지오해싱과 쿼드트리

## 지오해싱(Geohashing)

지오해싱은 [지오코딩(geocoding)](https://en.wikipedia.org/wiki/Address_geocoding) 방법 중 하나로 경도나 위도 같은 지리 좌표를 짧은 영숫자로 변환하는 것입니다. 2008년에 [Gustavo Niemeyer](https://twitter.com/gniemeyer)가 만들었습니다.

예를 들어, 샌프란시스코의 좌표 `37.7564, -122.4016`는 `9q8yy9mf`로 표현할 수 있습니다.

### 지오해싱의 동작 원리

지오해시는 Base 32 알파벳 인코딩을 사용하는 계층적인 공간 인덱스입니다. 지오해시의 첫번째 문자는 32개의 구획의 첫번째 위치를 나타냅니다. 그 구획 역시 32개의 구획을 가지고 있습니다. 다시 말해 점 하나를 나타내기 위해 필요한 정밀도까지 비트를 추가하면서 세계를 재귀적으로 작고 작은 구획으로 나누는 것입니다. 정밀도가 구획의 크기를 정하게 됩니다.

![geohashing](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/geohashing-and-quadtrees/geohashing.png)

지오해싱에서는 공통되는 앞부분이 길 때 점들 간의 거리가 공간적으로 더 가깝습니다. 더 긴 문자열은 더 정확한 위치를 의미합니다. 예를 들어 지오해시 `9q8yy9mf`와 `0q8yy9vx`은 앞부분 `9q8yy0`가 같기 때문에 공간적으로 가깝다고 할 수 있습니다.

지오해싱은 유저의 정확한 위치를 노출할 필요가 없을 경우에 어느 정도 익명성을 제공하는 데 사용할 수 있습니다. 단지 지오해시의 길이에 따라 해당 지역 가운데 어딘가 있다는 것만 알 수 있습니다.

다른 길이를 가진 지오해시의 구획 크기는 아래와 같습니다:

| Geohash length | Cell width | Cell height |
| -------------- | ---------- | ----------- |
| 1              | 5000 km    | 5000 km     |
| 2              | 1250 km    | 1250 km     |
| 3              | 156 km     | 156 km      |
| 4              | 39.1 km    | 19.5 km     |
| 5              | 4.89 km    | 4.89 km     |
| 6              | 1.22 km    | 0.61 km     |
| 7              | 153 m      | 153 m       |
| 8              | 38.2 m     | 19.1 m      |
| 9              | 4.77 m     | 4.77 m      |
| 10             | 1.19 m     | 0.596 m     |
| 11             | 149 mm     | 149 mm      |
| 12             | 37.2 mm    | 18.6 mm     |

### 사용 사례

지오해싱을 사용하는 일반적인 사례는 이렇습니다:

- 데이터베이스에 위츠를 저장하는 간단한 방법입니다.
- 위도/경도 좌표에 비해 공유하거나 기억하기 쉽기 때문에 URL로 소셜미디어에 공유가 간편합니다.
- 매우 간단한 문자열 비교와 효율적인 색인 탐색 덕분에 효율적으로 지점 근처의 이웃을 찾을 수 있습니다.

### 예제

지오해싱은 유명한 데이터베이스들에서 널리 이용되고 있습니다.

- [MySQL](https://www.mysql.com)
- [Redis](http://redis.io)
- [Amazon DynamoDB](https://aws.amazon.com/dynamodb)
- [Google Cloud Firestore](https://cloud.google.com/firestore)

## 쿼드트리(Quadtrees)

쿼드트리는 각 노드가 정확히 4개의 자식을 가지는 트리 데이터 구조입니다. 종종 2차원 공간을 재귀적으로 4분면 또는 4개의 공간으로 나누는 데 사용합니다. 각 자식 노드는 공간 정보를 저장합니다. 쿼드트리는 3차원 공간을 분할하는 [옥트리(Octrees)](https://en.wikipedia.org/wiki/Octree)의 2차원 버전이기도 합니다.

![quadtree](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/geohashing-and-quadtrees/quadtree.png)

### 쿼드트리의 종류

쿼드트리는 나타내는 데이터의 종류에 따라 달라지는데, 영역이나 점, 선, 곡선이 포함될 수 있습니다. 아래는 자주 사용되는 쿼드트리들입니다:

- 점 쿼드트리 (Point quadtrees)
- 점-영역 쿼드트리 (Point-region (PR) quadtrees)
- 다각형 맵 (Polygonal map (PM) quadtrees)
- 압축 쿼드트리 (Compressed quadtrees)
- 엣지 쿼드트리 (Edge quadtrees)

### 쿼드트리를 사용해야 하는 이유

위도와 경도만으로 충분하지 않을까요? 이론상 위도와 경도를 이용하면 [유클리드 거리](https://en.wikipedia.org/wiki/Euclidean_distance)로 서로 얼마나 가까운지 계산이 가능하지만 실제 사용시에는 커다란 데이터셋에는 CPU를 많이 사용하기 때문에 잘 확장되지 않습니다.

![quadtree-subdivision](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/geohashing-and-quadtrees/quadtree-subdivision.png)

쿼드트리는 위도/경도나 (x, y) 좌표로 표현되는 2차원 범위 안에 있는 점을 효율적으로 찾을 수 있습니다. 그에 더해 어느 임계값까지만 노드를 나누도록 하여 연산량을 덜 수도 있습니다. [힐베르트 커브](https://en.wikipedia.org/wiki/Hilbert_curve)와 같은 매핑 알고리즘을 사용하면 범위 쿼리 성능을 쉽게 개선할 수 있습니다.

### 사용 사례

쿼드트리의 일반적인 사용은 이렇습니다:

- 이미지 표시, 프로세싱, 압축
- 공간 색인 및 범위 질의
- 구글 맵스나 우버와 같은 위치 기반 서비스
- 메쉬 생성 및 컴퓨터 그래픽스
- 희소 데이터 저장소

# 서킷 브레이커

서킷 브레이커(circuit breaker)는 실패를 감지하고 점검 중에 반복하여 발생하는 실패 또는 일시적 외부 시스템 실패, 예외적인 시스템 장애 등을 방지하는 부분을 캡슐화하는 설계 패턴입니다.

![circuit-breaker](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/circuit-breaker/circuit-breaker.png)

서킷 브레이커의 기본 개념은 매우 간단합니다. 보호된 함수 호출을 실패를 탐지하는 서킷 브레이커 객체로 감쌉니다. 실패가 어느 수준에 이르게 되면 서킷 브레이커가 동작하여 모든 서킷 브레이커로의 호출이 에러를 반환하게 되어 보호되고 있는 호출이 불리지 않게 합니다. 보통은 그에 더해 서킷 브레이커가 동작할 때 탐지 알림도 발생하게끔 만듭니다.

## 서킷 브레이커가 필요한 이유

소프트웨어 시스템이 다른 프로세스에, 대개 네트워크를 통해 연결된 다른 머신에서 돌아가는 프로세스에 원격 호출을 하는 것은 자연스러운 일입니다. 메모리 안에서 일어나는 호출과 원격 호출이 다른 가장 큰 차이 하나는 바로 원격 호출이 실패하거나 타임아웃 제한까지 응답 없이 막혀 있을 수도 있다는 것입니다. 더 나아가 응답하지 않는 서비스에 많은 외부 호출이 있다면 핵심 리소스들을 전부 다 사용해 버려 여러 시스템에 단계적인 실패를 일으키는 최악의 상황으로 이끌 수 있습니다.

## 상태들

서킷 브레이커의 상태들에 대해 알아보겠습니다:

### 닫힘(Closed)

정상적인 경우 서킷 브레이커는 닫힌 상태입니다. 언제나와 같이 모든 요청들이 서비스로 전달됩니다. 지정한 수준 이상으로 실패가 늘어난다면 서킷 브레이커가 트리거되며 열린 상태로 전환됩니다.

### 열림(Open)

열림 상태에서는 서킷 브레이커가 서비스에 접근하지 않고 바로 에러를 반환시킵니다. 서킷 브레이커는 일정 시간이 지난 뒤 반열림 상태로 전환합니다. 대개 제한 시간 초과가 전달되는 모니터링 시스템이 같이 동작할 것입니다.

### 반열림(Half-open)

반열림 상태에서는 서킷 브레이커가 서비스에서 오는 요청들 중 제한된 숫자만 통과시켜 작업이 호출되게 합니다. 만약 요청이 성공적이라면 서킷 브레이커는 닫힘 상태로 전환됩니다. 하지만 요청이 여전히 실패하는 경우 다시 열림 상태로 갑니다.

# 속도 제한

속도 제한(Rate limiting)은 지정한 제한 이상으로 작업의 속도가 늘어나는 것을 방지하는 것입니다. 속도 제한은 거대 시스템에서 아래 놓여있는 서비스와 리소스들을 보호하기 위해 일반적으로 사용됩니다. 분산 시스템에서도 대개 방어적인 장치로 속도 제한을 사용하여 공유되는 리소스의 가용성을 유지할 수 있습니다. 또한 일정 시간동안 API로 접근하는 요청의 수를 제한함으로 API를 의도적이지 않거나 악의적인 과부하로부터 보호합니다.

![rate-limiting](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/rate-limiting/rate-limiting.png)

## 속도 제한이 필요한 이유

속도 제한은 거대 시스템에서 매우 중요한 부분이며 다음과 같은 목적을 달성합니다:

- 서비스 거부 (DoS) 공격의 결과로 발생하는 리소스 기아 상태를 회피합니다.
- 속도 제한은 리소스의 자동 스케일링에 가상의 최대값을 설정하여 모니터링하지 않았을 경우 기하급수적으로 비용이 증가할 수 있었을 운영 비용을 제어할 수 있도록 도와줍니다.
- 또한 일반적인 공격들을 방어하거나 경감시키는 데 사용할 수 있습니다.
- 막대한 양의 데이터를 처리하는 API에도 속도 제한이 데이터의 흐름을 제어하는 데 사용됩니다.

## 알고리즘

속도 제한을 구현하기 위한 다양한 알고리즘이 있으며 각자가 장단점을 가지고 있습니다. 그 중 몇 가지를 간단히 알아보겠습니다:

### 리키 버킷 (Leaky Bucket)

리키 버킷은 큐를 사용한 간단하고 직관적인 알고리즘입니다. 시스템이 요청을 등록할 때 큐의 뒷부분에 요청을 추가합니다. 큐의 첫번째 아이템을 일정 시간마다 처리하거나 first-in, first-out (FIFO) 스타일로 처리합니다. 큐가 가득 차면 추가적인 요청들은 버려집니다(또는 샙니다).

### 토큰 버킷 (Token Bucket)

_버킷_ 이라는 개념을 사용합니다. 요청이 들어오면 반드시 버킷의 토큰을 받아 처리합니다. 버킷에 남은 토큰이 없다면 요청이 거부되며 추후에 다시 요청이 시도되어야 합니다. 그 결과 일정 시간 뒤 토큰 버킷은 계속 갱신됩니다.

### 고정 윈도우 (Fixed Window)

이 시스템에서는 `n`초 크기의 윈도우를 사용하여 고정 윈도우 알고리즘 비율을 추적합니다. 들어오는 요청 각각이 윈도우의 카운터를 증가시킵니다. 카운터가 일정 값을 넘어가면 요청을 버립니다.

### 슬라이딩 로그 (Sliding Log)

슬라이딩 로그 속도 제한법은 각 요청에 달린 타임스탬프를 추적하는 것과 관련됩니다. 시스템이 이 로그들을 시간에 따라 정렬된 해시 집합이나 테이블에 저장합니다. 일정 값을 초과한 타임스탬프가 달린 로그는 버려집니다. 새 요청이 오면 우선 로그들의 합계를 구해 요청 속도을 구합니다. 만약 요청이 일정 수준 이상의 속도이라면 처리되지 않습니다.

### 슬라이딩 윈도우 (Sliding Window)

슬라이딩 윈도우는 고정 윈도우 알고리즘의 저렴한 처리 비용과 슬라이딩 로그의 개선된 경계 조건을 결합한 하이브리드적인 접근입니다. 고정 윈도우 알고리즘과 같이 각 고정 윈도우의 카운터를 추적합니다. 그 다음 트래픽이 급증하는 것을 부드럽게 만들기 위해 현재 타임스탬프에 기반해 이전 윈도우의 가중치 적용된 요청 속도 값을 처리합니다.

## 분산 시스템에서의 속도 제한

분산 시스템에서의 속도 제한은 꽤 복잡합니다. 분산 시스템에서 속도 제한을 사용했을 때 나타나는 두 가지 큰 문제점은 아래와 같습니다:

### 비일관성 (Inconsistencies)

여러 노드로 이루어진 클러스터를 사용할 때 전역 속도 제한 정책이 필요할 수 있습니다. 각 노드가 각자의 속도 제한을 추적하는 경우 요청을 다른 노드에 보낼 때 소비자가 전역 속도 제한을 초과할 수 있습니다. 노드 수가 많아질수록 사용자가 전역 속도 제한을 초과할 가능성이 높아집니다.

이 문제를 해결하기 위한 가장 간단한 방법은 로드 밸런서에 스티키 세션(sticky sessions)을 사용하여 각 소비자가 정확히 하나의 노드에만 요청을 보내게 하는 것이 있지만 내고장성을 없애버리며 확장에 대한 문제도 있습니다. 또다른 해결방법은 [Redis](https://redis.io)와 같은 중앙 집중화된 데이터 저장소를 사용하는 것이지만 지연 시간이 증가하고 경합 상태를 유발할 것입니다.

### 경합 상태 (Race Conditions)

경합 상태 문제는 현재 속도 제한 카운터를 저장소에서 가져와 증가시키고 다시 저장소에 넣는 식으로 단순히 _"읽고 저장"_ 하는 식의 접근을 할 때 발생합니다. 이 모델의 문제는 읽기-증가-저장 사이클을 도는 동안 또다른 요청이 왔을 경우 각자 증가시켜 적합하지 않은(낮은 숫자의) 카운터 값을 저장하려고 한다는 것입니다. 결국 소비자가 굉장히 많은 요청을 전송하여 속도 제어를 우회하게 만듭니다.

이 문제를 피하는 한 가지 방법은 해당 키에 대한 분산 락 메커니즘을 사용하여 다른 프로세스가 카운터에 접근하거나 쓰는 것을 방지하는 것입니다. 당연하겠지만 락이 엄청난 병목이 되어 확장이 생각보다 잘 되지 않을 것입니다. 다른 더 나은 방법은 _"저장하고 읽어서"_ 재빨리 카운터를 증가시킨 뒤 값을 확인하여 원자적인 작업을 방해하지 않게 할 수 있습니다.

# 서비스 디스커버리

서비스 디스커버리(Service discovery)는 컴퓨터 네트워크에 있는 서비스를 감지하는 것입니다. 서비스 발견 프로토콜(SDP)은 리소스를 확인하여 네트워크를 감지하는 네트워크 표준입니다.

## 서비스 디스커버리가 필요한 이유

모놀리식 어플리케이션에서의 서비스는 언어 차원에서의 메소드나 함수 호출로 다른 서비스를 호출합니다. 하지만 현대적인 마이크로서비스 기반의 어플리케이션은 일반적으로 위지가 계속해서 바뀌는 가상화되었거나 컨테이너화된 인스턴스로 이루어진 서비스 환경에서 돌아갑니다. 따라서 서비스 클라이언트가 동적으로 변하며 짧은 수명을 가진 서비스 인스턴스에 요청을 할 수 있는 방법이 필요합니다.

## 구현

두 가지 서비스 디스커버리 유형이 있습니다:

### 클라이언트 측 디스커버리

![client-side-service-discovery](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/service-discovery/client-side-service-discovery.png)

이 접근법에서 클라이언트는 모든 서비스의 네트워크 위치를 관리하고 저장하는 서비스 레지스트리에서 다른 서비스의 위치를 획득합니다.

### 서버 측 디스커버리

![server-side-service-discovery](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/service-discovery/server-side-service-discovery.png)

이 접근법에서는 로드 밸런서와 같은 중간 컴포넌트를 사용합니다. 클라이언트가 로드 밸런서를 통해 요청을 하면 로드 밸런서가 가능한 서비스 인스턴스에 요청을 전달합니다.

## 서비스 레지스트리(Service registry)

서비스 레지스트리는 기본적으로 클라이언트가 접근할 수 있는 서비스 인스턴스들의 네트워크상 위치들을 담고 있는 데이터베이스입니다. 서비스 레지스트리는 반드시 고가용성이어야 하며 최신 데이터를 가지고 있어야 합니다.

## 서비스 등록(Service Registration)

서비스 정보를 얻는 방법이 필요한데, 이를 서비스 등록이라고 합니다. 두 가지 서비스 등록 접근법을 살펴보겠습니다:

### 자체 등록(Self-Registration)

자체 등록 모델을 사용하면 서비스 인스턴스가 서비스 레지스트리에 자신을 등록하고 등록 해제할 책임이 있습니다. 필요하다면 그에 더해 등록이 계속 남아 있을 수 있도록 서비스 인스턴스가 하트비트를 날리기도 합니다.

### 서드파티 등록(Third-party Registration)

레지스트리가 배포 환경에 질의하거나 이벤트를 구독함으로 동작 중인 인스턴스의 변경사항을 추적하는 방법입니다. 새로 만들어진 서비스 인스턴스를 감지하면 데이터베이스에 기록합니다. 서비스 레지스트리가 종료된 서비스 인스턴스를 등록 해제합니다.

## 서비스 메쉬(Service mesh)

분산 어플리케이션에서 서비스 간 통신은 필수이지만 서비스의 수가 늘어남에 따라 클러스터 내부 또는 클러스터 간에 통신을 전달하는 것이 굉장히 복잡해집니다. 서비스 메쉬는 개별 서비스 간의 관리되며(managed) 관찰 가능하고(observable) 안전한(secure) 통신을 가능하게 합니다. 서비스를 감지하기 위해 서비스 디스커버리 프로토콜을 사용합니다. [Istio](https://istio.io/latest/about/service-mesh)와 [envoy](https://www.envoyproxy.io)가 가장 일반적으로 사용되는 서비스 메쉬 기술입니다.

## 예제

일반적으로 사용되는 서비스 디스커버리 인프라스트러쳐 툴입니다.

- [etcd](https://etcd.io)
- [Consul](https://www.consul.io)
- [Apache Thrift](https://thrift.apache.org)
- [Apache Zookeeper](https://zookeeper.apache.org)

# SLA, SLO, SLI

간단히 SLA, SLO, SLI에 대해 알아보겠습니다. 대부분 사업과 사이트 안정성 관련된 것들이지만 알아두면 좋을 내용입니다.

## 중요한 이유

SLA나 SLO, SLI는 유저들을 위해 만들어진 회사의 서비스에 대한 약속들을 정의하고 추적하며 살필 수 있게 합니다. SLA와 SLO, SLI 모두 팀이 각자의 서비스에서 지속적인 장애 관리 및 응답 체계에서의 개선에 대한 강조를 더해 유저의 신뢰를 얻을 수 있게 도와줍니다.

## SLA

서비스 수준 협약(Service Level Agreement)이라고도 하는 SLA는 주어진 서비스에 대한 회사와 사용자 간의 계약입니다. SLA는 서비스 가용성과 같은 특정 지표와 관련하여 회사가 사용자에게 하는 약속을 정의합니다.

_SLA는 때때로 회사의 사업 또는 법무 조직이 작성합니다._

## SLO

서비스 수준 목표(Service Level Objective)라고도 하는 SLO는 장애 응답이나 활성 시간과 같은 특정 지표와 관련하여 회사가 사용자에게 하는 약속입니다. SLO는 SLA 전체 계약 내에서 개별적인 약정으로 존재합니다. SLO는 SLA를 만족하기 위해 서비스가 반드시 지켜야 할 구체적인 목표입니다. SLO는 목표가 지켜지는지 확인할 수 있도록 언제나 간단 명료하며 쉽게 측정이 가능해야 합니다.

## SLI

서비스 수준 지표(Service Level Indicator)라고도 하는 SLI는 SLO가 지켜졌는지 판별하는 데 사용하는 핵심 지표입니다. SLO에 나타난 지표의 측정된 값이 바로 SLO입니다. SLI의 값은 SLA를 준수하기 위해 언제나 SLO에 명시한 값과 같거나 초과해야만 합니다.

# 재해 복구

재해 복구(Disaster Discovery, DR)는 자연 재해와 사이버 공격 또는 사업 중단과 같은 사건 이후 인프라스트럭쳐의 접근과 기능을 되찾는 과정입니다.

재해 복구는 재난에 영향을 받지 않는 위치에 있는 오프 프레미스(off-premise)환경의 데이터 복제 및 컴퓨터 프로세싱에 의존합니다. 서버가 재난으로 내려가게 되면 업무는 데이터가 백업되어 있는 두 번째 장소에서 잃어버린 데이터를 복구해야 합니다. 이상적으로 조직에서는 컴퓨터 프로세싱도 해당 원격 장소로 전송하여 작업이 계속되게 할 수 있습니다.

_재해 복구는 종종 시스템 디자인 면접에서 논의되는 주제는 아니지만 이 주제에 대해 기본적인 이해를 하고 있는 것이 중요합니다. 재해 복구에 대해 더 알아보려면 [AWS Well-Architected Framework](https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html)를 참고할 수 있습니다._

## 재해 복구가 중요한 이유

재해 복구를 통해 아래와 같은 이점을 가질 수 있습니다:

- 중단과 다운타임을 최소화합니다.
- 손실을 제한합니다.
- 빠른 복구
- 더 나은 소비자 리텐션

## 용어

재해 복구와 관련된 중요한 용어들을 알아보겠습니다:

![disaster-recovery](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/disaster-recovery/disaster-recovery.png)

### RTO

복구 시간 목표(Recovery Time Objective)는 서비스가 중단된 때로부터 복구된 때까지의 납득 가능한 최대 지연 시간을 말합니다. 서비스가 사용 가능하지 않은 경우 어느 정도가 받아들여질 만한 시간 간격인지를 정합니다.

### RPO

복구 시점 목표(Recovery Point Objective)는 최근 데이터 복구 시점으로부터 납득 가능한 최대 시간을 의미합니다. 최근 데이터 복구 포인트와 서비스 중단 사이의 받아들여질 만한 데이터 손실이 어느정도까지 가능한지를 정합니다.

## 전략

다양한 재해 복구 전략들이 재해 복구 계획의 일부가 될 수 있습니다.

### 백업

재해 복구의 가장 간단한 유형이며 데이터를 떨어진 곳이나 제거 가능한 드라이브에 저장하는 것입니다.

### 콜드 사이트(Cold site)

조직은 이 재해 복구 전략에서 기본적인 인프라스트럭쳐를 두 번째 위치에 설정합니다.

### 핫 사이트(Hot site)

핫 사이트는 항상 최신의 데이터 카피를 유지합니다. 핫 사이트는 콜드 사이트보다 설정하는 데 시간과 돈이 더 많이 들지만 다운타임을 극적으로 줄여줍니다.

# 가상머신과 컨테이너

가상화와 컨테이너화에 대해 알아보기 전에 가상머신과(VMs) 컨테이너에 대해 배우는 시간을 가져보겠습니다.

## 가상머신 (Virtual Machine, VM)

가상머신은 각자의 물리 하드웨어 시스템에 만들어진 CPU, 메모리, 네트워크 인터페이스, 스토리지를 가진 가상 컴퓨터처럼 동작하는 가상 환경입니다. 하이퍼바이저라고 부르는 소프트웨어가 하드웨어 리소스를 머신과 분리하여 VM에서 사용할 수 있도록 적절히 제공합니다.

VM은 시스템의 다른 부분과 격리되어 있으며 마치 서버와 같이 하드웨어 하나에 여러 VM이 존재하는 것도 가능합니다. VM은 리소스의 요구사항이나 효율적인 사용을 위해 호스트 서버들 간에 이동할 수도 있습니다.

### 하이퍼바이저란?

하이퍼바이저(Hypervisor)는 간혹 가상 머신 모니터(Virtual Machine Monitor, VMM)이라고도 부르는데, 운영체제와 리소스를 가상 머신과 격리하고 VM의 생성과 관리를 하는 역할을 합니다. 하이퍼바이저는 리소스를 CPU나 메모리, 저장소를 리소스 풀처럼 다루어 게스트 또는 새로운 가성 머신 사이에 쉽게 재배치될 수 있게 합니다.

### 가상 머신을 사용하는 이유

VM을 사용하는 가장 큰 이유는 서버 통합(server consolidation)때문입니다. 대다수의 운영 체제와 어플리케이션 배포는 가용 가능한 작은 양의 물리 리소스만을 사용합니다. 서버들을 가상화하여 각 물리 서버에 많은 가상 서버들을 배치하여 하드웨어 활용도를 높일 수 있습니다. 이를 통해 추가적인 물리 리소스를 구매해야 할 필요를 없애버립니다.

VM은 나머지 시스템과 격리된 환경을 제공하므로 VM에서 무엇이 돌아가건 호스트 하드웨어에서 돌아가는 어떤것에도 영향을 미치지 않습니다. VM이 격리되어 있기 때문에 새로운 어플리케이션을 테스트해보거나 프로덕션 환경을 구축하는 경우 VM을 사용하는 것이 좋은 선택입니다. 또한 특정 사용례를 위한 단일 목적 VM을 돌리는 것도 가능합니다.

## 컨테이너

컨테이너는 코드와 런타임의 특정 버전이나 라이브러리와 같은 의존성들을 모두 패키징하여 어플리케이션이 어느 환경에서건 안정적으로 실행되게 하는 표준 단위입니다. 컨테이너는 논리적 패키징 메커니즘을 제공하여 어플리케이션이 실제로 동작하는 환경에서 한 단계 추상화됩니다. 이런 디커플링은 컨테이너 기반 어플리케이션들이 목표 환경과 상관없이 간단하고 안정적으로 배포될 수 있게 합니다.

### 컨테이너가 필요한 이유

컨테이너를 사용하는 장점을 알아보겠습니다.

**책임의 분리**

컨테이너화는 개발자가 어플리케이션의 로직과 의존성에만 집중할 수 있고, 운영 팀은 배포와 관리에만 집중할 수 있게 하여 명확한 책임의 분리를 제공합니다.

**포터블 워크로드**

컨테이너는 가상적으로 어디에서나 동작하기 때문에 개발과 배포가 매우 편해집니다.

**어플리케이션 격리**

컨테이너는 운영 체제 단계에서 CPU와 메모리, 저장소 및 네트워크 리소스를 가상화하기 때문에 개발자에게 다른 어플리케이션들과는 논리적으로 격리된 운영체제 뷰를 제공합니다.

**애자일한 개발**

컨테이너는 의존성과 환경에 대한 염려를 피하기 때문에 개발자가 더욱 기민하게 행동할 수 있게 합니다.

**효율적인 운영**

컨테이너는 가볍고 필요한 컴퓨팅 리소스만큼만 이용할 수 있습니다.

## 가상화 vs 컨테이너화

![virtualization-vs-containerization](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/virtual-machines-and-containers/virtualization-vs-containerization.png)

하이퍼바이저는 기존의 가상화에서 물리 하드웨어를 가상화합니다. 그 결과 가상 머신에는 각각 게스트 OS와 OS가 실행되기 위해 필요한 하드웨어의 가상 복제본, 어플리케이션 및 라이브러리와 의존성들이 들어갑니다.

컨테이너는 하위 하드웨어를 가상화하는 대신 운영 체제를 가상화하여 컨테이너 각각이 어플리케이션과 그 의존성들만 가질 수 있어서 VM보다 훨씬 가볍습니다. 컨테이너는 OS 커널과 VM이 요구하는 메모리 영역을 공유합니다.

# OAuth 2.0과 OpenID Connect (OIDC)

## OAuth 2.0

오픈 인증(Open Authorization)이라는 뜻의 OAuth 2.0 유저의 인증정보를 공유할 필요 없이도 유저를 대표하여 리소스에 대한 허가된 접근을 제공하는 표준입니다. OAuth 2.0은 인가(authorization) 프로토콜이지 인증(authentication) 프로토콜이 아니기 때문에 원격 API나 유저 데이터와 같은 리소스 집합에 대해 접근을 허가하는 것을 우선순위로 하여 설계되었습니다.

### 개념

OAuth 2.0 프로토콜은 아래 개체들을 정의합니다:

- **리소스 주인(Resource Owner)**: 보호된 리소스를 가지고 있고 리소스로의 접근을 허가하는 유저 또는 시스템입니다.
- **클라이언트**: 보호된 리소스에 접근을 필요로 하는 시스템입니다.
- **허가 서버**: 클라이언트에게서 액세스 토큰을 위한 요청을 받고 성공한 인증과 리소스 주인의 인가 아래 토큰을 발급합니다.
- **리소스 서버**: 유저의 리소스를 보호하고 클라이언트에게서 접근 요청을 받는 서버입니다. 클라이언트에겟 액세스 토큰을 받고 검증한 다음 적절한 리소스를 제공합니다.
- **스코프**: 스코프는 리소스에 대해 부여될 접근 권한에 대한 이유를 명시합니다. 납득할 만한 스코프 값과 관련된 리소스들은 리소스 서버에 의존합니다.
- **액세스 토큰**: 최종 유저를 대표하여 리소스에 대한 접근이 허가되었음을 알려주는 데이터 조각입니다.

### OAuth 2.0이 동작하는 방법

OAuth 2.0이 동작하는 방법을 알아보겠습니다:

![oauth2](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/oauth2-and-openid-connect/oauth2.png)

1. 클라이언트가 인가 서버에 클라이언트 id와 시크릿을 식별 정보로 하여 허가를 요청합니다. 또한 스코프와 액세스 토큰과 인가 코드를 받을 엔드포인트 URI를 제공합니다.
2. 인가 서버는 클라이언트를 인가하고 요청한 스코프가 허용되었는지 검증합니다.
3. 리소스 주인이 인가 서버와 통신하여 접근을 허가합니다.
4. 인가 서버는 인가 유형에 따라 클라이언트에 인가 코드 또는 액세스 코드를 돌려줍니다. 갱신 토큰 또한 제공할 수 있습니다.
5. 클라이언트는 액세스 토큰을 사용하여 리소스 서버에 리소스 접근을 요청할 수 있습니다.

### 단점

OAuth 2.0의 가장 일반적인 단점은 아래와 같습니다:

- 내장된 보안 기능이 없음
- 표준 구현이 없음
- 스코프의 표준 집합이 없음

## OpenID Connect

OAuth 2.0은 어플리케이션의 데이터와 기능을 다른 곳에서 사용할 수 있도록 접근을 인가하는 것만을 위해 설계되었습니다. OpenID Connect (OIDC)는 OAuth 2.0 바로 위에 로그인 및 누가 로그인했는지 프로필 정보가 추가된 얇은 계층입니다.

인가 서버가 OIDC를 지원하면 리소스 주인의 정보를 클라이언트에 제공하기 때문에 종종 ID 제공자(Identity Provider, IdP)라고 부르기도 합니다. OpenID Connect는 상대적으로 새것이기 때문에 OAuth에 비해 채택이 덜 되고 산업 구현의 모범 사례도 적습니다.

### 개념

OIDC 프로토콜은 아래 개체들을 정의합니다:

- **신뢰 당사자**: 현재 어플리케이션
- **OpenID 제공자**: 본질적으로 신뢰 당사자에게 원타임 코드를 제공하는 중간 서비스.
- **토큰 엔드포인트**: 원타임 코드(One-time Code, OTC)를 받아 한 시간동안 사용 가능한 액세스 코드를 제공하는 웹서버입니다. 토큰이 JSON 웹 토큰(JSON Web Token, JWT)으로 제공된다는 점이 OIDC가 OAuth 2.0과 가장 다른 부분입니다.
- **사용자 정보 엔드포인트**: 신뢰 당사자가 이 엔드포인트와 통신하면서 보안 토큰을 제공하고 최종 유저에 대한 정보를 받습니다.

OAuth 2.0과 OIDC 모두 구현하기 쉽고 대부분의 웹과 모바일 어플리케이션이 지원하는 JSON 기반입니다. 하지만 OpenID Connect 스펙은 기본 OAuth보다 더 엄격합니다.

# 싱글 사인온

싱글 사인온 (Single Sign-On, SSO)는 사용자가 한 가지 로그인 자격증명만으로 여러 어플리케이션이나 웹사이트에 접근을 제공하는 인증 과정입니다. 사용자가 다른 어플리케이션에 따로 로그인하지 않아도 되게 합니다.

ID 제공자(IdP)라고 하는 중앙화된 시스템이 사용자 자격 증명이나 다른 식별 정보들 보관하고 관리합니다. ID 제공자는 다른 웹사이트와 어플리케이션에 접근을 제공하는 신뢰된 시스템입니다.

싱글 사인온(SSO) 기반 인증 시스템은 직원이 조직의 여러 어플리케이션에 접근해야 하는 엔터프라이즈 환경에서 일반적으로 사용됩니다.

## 컴포넌트

싱글 사인온의 핵심 컴포넌트들을 알아보겠습니다.

### ID 제공자 (IdP)

사용자 식별 정보는 ID 제공자(IdP)라고 부르는 중앙화된 시스템이 저장하고 관리합니다. ID 제공자는 사용자를 인가하고 서비스 제공자에게로의 접근을 제공합니다.

ID 제공자는 사용자 이름과 비밀번호를 검증하거나 별도의 ID 제공자가 공급하는 식별자를 검증하는 식으로 사용자를 직접 인가합니다. ID 제공자는 서비스 제공자가 사용자를 관리할 필요가 없도록 사용자 식별자를 관리하는 역할을 맡습니다.

### 서비스 제공자 (Service Provider)

서비스 제공자는 최종 사용자에게 서비스를 제공합니다. 사용자의 신원 확인을 위해서 ID 제공자에 의존하며 대개 사용자의 몇몇 특성 역시 ID 제공자가 관리합니다. 서비스 제공자는 사용자의 각 서비스에 유일한 사용자 특성을 위해 로컬 계정을 관리하기도 합니다.

### ID 브로커 (Identity Broker)

ID 브로커는 여러 서비스 제공자와 다양한 다른 ID 제공자를 연결해주는 중개자로 동작합니다. ID 브로커를 사용하여 어플리케이션에 따른 프로토콜을 신경써야 하는 귀찮음 없이 싱글 사인온을 수행할 수 있습니다.

## SAML

보안 검증 마크업 언어(Security Assertion Markup Language, SAML)은 클라이언트들이 다른 시스템들에 신원이나, 인가, 권한과 같은 보안 정보를 공유하는 공개 표준입니다. SAML은 데이터 공유에 XML 표준을 사용하여 구현되었습니다.

특히 SAML은 신원 연합을 가능하게 하여 ID 제공자들이 경계 없고 안전하게 인가된 신원 정보와 그 속성들을 서비스 제공자에게 전달할 수 있습니다.

## SSO가 동작하는 방식

싱글 사인온이 동작하는 방법에 대해 알아보겠습니다:

![sso](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/single-sign-on/sso.png)

1. 사용자가 사용하고자 하는 어플리케이션에 리소스를 요청합니다.
2. 어플리케이션은 인가를 위해 ID 제공자로 사용자를 리디렉션합니다.
3. 사용자는 인증 정보로 로그인합니다 (대개 id와 패스워드).
4. ID 제공자는 클라이언트 어플리케이션에 싱글 사인온 응답을 돌려줍니다.
5. 어플리케이션이 사용자에게 접근을 허가합니다.

## SAML vs OAuth 2.0 및 OpenID Connect (OIDC)

SAML이나 OAuth, OIDC는 많이 다릅니다. SAML은 메시징에 XML을 사용하고 OAuth와 OIDC는 JSON을 사용합니다. SAML이 엔터프라이즈 보안에 맞춰져 있다면 OAuth는 비슷한 경험을 제공합니다.

OAuth와 OIDC는 확장적으로 RESTful 통신을 사용합니다. 그렇기 때문에 모바일이나 현대 웹 어플리케이션들이 더 나은 사용자 경험을 위해 OAuth와 OIDC를 찾게 됩니다. 그 반면에 SAML은 브라우저에 세션 쿠키를 남겨 사용자가 특정 웹 페이지에 접근할 수 있게 합니다. 짧은 워크로드에는 매우 좋습니다.

OIDC는 개발자 친화적이며 구현하기 간편하여 구현될 수 있는 사용 사례가 많아집니다. 복붙으로 구현하기도 매우 빠르며 자유롭게 사용할 수 있는 범용 프로그래밍 언어로 작성된 라이브러리들이 있습니다. SAML은 설치 및 유지보수가 복잡하기 때문에 엔터프라이즈급 회사들만이 제대로 사용할 수 있습니다.

본질적으로 OpenID Connect는 OAuth 프레임워크 위에 있는 계층입니다. 따라서 사용자에게 어떤 서비스 제공자에게 접근할 지 동의를 요청하는 내장된 레이어를 사용할 수 있습니다. SAML 역시 동의 흐름을 만들 수 있지만 프로토콜 단에서 지원하는 것이 아니라 개발자가 직접 하드코딩을 해야 가능합니다.

_두 인가 프로토콜 모두 각자 하는 일에서는 좋습니다. 언제나 그랬듯이 많은 것들이 사용 사례와 타겟 사용자에 따라 달라집니다._

## 장점

싱글 사인온의 혜택은 아래와 같습니다:

- 사용자가 한 가지 인증 정보를 기억하면 되므로 사용하기 쉽습니다.
- 긴 인증 절차를 거치지 않아도 되므로 접근이 간편합니다.
- 보안과 규정 준수를 강제하여 민감한 데이터를 보호합니다.
- IT 지원과 관리 시간을 줄여 관리를 단순화합니다.

## 단점

싱글 사인온으 단점은 아래와 같습니다:

- 단일 암호 취약점, 핵심 SSO 암호가 노출된 경우 모든 지원 어플리케이션들이 노출됩니다.
- 싱글 사인온을 사용하는 인증 과정은 매번 SSO 제공자에게 검증을 요청해야 하므로 기존 인증 방법보다 느립니다.

## 예

아래는 일반적으로 사용되는 ID 제공자(IdP)들입니다:

- [Okta](https://www.okta.com)
- [Google](https://cloud.google.com/architecture/identity/single-sign-on)
- [Auth0](https://auth0.com)
- [OneLogin](https://www.onelogin.com)

# SSL, TLS, mTLS

SSL, TLS, mTLS와 같은 중요한 보안 통신 프로토콜에 대해 간단히 다뤄보겠습니다. 여기서는 시스템 설계 관점에서 _"큰 그림"_ 만 보기 때문에 그리 중요하지는 않지만 알아두면 좋습니다. 

## SSL

SSL은 보안 소켓 계층(Secure Sockets Layer)의 약자이며 인터넷에서 일어나는 통신들을 암호화하고 보호하는 프로토콜을 지칭합니다. 1995년에 처음 개발되었지만 TLS(Transport Layer Security)가 대체하여 폐기되었습니다.

### SSL이 폐기되었음에도 SSL 인증서라고 하는 이유

대부분의 주요 인증서 제공자들은 인증서들을 여전히 SSL 인증서라고 부르며 이런 이름 규약이 존재하는 이유입니다.

### SSL이 중요한 이유

초창기 웹 데이터는 평문으로 전송되었기 때문에 메시지를 가로챈 사람은 누구나 읽을 수 있었습니다. SSL이 이 문제를 해결하고 사용자 개인정보를 보호하기 위해 만들어졌습니다. SSL은 사용자와 웹서버 간에 데이터를 암호화하여 경로에 간섭하는 공격자들을 막아 특정한 사이버 공격을 멈춥니다. 

## TLS

트랜스포트 계층 보안(Transport Layer Security), 또는 TLS는 인터넷을 통한 통신 프라이버시와 정보보호를 가능하게 하도록 설계된 널리 사용되는 보안 프로토콜입니다. TLS는 이전에 보안 소켓 계층(SSL)이라고도 불리던 암호 프로토콜에서 발전했습니다. TLS의 주요 사용례는 웹 어플리케이션과 서버 간의 통신을 암호화하는 것입니다.

TLS 프로토콜이 달성하는 세 가지 주요 컴포넌트가 있습니다:

- **암호화**: 서드파티에서 전송되는 데이터를 숨깁니다.
- **인증**: 정보를 요청하는 주체가 적합한지 검증합니다.
- **무결성**: 데이터가 조작되거나 간섭되었는지 검증합니다.

## mTLS

상호 TLS, 또는 mTLS라고도 부르는 이것은 상호 인증을 하는 방법입니다. mTLS는 각자 올바른 비밀키를 가지고 있는지 검증함으로 각 네트워크의 말단에 있는 주체가 누구인지를 확인합니다. 각각의 TLS 인증서에 들어있는 정보는 추가적인 검증을 제공합니다.

### mTLS를 사용하는 이유

mTLS는 트래픽이 클라이언트와 서버 간 양방향 트래픽 모두 안전하고 신뢰할 수 있음을 확인하도록 도와줍니다. 조직의 네트워크나 어플리케이션에 로그인하는 사용자에게 추가적인 보안 계층을 제공합니다. 또한 사물인터넷(IoT) 기기처럼 로그인 절차를 거치지 않는 클라이언트 기기의 연결도 확인할 수 있습니다.

mTLS는 최근 [제로 트러스트 보안 모델](https://en.wikipedia.org/wiki/Zero_trust_security_model)을 사용하는 마이크로서비스나 분산 시스템에서 각자를 확인하는 데 일반적으로 사용됩니다.

# 시스템 디자인 인터뷰

시스템 디자인은 굉장히 광범위한 주제이며 시스템 디자인 인터뷰는 여러분이 추상적인 문제에 대한 기술적인 해결책을 낼 수 있는지 평가하기 위해 기획된 것이지 어떤 정답을 요구하는 것이 아닙니다. 시스템 디자인 인터뷰에 대한 독특한 관점은 지원자와 면접관이라는 양방향 속성에서 나옵니다.

예측 역시 엔지니어링 수준에 따라 다릅니다. 실제 경력이 많은 사람은 신입 지원자와 매우 다른 접근 방법을 취할 것입니다. 그 결과 단 하나의 전략으로 규격화된 인터뷰를 할 수 있는 방법을 생각해 내는 것은 매우 어렵습니다.

시스템 디자인 인터뷰에서 사용할 수 있는 일반적인 전략들을 알아보겠습니다.

## 요구사항 확정

시스템 디자인 인터뷰 질문은 천성적으로 애매모호하거나 추상적입니다. 인터뷰 시작단계에서 문제의 정확한 범위를 물어보고 기능적인 요구사항을 명확히 하는 것이 필수입니다. 대개 요구사항은 세 가지로 나뉩니다.

### 기능적 요구사항

기능적 요구사항은 시스템이 제공하는, 최종 유저가 구체적으로 기본 기능으로써 요구하는 것입니다. 이 모든 기능들이 계약의 일부로 시스템에 필요 기능으로 포함되어야 합니다.

예를 들어 봅시다.

- 이 시스템에 어떤 기능을 설계해야 하는가?
- 이 설계에서 고려해야할 엣지 케이스가 있다면 무엇일까?

### 비기능적 요구사항

비기능적 요구사항은 프로젝트 계약을 지키기 위한 질적 제약사항들입니다. 어떤 지표들을 구현할지 그 우선순위와 범위는 프로젝트마다 다릅니다. 비행동적(non-behavioral) 요구사항이라고도 합니다. 예를 들어 이식성, 유지보수성, 안정성, 확장성, 보안 등등입니다.

예를 들어 봅시다.

- "각 요청이 최소 지연 시간 내에 처리되어야 한다"
- "시스템은 고가용성이어야 한다"

### 확장된 요구사항

보통 "있으면 좋은" 요구사항들이라 시스템의 범위를 벗어날 수도 있습니다.

예를 들어 봅시다:

- "이 시스템은 메트릭과 분석을 기록해야 합니다"
- "서비스 헬스나, 성능 모니터링은?"

## 추정 및 제약

설계할 시스템의 크기를 추정하는 것입니다. 아래와 같은 질문을 하는 것이 중요합니다:

- "이 시스템이 다뤄야 하는 목표 스케일이 어느 정도입니까?"
- "시스템의 읽기/쓰기 비율은 어떻게 됩니까?"
- "초당 몇 건의 요청이 있습니까?"
- "얼마나 많은 저장소가 필요합니까?"

이 질문들이 나중에 설계의 크기를 키우는 데 도움이 됩니다.

## 데이터 모델 설계

일단 추정이 되었다면 데이터베이스 스키마를 정의하는 것부터 시작할 수 있습니다. 인터뷰의 첫 단계에서 설계 데이터 모델 설계를 하면 모든 시스템의 핵심인 데이터의 흐름을 이해하는 데 도움이 될 것입니다. 이 단계에서 모든 개체들과 그 관계를 정의하게 됩니다.

- "이 시스템에서 다른 개체는 무엇입니까?"
- "이 개체들 간에 관계는 어떱니까?"
- "얼마나 많은 테이블이 필요합니까?"
- "여기서 NoSQL이 더 좋은 선택입니까?"

## API 설계

그 다음으로 시스템의 API 설계를 시작할 수 있습니다. API는 명시적으로 시스템이 무엇을 하기를 원하는지 정의하는 데 도움이 됩니다. 코드를 작성할 필요 없이 단지 파라미터나 함수, 클래스, 타입, 개체 등 API에 필요한 간단한 인터페이스만 정의하면 됩니다.

예를 들면 이렇습니다:

```tsx
createUser(name: string, email: string): User
```

가능한 인터페이스를 간단하게 유지하고 추가적인 요구사항이 있을 때 다시 보는 것이 좋습니다.

## 고수준 컴포넌트 설계

데이터 모델과 API 디자인을 마쳤으므로, 문제를 해결하는 데 필요한 (로드 밸런서나 API 게이트웨이와 같은) 시스템 컴포넌트를 정하고 첫 번째 시스템 디자인 초안을 만들어야 합니다.

- "모놀리식 또는 마이크로서비스 구조를 설계하는 것이 최선입니까?"
- "어느 타입의 데이터베이스를 사용해야 합니까?"

기본적인 다이어그램을 그렸다면 클라이언트의 관점에서 이 시스템이 어떻게 동작하는지 면접관과 이야기해볼 수 있습니다.

## 세부 설계

설계된 시스템의 주요 컴포넌트들의 세부사항을 볼 차례입니다. 언제나 그렇듯이 면접관과 함께 어느 컴포넌트가 추가적인 개선이 필요한지 논의해야 합니다.

여기가 여러분의 전문분야의 경험치를 보여줄 좋은 기회입니다. 다른 접근법, 장점 및 단점을 말해 보십시오. 여러분의 설계상 결정을 설명하고 예를 들어 주장을 뒷받침하십시오. 또한 이 때는 비록 선택일지라도 도움이 될 만한 추가적인 기능에 대해 이야기할 수 있는 좋은 기회입니다.

- "어떻게 데이터를 파티션할 수 있습니까?"
- "부하 분산에 대해 어떻게 생각하십니까?"
- "캐시를 사용할 수 있습니까?"
- "갑작스러운 트래픽 급증을 어떻게 처리할 수 있습니까?"

또한 "SQL 데이터베이스는 확장이 안 되지 때문에 NoSQL이 무조건 좋다고 생각합니다."와 같이 특정 기술에 너무 매몰되려 하지 않아야 합니다. 수년간 많은 사람들을 면접봤던 사람으로서, 저는 여러분이 아는 것과 알지 못하는 것에 대해 겸손해야 한다는 의견에 동의합니다. 여러분의 지식에 예제를 더하여 이 인터뷰 구간을 통과하시기 바랍니다.

## 병목을 특정하고 해결하기

마지막으로 병목을 찾고 병목을 완화하는 방법에 대해 의논할 차례입니다. 중요한 질문들이 있습니다:

- "충분한 데이터베이스 복제본이 있습니까?"
- "단일 실패점이 존재합니까?"
- "데이터베이스 샤딩이 필요합니까?"
- "더욱 활력있는 시스템을 만들려면 어떻게 해야 합니까?"
- "캐시의 가용성을 어떻게 더 나이지게 할 수 있습니까?"

여러분이 인터뷰에 들어가는 회사의 엔지니어링 블로그를 읽어 두십시오. 그들이 사용하는 기술 스택이 어떤지, 어떤 문제들이 그들에게 중요한지 느끼는 데 도움이 될 것입니다.

# URL 줄이기

URL 줄이기를 설계해 봅시다. 비슷한 서비스로는 [Bitly](https://bitly.com)와 [TinyURL](https://tinyurl.com/app)가 있습니다.

## URL 줄이기란

URL 줄이기(URL shortener)는 긴 URL에 대한 별칭 또는 짧은 URL을 만들어주는 서비스입니다. 사용자가 짧은 링크에 접속했을 때 원래 URL로 리다이렉션됩니다.

예를 들어 아래 긴 URL은 더 짧은 URL로 바뀝니다.

**긴 URL**: [https://karanpratapsingh.com/courses/system-design/url-shortener](https://karanpratapsingh.com/courses/system-design/url-shortener)

**짧은 URL**: [https://bit.ly/3I71d3o](https://bit.ly/3I71d3o)

## URL 줄이기가 필요한 이유

URL 줄이기는 일반적으로 URL을 공유할 때 공간을 절약합니다. 또한 사용자는 더 짧은 URL을 입력할 때 오타를 낼 가능성이 줄어듭니다. 게다가 기기 간 연결을 최적화해 개별 링크를 추적할 수 있게 해 줍니다.

## 요구사항

URL 줄이기 시스템은 아래 요구사항을 만족해야 합니다:

### 기능적 요구사항

- 서비스는 주어진 URL에 대해 _더 짧고 유니크한_ 별명 링크를 생성해야 합니다.
- 사용자는 짧은 URL을 방문했을 때 원래 URL로 리디렉션되어야 합니다.
- 링크는 일정 시간 이후 만료되어야 합니다.

### 비기능적 요구사항

- 최소 지연 시간으로 고가용성이어야 합니다.
- 시스템은 크기 확장이 가능해야 하며 효율적이어야 합니다.

### 확장 요구사항

- 서비스 남용을 방지하십시오.
- 리다이렉션에 대한 분석 및 메트릭을 기록하십시오.

## 추정 및 제한

추정 및 제약사항부터 시작해보겠습니다.

_참고: 면접관에게 어떤 크기 또는 트래픽 관련 가정이 있는지 확인해 봐야 합니다._

### 트래픽

읽기가 많은 시스템일 것이기 때문에 `100:1` 비율로 읽기 쓰기가 있을 것으로 가정하고 매달 1억개의 링크가 생긴다고 추정해 봅시다.

**월별 읽기/쓰기**

월별 읽기를 계산해 봅시다:

$$
100 \times 1 \space 억 = 100 \space 억/월
$$

비슷하게 쓰기도 계산해 봅니다:

$$
1 \times 1 \space 억 = 1 \space 억/월
$$

**시스템의 초당 요청(Requests Per Second, RPS)**

매월 1억 개의 요청은 초당 40개의 요청으로 생각할 수 있습니다.

$$
\frac{1 \space 억}{(30 \space 일 \times 24 \space 시간 \times 3600 \space 초)} = \sim 40 \space URLs/초
$$

그리고 `100:1` 읽기/쓰기 비율을 생각해 보면 전체 리디렉션 수는 아래와 같습니다:

$$
100 \times 40 \space URLs/초 = 4000 \space 요청/초
$$

### 대역폭

매 초 40개의 URL을 예상하고 있고 각 요청이 500바이트라고 했을때 쓰기 요청에 사용되는 전체 유입 데이터는 아래와 같습니다:

$$
40 \times 500 \space bytes = 20 \space KB/초
$$

비슷하게 읽기 요청의 경우 4천회의 리디렉션을 예상하기 때문에 전체 출력 데이터는 아래와 같습니다:

$$
4000 \space URLs/초 \times 500 \space bytes = \sim 2 \space MB/초
$$

### 저장소

저장소의 경우 데이터베이스에 링크 또는 레코드를 10년간 저장한다고 가정합니다. 매월 1억개의 요청이 예상되기 때문에 저장해야 할 전체 레코드 수는 아래와 같습니다:

$$
1 \space 억 \times 10\space 년 \times 12 \space 개월 = 120 \space 억
$$

비슷하게 저장된 레코드는 500바이트일 것으로 가정합니다. 그 결과 6TB 가량의 저장소가 필요하게 됩니다:

$$
120 \space 억 \times 500 \space bytes = 6 \space TB
$$

### 캐시

캐시의 경우 80/20의 법칙이라고도 알려진 고전적인 [파레토의 법칙](https://en.wikipedia.org/wiki/Pareto_principle)을 따를 것입니다. 그말인즉슨 80%의 요청이 20%의 데이터만 요구할 것이기 때문에 요청의 20%만 캐시해도 됩니다.

초당 4천개의 읽기 또는 리디렉션 요청이 올 것이기 때문에 결국 매일 3억 5천만개의 요청으로 생각할 수 있습니다.

$$
4000 \space URLs/초 \times 24 \space 시간 \times 3600 \space 초 = \sim 3 \space 억 \space 5000 \space 만 \space 요청/일
$$

따라서 매일 35GB의 메모리가 필요합니다.

$$
20 \space percent \times 3 \space 억 \space 5000 \space 만 \times 500 \space bytes = 35 \space GB/일
$$

### 고수준 추정

고수준에서 추정한 값은 아래와 같습니다:

| Type                 | Estimate   |
| -------------------- | ---------- |
| Writes (New URLs)    | 40/s       |
| Reads (Redirection)  | 4K/s       |
| Bandwidth (Incoming) | 20 KB/s    |
| Bandwidth (Outgoing) | 2 MB/s     |
| Storage (10 years)   | 6 TB       |
| Memory (Caching)     | ~35 GB/day |

## 데이터 모델 설계

그 다음으로 데이터 모델 설계에 초점을 맞춥니다. 여기 데이터베이스 스키마가 있습니다:

![url-shortener-datamodel](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/url-shortener/url-shortener-datamodel.png)

우선은 두 개의 테이블만으로 시작합니다.

**users**

`name`이나 `email`, `createdAt`과 같은 사용자의 세부사항을 저장합니다.

**urls**

`expiration`, `hash`, `originalURL`, 짧은 URL을 만든 유저의 이름인 `userID`와 같은 새로 만들어진 짧은 URL의 속성을 담습니다. 또한 `hash` 컬럼을 [색인](#색인)으로 사용하여 쿼리 성능을 올립니다.

### 어떤 데이터베이스를 사용해야 합니까?

데이터가 강하게 관계적이 아니기 때문에 [Amazon DynamoDB](https://aws.amazon.com/dynamodb)나 [Apache Cassandra](https://cassandra.apache.org/_/index.html) 또는 [MongoDB](https://www.mongodb.com)가 좋은 선택일 수 있습니다. SQL 데이터베이스를 사용한다고 결정했다면 [Azure SQL Database](https://azure.microsoft.com/en-in/products/azure-sql/database) 나 [Amazon RDS](https://aws.amazon.com/rds)를 사용할 수 있습니다.

_더 자세히 알아보려면 [SQL vs NoSQL](#sql-vs-nosql-데이터베이스)를 참고하세요._

## API 설계

서비스에 사용할 기본적인 API를 만들어 보겠습니다:

### URL 생성

이 API는 원시 URL을 넣어 이 시스템에 새로운 짧은 URL을 만듭니다.

```tsx
createURL(apiKey: string, originalURL: string, expiration?: Date): string
```

**파라미터**

apiKey (`string`): 사용자가 제공한 API 키.

originalURL (`string`): 짧게 만들 원시 URL.

expiration (`Date`): 새 URL의 만료 시간 _(선택)_.

**반환**

Short URL (`string`): 새 짧은 URL.

### URL 가져오기

이 API는 짧은 URL로 원시 URL을 가져옵니다.

```tsx
getURL(apiKey: string, shortURL: string): string
```

**파라미터**

apiKey (`string`): 사용자가 제공한 API 키

shortURL (`string`): 원시 URL에 매핑된 짧은 URL

**반환**

Original URL (`string`): 받아온 원시 URL

### URL 삭제

이 API는 시스템에 있는 짧은 URL을 제거합니다.

```tsx
deleteURL(apiKey: string, shortURL: string): boolean
```

**파라미터**

apiKey (`string`): 사용자가 제공한 API 키

shortURL (`string`): 삭제할 짧은 URL

**반환**

Result (`boolean`): 작업이 성공했는지 아닌지 나타냅니다.

### API키가 필요한 이유

위에서 눈치채셨을 수 있지만 서비스의 오남용을 막기 위해 API 키를 사용합니다. API키를 사용하여 사용자가 초당 또는 분당 보내는 요청수를 제한할 수 있습니다. API키는 개발자 API를 만들 때 꽤나 표준으로 사용되며 추가 요구사항에 대한 답이 됩니다.

## 고수준 설계

시스템의 고수준 설계를 해 봅시다.

### URL 인코딩

시스템의 첫 목표는 주어진 URL을 짧게 만드는 것입니다. 여러 접근 방법들을 살펴보겠습니다:

**Base62 접근법**

원시 URL을 A-Z까지의 대문자와 a-z까지의 소문자, 0-9의 숫자를 사용하는 [Base62](https://en.wikipedia.org/wiki/Base62)로 인코딩합니다.

$$
Number \space of \space URLs = 62^N
$$

여기서,

`N`: 생성된 URL에 사용되는 문자 수

따라서 7글자짜리 URL을 만든다면 3.5조개 가량의 다른 URL을 만들 수 있습니다.

$$
\begin{gather*}
62^5 = \sim 9.16 \space 억 \space URLs \\
62^6 = \sim 568 \space 억 \space URLs \\
62^7 = \sim 3.5 \space 조 \space URLs
\end{gather*}
$$

가장 간단한 해결방법이지만 키 중복이나 충돌이 없다고 보장할 수는 없습니다.

**MD5 접근법**

[MD5 메시지 다이제스트 알고리즘](https://en.wikipedia.org/wiki/MD5)은 128비트 해시값(또는 32자리 16진수)을 만드는 널리 사용되는 해시 함수입니다. 이 32자리 16진수를 사용하여 7글자 길이의 URL을 만들 수 있습니다.

$$
MD5(original\_url) \rightarrow base62encode \rightarrow hash
$$

하지만 중복과 충돌이라는 새로운 문제가 생깁니다. 유니크한 것이 나올 때까지 해시를 계산할 수 있지만 시스템의 오버헤드를 증가시킬 것입니다. 더 확장 가능한 접근 방법을 찾아보는 것이 좋습니다.

**카운터 접근법**

생성된 키의 수를 관리하는 서버 하나로 시작합니다. 서비스가 요청을 받으면 유니크한 숫자를 리턴하는 카운터에 접근해 카운터를 증가시킵니다. 다음 요청이 오면 유니크한 카운터 값을 반환하고 계속 반복합니다.

$$
Counter(0-3.5 \space trillion) \rightarrow base62encode \rightarrow hash
$$

이 접근법의 문제는 카운터가 단일 실패점이 된다는 점입니다. 카운터 인스턴스 여러 개를 실행한다면 본질적으로 분산 시스템이기 때문에 충돌 문제가 발생합니다.

이 문제를 해결하기 위해 [Zookeeper](https://zookeeper.apache.org)와 같은 분산 동기화를 제공하는 분산 시스템 관리자를 사용할 수 있습니다. Zookeeper는 여러 숫자 범위를 관리할 수 있게 해 줍니다.

$$
\begin{align*}
& Range \space 1: \space 1 \rightarrow 1,000,000 \\
& Range \space 2: \space 1,000,001 \rightarrow 2,000,000 \\
& Range \space 3: \space 2,000,001 \rightarrow 3,000,000 \\
& ...
\end{align*}
$$

서버가 최대 범위에 도달하면 주키퍼가 아직 사용하지 않는 카운터 범위를 새 서버에 할당합니다. 이 방법은 중복되지 않고 충돌도 회피하는 URL을 보장합니다. 또한 여러 Zookeeper 인스턴스를 실행하여 단일 실패점을 없앨 수 있습니다.

### 키 생성 서비스 (Key Generation Service, KGS)

앞서 말했듯이 거대 환경에서 중복이나 충돌 없이 유니크한 키를 생성하는 것은 도전이 됩니다. 이 문제를 해결하기 위해 미리 유니크한 키를 만들어 두고 나중을 위해 별도 데이터베이스에 저장해 두는 독립적인 키 생성 서비스(KGS)를 만들어 볼 수 있습니다. 이 방법은 우리의 문제를 간단하게 만들 수 있습니다.

**동시 접근들을 다루는 방법**

키가 사용되면 더 이상 사용하지 않도록 데이터베이스에 기록합니다. 하지만 여러 서버 인스턴스가 동시에 데이터를 읽어들이고 있기 때문에 둘 또는 그 이상의 서버들이 같은 키를 사용할 수 있습니다.

가장 쉬운 해결 방법은 두 개의 테이블에 키를 저장하는 것입니다. 키가 사용되자마자 적절한 락을 사용하여 키를 별도의 테이블로 옮기는 것입니다. 또한 읽기 속도를 향상하기 위해 메모리에 키 일부를 보관할 수 있습니다.

**KGS 데이터베이스 추산**

이대로라면 ~568억개의 유니크한 6글자짜리 키를 생성하여 300GB의 용량을 사용하게 될 것입니다.

$$
6 \space characters \times 568 \space 억 = \sim 390 \space GB
$$

간단한 예제에 390GB라는 크기가 엄청 커 보일 수 있지만 전체 서비스 라이프타임을 위한 것이고 키 데이터베이스 크기는 메인 데이터베이스 크기를 키우지 않을 것이라는 점을 기억하시기 바랍니다.

### 캐싱

[캐싱](#캐싱)에 대해 생각해 보겠습니다. 예측에 따라 서비스의 요청 20%을 캐시하기 위해 매일 ~35GB 가량의 메모리가 필요합니다. 이 경우 [Redis](https://redis.io)나 [Memcached](https://memcached.org)를 API서버와 함께 사용할 수 있습니다.

_더 자세히 알아보려면 [caching](#캐싱) 항목을 참고하십시오._

### 설계

핵심 컴포넌트들을 특정지었으니 시스템 디자인의 첫 번째 초안을 만들어 보겠습니다.

![url-shortener-basic-design](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/url-shortener/url-shortener-basic-design.png)

이렇게 동작합니다:

**새 URL 만들기**

1. 사용자가 새 URL을 만들 때 API 서버가 키 생성 서비스(KGS)에 새 키를 요청합니다.
2. 키 생성 서비스가 유니크 키를 API 서버에 제공하고 키를 사용된 것으로 표시합니다.
3. API 서버가 새 URL을 데이터베이스에 작성하고 캐시합니다.
4. 서비스는 사용자에게 HTTP 201 (Created) 응답을 전달합니다.

**URL에 접근하기**

1. 클라이언트가 특정 짧은 URL에 접근하면 API 서버로 요청이 보내집니다.
2. 요청이 캐시 히트하거나 미스가 나서 데이터베이스에서 가져온 경우 HTTP 301 (Redirect)를 보내 원시 URL로 가게 합니다.
3. 데이터베이스에도 키가 없다면 사용자에게 HTTP 404 (Not found) 에러를 보냅니다.

## 세부 디자인

더 자세한 디자인에 대해 이야기해볼 시간입니다.

### 데이터 파티션

데이터베이스를 수평 확장하려면 데이터를 파티션해야 합니다. ([샤딩(Sharding)](https://karanpratapsingh.com/courses/system-design/sharding)이라고도 하는) 수평적 파티션이 좋은 시작일 겁니다. 아래와 같은 파티션 전략을 사용해볼 수 있습니다:

- 해시 기반 파티션
- 리스트 기반 파티션
- 범위 기반 파티션
- 복합 파티션

위의 방법을 사용하더라도 여전히 불균형한 데이터 및 부하 분산이 생기지만 [일관된 해싱](https://karanpratapsingh.com/courses/system-design/consistent-hashing)으로 해결할 수 있습니다.

_더 알아보고 싶으시면 [샤딩](#샤딩) 항목과 [일관된 해싱](#일관된-해싱) 항목을 참고하세요._

### 데이터베이스 정리

정리는 서비스의 유지보수 단계에 가깝고 만료된 엔트리들을 보관할지 제거할지에 따라 달라집니다. 만료된 엔트리를 제거하기로 결정했다면 두 가지 다른 방법으로 접근할 수 있습니다.

**능동적 정리(Active cleanup)**

능동적 정리는 주기적으로 저장소와 캐시에서 만료된 링크를 제거하는 별도의 정리 서비스를 실행하는 것입니다. 아마 [cron job](https://en.wikipedia.org/wiki/Cron)과 같은 가벼운 서비스가 될 겁니다.

**수동적 정리(Passive cleanup)**

수동적 정리는 유저가 만료된 링크에 접근하려고 할 때 엔트리를 지우는 것입니다. 데이터베이스와 캐시가 게으른 정리(lazy cleanup)를 하게 됩니다.

### 캐시

[캐싱](#캐싱)에 대해 이야기해보겠습니다.

**캐시 축출 전략**

이전에 살펴봤듯이 [Redis](https://redis.io)나 [Memcached](https://memcached.org)와 같은 솔루션을 사용하여 매일 20%가량의 트래픽을 캐시한다고 했었습니다. 하지만 어떤 캐치 축출 전략이 이 문제의 요구사항에 가장 적합할까요?

[Least Recently Used (LRU)](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU))가 좋은 정책이 될 수 있습니다. LRU는 가장 적게 사용되는 키를 우선적으로 폐기합니다.

**캐시 미스 처리**

캐시 미스가 발생하면 서버는 데이터베이스에 접근해 새 엔트리를 캐시에 업데이트합니다.

### 지표 및 분석

분석 및 지표를 기록하는 것은 확장적 요구사항입니다. 방문자의 국가나 플랫폼, 조회수 등과 같은 메타데이터를 URL 엔트리와 함께 데이터베이스에 저장할 수 있습니다.

### 보안

보안을 위해 개인용 URL과 인가를 추가할 수 있습니다. 별도의 테이블을 사용하여 특정 URL에 접근 권한이 있는 사용자 id를 저장합니다. 사용자가 필요한 권한을 가지고 있지 않으면 HTTL 401(Unauthroized) 에러를 반환합니다.

[API Gateway](https://karanpratapsingh.com/courses/system-design/api-gateway)에도 인가나 속도 제한, 부하 분산과 같은 기능이 제공되기 때문에 이용해볼 수 있습니다.

## 병목을 특정하고 해결하기

![url-shortener-advanced-design](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/url-shortener/url-shortener-advanced-design.png)

이 설계에서 단일 실패점과 같은 병목을 찾아내고 해결해 봅시다.

- "만약 API 서비스나 키 생성 서비스가 충돌한다면?"
- "컴포넌트 간 트래픽을 어떻게 분산할 것인가?"
- "데이터베이스 부하를 어떻게 줄일 수 있을까?"
- "KGS가 사용하는 키 데이터베이스가 떨어진다면?"
- "캐시의 가용성을 개선하려면?"

시스템을 더욱 탄력적으로 만들기 위해 아래와 같이 해 볼 수 있습니다:

- 여러 서버 인스턴스와 키 생성 서비스를 실행합니다.
- 클라이언트와 서버, 데이터베이스 및 캐시 서버에 [로드 밸런싱](#로드-밸런싱)을 도입합니다.
- 읽기가 많은 시스템이기 때문에 데이터베이스에 여러 개의 읽기 레플리카를 사용합니다.
- 키 데이터베이스가 떨어질 것을 대비한 레플리카를 준비합니다.
- 분산 캐시에 여러 인스턴스와 레플리카를 사용합니다.

# WhatsApp

[WhatsApp](https://whatsapp.com)이나 [Facebook Messenger](https://www.messenger.com), [WeChat](https://www.wechat.com) 같은 인스턴트 메시징 서비스를 설계해 보겠습니다.

## WhatsApp이란

WhatsApp은 사용자들에게 인스턴스 메시징 서비스를 제공하는 채팅 어플리케이션입니다. 세계에서 가장 많이 사용되는 모바일 어플리케이션 중 하나이며 180개 이상의 국가에서 20억 명이 넘는 사용자를 연결합니다. WhatsApp은 웹으로도 이용 가능합니다.

## 요구사항

이 시스템은 아래와 같은 요구사항을 만족해야 합니다:

### 기능적 요구사항

- 1:1 채팅 지원
- 그룹 채팅 (최대 100명)
- 파일 공유도 지원해야 함 (이미지, 비디오 등)

### 비기능적 요구사항

- 최소 지연 시간으로 고가용성
- 크기 확장이 가능해야 하며 효율적이어야 함

### 확장적 요구사항

- 메시지 전송, 송신, 수신 처리
- 사용자의 최근 이용 시간 보여주기
- 푸시 알림

## 추정 및 제약사항

추정 및 제약사항부터 시작해보겠습니다.

_참고: 면접관에게 미리 스케일 또는 트래픽 관련된 가정사항들을 확인해 두어야 합니다._

### 트래픽

5천만 명의 일일 활성 사용자(DAU)가 있다고 하고 각 사용자는 하루에 둘 이상의 사람에게 적어도 10개 이상의 메시지를 보낸다고 가정합니다. 계산해보면 매일 20억 건의 메시지가 나옵니다.

$$
5 \space 천만 \times 20 \space 메시지 = 20 \space 억/일
$$

또한 메시지는 이미지나 비디오, 그 외 파일들과 같은 미디어를 포함할 수 있습니다. 전체의 5% 가량의 메시지가 사용자가 공유하는 미디어 파일이라고 가정하면 추가로 2억개의 파일을 추가적으로 저장해야 합니다.

$$
5 \space 퍼센트 \times 20 \space 억 = 2 \space 억/일
$$

**시스템의 초당 요청(Requests Per Second, RPS)수**

일일 2억건의 요청은 초당 2.4만개의 요청에 해당합니다.

$$
\frac{20 \space 억}{(24 \space 시간 \times 3600 \space 초)} = \sim 2.4 \space 만 \space 요청/초
$$

### 저장소

각 메시지가 평균 100바이트라고 가정하면 대략 매일 200GB정도의 데이터베이스 저장소가 필요합니다.

$$
20 \space 억 \times 100 \space bytes = \sim 200 \space GB/일
$$

요구사항에 따라 하루 전체 메시지 중 5퍼센트(1억건)가 미디어 파일입니다. 평균 파일 크기가 50KB라면 매일 10TB의 저장소가 필요하게 됩니다.

$$
1 \space 억 \times 100 \space KB = 10 \space TB/일
$$

그리고 10년간 저장소가 38PB 가량 필요하게 됩니다.

$$
(10 \space TB + 0.2 \space TB) \times 10 \space 년 \times 365 \space 일 = \sim 38 \space PB
$$

### 대역폭

매일 10.2TB의 데이터 유입을 처리해야 하기 때문에 최소 초당 120MB의 대역폭이 필요합니다.

$$
\frac{10.2 \space TB}{(24 \space 시간 \times 3600 \space 초)} = \sim 120 \space MB/초
$$

### 고수준 추정

고수준에서 추정해본 값들입니다.

| Type                      | Estimate   |
| ------------------------- | ---------- |
| Daily active users (DAU)  | 50 million |
| Requests per second (RPS) | 24K/s      |
| Storage (per day)         | ~10.2 TB   |
| Storage (10 years)        | ~38 PB     |
| Bandwidth                 | ~120 MB/s  |

## 데이터 모델 설계

요구사항을 만족하는 일반적인 데이터 모델은 이렇습니다.

![whatsapp-datamodel](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/whatsapp/whatsapp-datamodel.png)

테이블은 아래와 같습니다:

**users**

`name`이나 `phoneNumber`과 같은 사용자의 정보를 담고 있습니다.

**messages**

이름에서 알 수 있듯이 메시지를 저장하며 그 외에 `type` (텍스트, 이미지, 비디오 등)이나 타임스탬프와 같이 메시지 전달에 필요한 속성들이 있습니다. 또한 메시지는 해당하는 `chatID`나 `groupID`가 있습니다.

**chats**

기본적으로 두 사용자 간의 비공개 채팅을 나타내며 여러 메시지를 담고 있습니다.

**users_chats**

여러 사용자들이 여러 채팅방을 가질 수 있기 때문에 (N:M 관계) 유저와 채팅방, 채팅방과 유저의 매핑을 합니다.

**groups**

여러 사용자로 이루어진 그룹을 표현합니다.

**users_groups**

여러 사용자들이 여러 그룹의 일원이 될 수 있기 때문에 (N:M 관계) 유저와 그룹, 그룹과 유저의 매핑을 합니다.

### 데이터베이스 종류 선택

데이터 모델이 연관적으로 보이긴 하지만 모든 것들을 데이터베이스 하나에 저장할 필요는 없습니다. RDB는 확장성을 제한하기 때문에 병목이 되기 쉽습니다.

여기서는 서비스 간에 데이터를 나누어 각 서비스는 특정 테이블의 오너십을 가집니다. 그 다음 사용처에 따라 [PostgreSQL](https://www.postgresql.org)과 같은 연관 데이터베이스나 [Apache Cassandra](https://cassandra.apache.org/_/index.html)와 같은 분산 NoSQL 데이터베이스를 사용하겠습니다.

## API 설계

서비스의 기본적인 API 디자인을 해 보겠습니다.

### 채팅 또는 그룹 전체 가져오기

아래 API는 주어진 `userID`와 관련된 채팅방과 그룹 전체를 가져옵니다.

```tsx
getAll(userID: UUID): Chat[] | Group[]
```

**인자**

userID (`UUID`): 현재 사용자의 ID입니다.

**반환**

결과 (`Chat[] | Group[]`): 사용자가 참여한 모든 채팅방과 그룹입니다.

### 메시지 가져오기

주어진 `channelID`(채팅 또는 그룹 id)에서 사용자의 모든 메시지들을 가져옵니다.

```tsx
getMessages(userID: UUID, channelID: UUID): Message[]
```

**인자**

userID (`UUID`): 현재 사용자의 ID입니다.

channelID (`UUID`): 메시지를 가져올 채널(채팅 또는 그룹)의 ID입니다.

**반환**

메시지 (`Message[]`): 주어진 채팅이나 그룹의 전체 메시지입니다.

### 메시지 전송

사용자가 채널(채팅 또는 그룹)에 메시지를 보냅니다.

```tsx
sendMessage(userID: UUID, channelID: UUID, message: Message): boolean
```

**인자**

userID (`UUID`): 현재 사용자의 ID입니다.

channelID (`UUID`): 사용자가 메시지를 보내려고 하는 채널(채팅 또는 그룹)의 ID입니다.

message (`Message`): 유저가 보내려고 하는 메시지입니다(텍스트, 이미지, 비디오 등) 

**반환**

결과 (`boolean`): 작업이 성공 또는 실패를 나타냅니다.

### 그룹에 참가 또는 탈퇴

그룹에 참가하거나 탈퇴합니다.

```tsx
joinGroup(userID: UUID, channelID: UUID): boolean
leaveGroup(userID: UUID, channelID: UUID): boolean
```

**인자**

userID (`UUID`): 현재 사용자의 ID입니다.

channelID (`UUID`): 사용자가 참여하거나 탈퇴하려고 하는 채널의 ID입니다.

**반환**

결과 (`boolean`): 작업의 성공 또는 실패를 나타냅니다.

## 고수준 설계

시스템의 고수준 설계를 해 보겠습니다.

### 구조

여기서는 서비스의 수평 확장과 디커플링이 더 쉬운 [마이크로서비스 구조](#마이크로서비스)를 사용할 것입니다. 각 서비스는 각자의 데이터 모델에 오너십을 가지게 됩니다. 이 시스템을 몇 개의 핵심 서비스로 나눠봅시다.

**사용자 서비스**

인증이나 사용자 정보와 같은 사용자 관련 작업들을 하는 HTTP 기반 서비스입니다.

**채팅 서비스**

채팅 서비스에서는 웹소켓을 사용하여 클라이언트와 연결을 맺고 채팅이나 그룹 메시지 관련된 기능들을 처리합니다. 또한 사용자가 접속중인지 확인할 수 있는 세션과 같은 모든 활성 연결들을 추적하기 위해 캐시를 사용합니다.

**알림 서비스**

알림 서비스는 사용자에게 간단히 푸시 알림을 전송합니다. 나중에 따로 자세히 설명하겠습니다.

**존재(?) 서비스**

존재 서비스는 모든 사용자의 _최근 활동_ 을 추적합니다. 나중에 따로 자세히 설명하겠습니다.

**미디어 서비스**

미디어 서비스는 미디어(이미지, 비디오, 파일 등) 업로드를 처리합니다. 나중에 따로 자세히 설명하겠습니다.

**서비스 간 통신과 서비스 디스커버리에 대하여**

이 서비스가 마이크로서비스 기반이기 때문에 서비스들은 서로 많이 통신하게 될 겁니다. 일반적으로 REST나 HTTP도 잘 동작하겠지만 더 가볍고 효율적인 [gRPC](https://karanpratapsingh.com/courses/system-design/rest-graphql-grpc#grpc)를 사용하여 더욱 성능을 개선할 수 있습니다.

[서비스 디스커버리](#서비스-디스커버리) 역시 고려 대상입니다. 또한 서비스 간에 관리되며 관찰 가능하고 안전한 통신을 위해 서비스 메시를 사용할 수도 있습니다.

_참고: [REST, GraphQL, gRPC](#rest-graphql-grpc) 챕터에서 각자가 어떻게 다른지 비교해 보십시오._

### 실시간 메시징

효율적으로 메시지를 주고받는 방법이 있을까요? 두 가지 옵션을 살펴보겠습니다.

**풀 모델(Pull model)**

클라이언트가 서버에 새로운 메시지가 왔는지 주기적으로 HTTP 요청합니다. [롱폴링](#롱폴링)과 같은 방식으로 구현할 수 있습니다.

**푸시 모델(Push model)**

클라이언트가 서버와 수명이 긴 연결을 맺고 새로운 데이터가 생기면 그 즉시 클라이언트에 전달되는 방식입니다. [웹소켓](#웹소켓)이나 [서버 전송 이벤트 (SSE)](#서버-전송-이벤트-sse)를 사용할 수 있습니다.

풀 모델 방식은 서버에 불필요한 요청 오버헤드를 만들고 대부분의 시간 동안 응답이 비어 있을 것이기 때문에 자원을 낭비하게 됩니다. 지연 시간을 줄이기 위해서는 일단 연결을 맺고 나면 데이터가 발생하자마자 어떤 지연 없이 즉시 전달해주는 [웹소켓](#웹소켓)으로 푸시 모델을 쓰는 것이 더 좋은 선택입니다. 또한 웹소켓은 전이중 통신을 지원하는 반면 [서버 전송 이벤트 (SSE)](#서버-전송-이벤트-sse)는 단방향만 지원합니다.

_참고: [롱폴링, 웹소켓, 서버 전송 이벤트 (SSE)](#롱폴링-웹소켓-서버-전송-이벤트-sse)를 더 살펴보십시오._

### 최근 활동

최근 활동 기능을 구현하려면 클라이언트가 주기적으로 서버에 핑을 날려 활성화 상태를 알려주는 [하트비트](https://en.wikipedia.org/wiki/Heartbeat_(computing)) 메커니즘을 사용할 수 있습니다. 가능한 부하를 제일 적게 주어야 하기 때문에 최근 활동 타임스탬프를 아래와 같이 캐시에 저장합니다:

| Key    | Value               |
| ------ | ------------------- |
| User A | 2022-07-01T14:32:50 |
| User B | 2022-07-05T05:10:35 |
| User C | 2022-07-10T04:33:25 |

이 캐시는 사용자의 최근 활동 시간을 알려줍니다. 존재 서비스가 이 기능을 처리하게 되는데, [Redis](https://redis.io)나 [Memcached](https://memcached.org)가 캐시로 사용될 것입니다.

다른 구현 방법은 사용자의 마지막 활동을 추적하는 것인데, _"사용자가 30초간 아무런 동작이 없었다"_ 와 같이 마지막 활동 시간이 어느 정도 지나게 되면 사용자를 오프라인으로 표시하고 최근 활동 시간 타임스탬프를 기록합니다. 이 방법은 더욱 게으른 업데이트에 속하며 특정 상황에서 하트비트를 쓰는 것보다 더 유용할 수 있습니다.

### 알림

메시지가 채팅방이나 그룹에 보내지면 우선 수신자가 활동중인지 확인합니다. 이 정보는 유저의 활성 연결이나 최근 활동을 사용하여 알아낼 수 있습니다.

수신자가 활동중이지 않다면 채팅 서비스는 이벤트 하나를 [메시지큐](#메시지-큐)에 추가하고 거기에는 나중에 올바른 플랫폼으로 알림을 보내기 위해 필요한 클라이언트의 디바이스 플랫폼 정보와 같은 메타데이터가 더해집니다.

알림 서비스는 메시지 큐에서 온 이벤트를 소비하고 클라이언스 디바이스 플랫폼에 따라 (Android, iOS, web 등) 요청을 [Firebase Cloud Messaging (FCM)](https://firebase.google.com/docs/cloud-messaging) 또는 [Apple Push Notification Service (APNS)](https://developer.apple.com/documentation/usernotifications)으로 전달합니다. 또한 이메일이나 SMS도 지원할 수 있습니다.

**메시지 큐를 사용하는 이유**

대부분의 메시지 큐는 일반적으로 보낸 순서대로 적어도 한 번 메시지를 처리하는 best-effort 순서를 제공하기 때문에 이 부분이 서비스의 기능과 관련하여 중요합니다.

마치 클래식한 [발행-구독](#발행-구독)처럼 보이지만 사실 모바일 기기나 브라우저가 각각 푸시 알림을 처리하는 방식이기 때문에 다릅니다. 알림은 대개 메시지 팬아웃보다는 파이어베이스 클라우드 메시징(FCM)이나 애플 푸시 알림 서비스(APNS)같은 백엔드 서비스로 흔히 보는 외부 서비스에서 처리됩니다. 그밖에 [Amazon SQS](https://aws.amazon.com/sqs)나 [RabbitMQ](https://www.rabbitmq.com)로 이 기능을 보조할 수 있습니다.

### 수신 확인하기

수신 확인 구현은 까다로울 수 있는데 여기서는 클라이언트에게서 메시지가 잘 전달되었고 해당하는 `deliveredAt` 필드를 업데이트하는 [확인(ACK)](<https://en.wikipedia.org/wiki/Acknowledgement_(data_networks)>)을 기다리게 할 것입니다. 비슷하게 사용자가 채팅방을 열었을 때 그에 대한 `seenAt` 타임스탬프 필드를 업데이트하여 메시지가 읽혀졌음을 표시할 것입니다.

### 설계

핵심 컴포넌트들을 확인했으니 이제 시스템 설계의 초안을 만들어 보겠습니다.

![whatsapp-basic-design](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/whatsapp/whatsapp-basic-design.png)

## 세부 설계

설계시 내린 결정들을 자세히 알아보겠습니다.

### 데이터 분할

데이터베이스를 수평 확장하기 위해서는 데이터를 분할해야 합니다. 수평 분할([샤딩](#샤딩))이 시작하기에 좋을 겁니다. 아래와 같은 분할 전략을 사용할 수 있습니다:

- 해시 기반 분할
- 리스트 기반 분할
- 범위 기반 분할
- 혼합 분할

위에 나열한 방법들도 여전히 균일하지 않은 데이터나 부하 분산이 일어날 수 있습니다. 이 문제를 해결하기 위해서 [일관된 해싱](#일관된-해싱)을 사용할 수 있습니다.

_더 자세한 내용은 [샤딩](#샤딩)과 [일관된 해싱](#일관된-해싱) 항목을 참고하세요._

### 캐싱

메시징 어플리케이션에서는 사용자가 최신의 데이터를 기대하기 때문에 캐싱을 사용하는 데 주의해야 하지만 그룹 채팅방 같은 곳에서는 많은 사용자들이 같은 메시지를 요청할 것입니다. 그렇기 때문에 리소스 사용량 스파이크를 방지하기 위해서 오래된 메시지를 캐시할 수 있습니다.

일부 그룹 채팅방은 수천개의 메시지가 있을 것이고 네트워크를 통해 보내는 것이 매우 비효율적일 것입니다. 이 비효율을 개선하기 위해 시스템 API에 페이징을 추가할 수 있습니다. 이 결정은 제한된 네트워크 대역폭을 가진 유저들에게 아직 요청하지 않은 메시지를 받지 않도록 함으로 도움이 될 것입니다.

**캐시 추출 정책**

[Redis](https://redis.io)나 [Memcached](https://memcached.org) 같은 솔루션으로 매일 20% 가량의 트래픽을 캐시하는 것이 가능하지만 어떤 캐시 추출 정책을 사용하는게 좋을까요?

이 시스템에는 [LRU](<https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)>)가 좋을 수 있습니다. LRU 정책은 사용한 지 가장 오래된 키를 먼저 폐기합니다.

**캐시 미스 처리**

서버는 캐시 미스가 발생하는 경우 데이터베이스에 직접 접근하여 캐시에 새 엔트리를 업데이트합니다.

_더 자세한 내용은 [캐싱](#캐싱) 항목을 참고하세요._

### 미디어 접근 및 저장

이미 알고 있듯이 대부분의 저장소 공간은 이미지나 비디오, 다른 파일들을 저장하는 데 사용될 것입니다. 이 미디어 서비스는 사용자의 미디어 파일들을 저장하고 접근하는 기능을 모두 다룰 것입니다.

하지만 어디에 파일을 저장해야 확장이 가능할까요? 음, [오브젝트 저장소](#오브젝트-저장소)를 써볼 수 있겠습니다. 오브젝트 저장소는 오브젝트라고 하는 조각으로 데이터 파일을 쪼갭니다. 그런 다음 여러 네트워크 시스템에 걸쳐 퍼질 수 있는 단일 저장소에 오브젝트들을 저장합니다. 또한 [HDFS](https://karanpratapsingh.com/courses/system-design/storage#hdfs)나 [GlusterFS](https://www.gluster.org)와 같은 분산 파일 저장소를 사용할 수도 있습니다.

_재미있는 사실: WhatsApp은 사용자가 한 번 다운로드받은 미디어를 삭제합니다._

또한 [Amazon S3](https://aws.amazon.com/s3)나 [Azure Blob Storage](https://azure.microsoft.com/en-in/services/storage/blobs),  [Google Cloud Storage](https://cloud.google.com/storage)와 같은 오브젝트 저장소도 이 경우에 활용할 수 있습니다.

### 콘텐츠 배포 네트워크(CDN)

[Content Delivery Network (CDN)](https://karanpratapsingh.com/courses/system-design/content-delivery-network)은 대역폭 가격을 줄이면서 콘텐츠의 가용성과 중복성을 향상시키는 방법입니다. 일반적으로 이미지나 비디오와 같은 정적 파일들은 CDN에서 제공합니다. [Amazon CloudFront](https://aws.amazon.com/cloudfront)나 [Cloudflare CDN](https://www.cloudflare.com/cdn)과 같은 서비스를 사용할 수 있습니다.

### API 게이트웨이

HTTP나 웹소켓, TCP/IP와 같은 여러 프로토콜을 사용하고 여러 개의 L4(트랜스포트 계층) 또는 L7(어플리케이션 계층)의 로드 밸런서를 배포하는 것은 비쌉니다. 그 대신 여러 프로토콜을 문제 없이 지원하는 [API 게이트웨이](#API-게이트웨이)를 사용할 수 있습니다.

API 게이트웨이는 인가, 인증, 속도 제한, 스로틀링, API 버전관리와 같이 서비스의 품질을 올리는 기능들도 제공합니다.

[Amazon API Gateway](https://aws.amazon.com/api-gateway)나 [Azure API Gateway](https://azure.microsoft.com/en-in/services/api-management)와 같은 서비스도 사용할 수 있습니다.

## 병목을 찾아내고 해결하기

![whatsapp-advanced-design](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/whatsapp/whatsapp-advanced-design.png)

이번 디자인에서 단일 실패점과 같은 병목을 찾아보고 해결해 봅시다.

- "서비스가 충돌한다면 어떻게 되는가?"
- "어떻게 컴포넌트 간 트래픽을 분산할 것인가?"
- "어떻게 데이터베이스의 부하를 줄일 수 있을 것인가?"
- "어떻게 캐시의 가용성을 늘릴 수 있을 것인가?"
- "API 게이트웨이가 단일 실패점이 되지 않을 것인가?"
- "어떻게 알림 시스템을 튼튼하게 만들 것인가?"
- "어떻게 미디어 저장소 비용을 줄일 수 있을 것인가?"
- "채팅 서비스가 과도하게 반응형인 것은 아닌가?"

시스템을 더욱 탄력적으로 만들기 위해 아래와 같이 해볼 수 있습니다:

- 각 서비스들에 대해 여러 인스턴스를 실행합니다.
- 클라이언트와 서버, 데이터베이스, 캐시 서버들 간에 [로드 밸런서](#로드-밸런싱)를 도입합니다.
- 데이터베이스에 여러 개의 읽기전용 레플리카를 사용합니다.
- API 게이트웨이에 준비된(standby) 레플리카를 사용할 수 있습니다.
- 분산 시스템에서는 정확히 한번 전송이나 메시지 순서 정렬이 쉽지 않기 때문에 [Apache Kafka](https://kafka.apache.org)또는 [NATS](https://nats.io)와 같은 전용 [메시지 브로커](#메시지-브로커)를 사용하여 알림 시스템을 더 튼튼하게 만들 수 있습니다.
- WhatsApp과 같이 미디어 서비스에 미디어 프로세싱 및 압축 기능을 더하여 용량이 큰 파일들을 압축하고 저장 공간과 비용을 절약할 수 있습니다.
- 그룹 채팅 서비스를 일반 채팅 서비스와 분리해 서비스 간 의존성을 더 줄입니다.

# 트위터

[트위터](https://twitter.com)나 [페이스북](https://facebook.com), [인스타그램](https://instagram.com) 등과 같은 소셜 미디어 서비스를 설계해 봅시다.

## 트위터란?

트위터는 사용자들이 트윗이라고 하는 (최대 280글자의) 짧은 메시지를 올리거나 읽을 수 있는 소셜 미디어 서비스입니다. 웹과 Android 또는 iOS와 같은 모바일 플랫폼으로 이용할 수 있습니다.

## 요구사항

이 시스템은 아래와 같은 요구사항을 만족해야 합니다:

### 기능적 요구사항

- 새 트윗을 올릴 수 있어야 합니다(텍스트, 이미지, 비디오 등이 될 수 있음.).
- 다른 사용자를 팔로우할 수 있어야 합니다.
- 팔로우하고 있는 사용자의 트윗을 포함한 뉴스피드 기능이 있어야 합니다.
- 트윗을 검색할 수 있어야 합니다.

### 비기능적 요구사항

- 최소한의 지연으로 높은 가용성이어야 합니다.
- 확장 가능하며 효율적인 시스템이어야 합니다.

### 추가 요구사항

- 메트릭 및 분석
- 리트윗 기능
- 트윗 좋아요 기능

## 추정 및 제약

추정과 제약사항부터 시작해보겠습니다.

_참고: 규모나 트래픽과 관련된 가정들은 면접관과 먼저 확인해야 합니다._

### 트래픽

이 시스템은 읽기가 무거운 시스템이 될 것이며 10억명의 전체 유저와 2억명의 일일 활성 유저(DAU)가 있으며 각 유저는 하루에 평균적으로 5개의 트윗을 올린다고 가정합니다. 이는 하루에 10억개의 트윗이 생길 것으로 예상할 수 있습니다.

$$
2 \space 억 \times 5 \space 트윗 = 10 \space 억/일
$$

트윗은 이미지나 비디오와 같은 미디어를 포함할 수 있습니다. 대략 10퍼센트의 트윗들이 사용자에 의해 공유되는 미디어 파일이라고 가정하면 추가적으로 1억개의 파일을 저장해야 합니다.

$$
10 \space 퍼센트 \times 10 \space 억 = 1 \space 억/일
$$

**시스템의 초당 요청수(Requests Per Second, RPS)**

매일 10억건의 요청은 초당 1만 2천개의 요청으로 계산할 수 있습니다.

$$
\frac{10 \space 억}{(24 \space 시간 \times 3600 \space 초)} = \sim 1.2만 \space 요청/초
$$

### 저장 공간

각 메시지가 평균적으로 100바이트라고 가정한다면 매일 100GB 가량의 데이터베이스 저장 공간이 필요하게 됩니다.

$$
10 \space 억 \times 100 \space bytes = \sim 100 \space GB/일
$$

또한 일일 메시지의 대략 10퍼센트 가량이 미디어 파일이라는 점을 알고 있습니다. 각 파일이 평균 50KB임을 가정한다면 매일 5TB의 저장 공간이 필요하게 됩니다.

$$
1 \space 억 \times 100 \space KB = 5 \space TB/일
$$

그리고 10년간 대략 19PB의 저장공간이 필요하게 됩니다.

$$
(5 \space TB + 0.1 \space TB) \times 365 \space 일 \times 10 \space 년 = \sim 19 \space PB
$$

### 대역폭

이 시스템이 매일 5.1TB의 네트워크 입력을 받아야 하기 때문에 초당 60MB의 최소 대역폭이 필요하게 됩니다.

$$
\frac{5.1 \space TB}{(24 \space 시간 \times 3600 \space 초)} = \sim 60 \space MB/초
$$

### 고수준 추정

고수준 추정은 아래와 같습니다:

| Type                      | Estimate    |
| ------------------------- | ----------- |
| Daily active users (DAU)  | 100 million |
| Requests per second (RPS) | 12K/s       |
| Storage (per day)         | ~5.1 TB     |
| Storage (10 years)        | ~19 PB      |
| Bandwidth                 | ~60 MB/s    |

## 데이터 모델 설계

아래는 요구사항을 반영하는 일반 데이터 모델입니다.

![twitter-datamodel](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/twitter/twitter-datamodel.png)

테이블 종류는 아래와 같습니다:

**users**

이 테이블은 사용자의 이름이나 이메일, 생년월일과 같은 정보를 담고 있습니다.

**tweets**

이름에서 알 수 있듯이 이 테이블은 트윗 정보를 저장하고 타입`type`(텍스트, 이미지, 비디오 등)이나 내용`content`과 같은 속성도 저장합니다. 또한 트윗에 해당하는 사용자 ID`userID`도 저장합니다.

**favorites**

어플리케이션의 트윗 즐겨찾기 기능을 위한 사용자와 트윗을 매핑해주는 테이블입니다.

**followers**

사용자들이 서로를 팔로우할 수 있기 때문에 (N:M 관계) 이 테이블은 팔로워와 팔로이([followees](https://en.wiktionary.org/wiki/followee))를 매핑해줍니다.

**feeds**

해당하는 `userID`에 대한 피드 속성을 저장합니다.

**feeds_tweets**

이 테이블은 트윗과 피드를 매핑합니다(N:M 관계).

### 어느 데이터베이스를 사용해야 할까

데이터 모델이 관계형처럼 보이기는 하지만 모든 정보를 데이터베이스 하나에 저장해봤자 확장성을 저해하고 곧바로 병목이 될 것이기 때문에 그렇게 할 필요는 없습니다.

여기서는 서비스 간에 데이터를 나누어 각 서비스가 특정 테이블에 오너십을 가지도록 할 것입니다. 그런 뒤에 [PostgreSQL](https://www.postgresql.org)과 같은 관계형 데이터베이스나 [Apache Cassandra](https://cassandra.apache.org/_/index.html)와 같은 NoSQL 데이터베이스를 필요에 맞게 사용할 수 있습니다.

## API 설계

서비스데 대한 기본 API 설계를 해 보겠습니다.

### 트윗 올리기

이 API는 사용자가 플랫폼에 트윗을 올릴 수 있게 합니다.

```tsx
postTweet(userID: UUID, content: string, mediaURL?: string): boolean
```

**파라미터**

userID (`UUID`): 사용자의 ID입니다.

content (`string`): 트윗의 내용입니다.

mediaURL (`string`): 첨부된 미디어의 주소입니다 _(optional)_.

**반환**

Result (`boolean`): 작업이 성공했는지 여부를 표현합니다

### 사용자 팔로우 또는 언팔로우하기

이 API는 사용자가 다른 사용자를 팔로우 또는 언팔로우할 수 있게 해줍니다.

```tsx
follow(followerID: UUID, followeeID: UUID): boolean
unfollow(followerID: UUID, followeeID: UUID): boolean
```

**파라미터**

followerID (`UUID`): 현재 사용자의 ID입니다.

followeeID (`UUID`): 사용자가 팔로우 또는 언팔로우하기를 원하는 대상 사용자의 ID입니다.

mediaURL (`string`): 첨부된 미디어의 URL입니다 _(optional)_.

**반환**

Result (`boolean`): 작업이 성공했는지 여부를 표현합니다.

### 뉴스피드 가져오기

이 API는 뉴스피드에 보여질 모든 트윗들을 반환합니다.

```tsx
getNewsfeed(userID: UUID): Tweet[]
```

**파라미터**

userID (`UUID`): 사용자의 ID입니다.

**반환**

Tweets (`Tweet[]`): 뉴스피드에 나타날 모든 트윗들입니다.

## 고수준 설계

시스템의 고수준 설계를 해 봅시다.

### 구조

[마이크로서비스](#마이크로서비스) 구조가 수평 확장과 서비스 의존성 해소가 쉽기 때문에 이 구조를 사용할 것입니다. 각 서비스는 각 데이터모델에 오너십을 가질 것입니다. 이 시스템을 몇몇의 핵심 서비스로 나누어 보겠습니다.

**사용자 서비스**

이 서비스는 인증이나 사용자 정보와 같이 사용자와 관련된 일들을 합니다.

**뉴스피드 서비스**

이 서비스는 사용자 뉴스피드를 생성하고 발행하는 일을 담당합니다. 따로 자세하게 설명하겠습니다.

**트윗 서비스**

이 서비스는 트윗을 올리는 것이나 즐겨찾기와 같은 트윗과 관련된 사용례를 담당합니다.

**검색 서비스**

이 서비스는 검색과 관련된 기능을 담당합니다. 따로 자세하게 설명하겠습니다.

**미디어 서비스**

이 서비스는 미디어(이미지, 비디오, 파일 등) 업로드를 담당합니다. 따로 자세하게 설명하겠습니다.

**알림 서비스**

간단하게 사용자에게 푸시 알림을 보내는 서비스입니다.

**분석 서비스**

이 서비스는 메트릭과 분석 사용을 위해 사용될 것입니다.

**서비스간 통신과 서비스 디스커버리**

이 구조는 마이크로서비스 기반이기 때문에 각각의 서비스가 서로 통신하게 될 것입니다. 일반적으로 REST 또는 HTTP로도 잘 동작하겠지만 더 가볍고 효율적인 [gRPC](#grpc)를 사용하여 더 성능을 개선할 수 있습니다.

[서비스 디스커버리](#서비스-디스커버리) 역시 고려해야 합니다. 서비스 메쉬를 사용하여 각 서비스 간에 관리되고 관측 가능하며 안전한 통신을 만들 수 있습니다.

_Note: [REST, GraphQL, gRPC](#rest-graphql-grpc)에 대해 더 알아보고 각각이 어떻게 다른지 비교해보십시오._

### 뉴스피드

뉴스피드를 생각해봤을 때 구현이 쉬워보일 수 있지만 이 기능을 구성하거나 부러트릴 수 있는 수많은 것들이 있습니다. 이 문제를 두 부분으로 나누어 보겠습니다:

**생성**

사용자 A의 피드를 생성하려고 한다고 가정하고, 아래와 같은 순서를 밟을 것입니다:

1. 사용자 A가 팔로우하는 모든 유저와 엔티티(해시태그, 토픽 등)의 ID들을 받아옵니다.
2. 받아온 각 ID에 대해 연관된 트윗을 가져옵니다.
3. 연관성, 시간, 참여도 등의 파라미터로 랭킹 알고리즘을 사용하여 트윗의 순서를 정합니다.
4. 페이징된 데이터로 클라이언트에게 랭크된 트윗을 전달합니다.

피드 생성은 시간이 많이 드는 무거운 작업니다. 특히 많은 사람을 팔로우하는 사용자들의 경우 더욱 그러합니다. 성능을 개선하기 위해 피드를 미리 생성하고 캐시에 저장해놓은 뒤 주기적으로 피드를 갱신하고 새 트윗에 대해 랭킹 알고리즘을 돌리면 됩니다.

**발행**

발행은 피드 데이터를 각 유저에게 푸시하는 단계입니다. 사용자가 친구나 팔로우를 수백만 명 가지고 있을 수도 있기 때문에 매우 무거운 작업이 될 수 있습니다. 이 문제를 해결하기 위해 세 가지 다른 방법을 사용해볼 수 있습니다:

- 풀(pull) 모델 (또는 팬아웃 온 로드 Fan-out on load)

![newsfeed-pull-model](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/twitter/newsfeed-pull-model.png)

사용자가 트윗을 생성하고 팔로워가 뉴스피드를 다시 로드하면 피드가 생성되고 메모리에 저장됩니다. 가장 최신 피드는 사용자가 요청했을 때만 불러와집니다. 이 방법은 데이터베이스에서의 쓰기 작업을 줄여줍니다.

이 방법의 단점은 서버에서 데이터를 당겨오지 않는 이상 최신 피드를 볼 수 없기 때문에 서버에 읽기 작업수를 늘리게 됩니다.

- 푸시(push) 모델 (또는 팬아웃 온 라이트 Fan-out on write)

![newsfeed-push-model](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/twitter/newsfeed-push-model.png)

이 모델은 사용자가 트윗을 생성할 때 모든 팔로워들의 피드에 즉시 푸시됩니다. 이렇게 하면 시스템이 업데이트 확인을 위해 유저의 모든 팔로워 목록을 돌지 않아도 됩니다.

하지만 이 방법의 단점은 데이터베이스의 쓰기 작업을 증가시킬 수 있다는 것입니다.

- 하이브리드 모델

세 번째 방법은 풀 및 푸시 모델의 하이브리드입니다. 위의 두 모델의 장점이 되는 기능들을 결합하고 둘 사이의 균형 잡힌 접근법을 제공하려고 합니다.

하이브리드 모델에서는 적은 팔로워를 보유한 사용자에 대해서는 푸시 모델을 사용합니다. 셀럽들과 같이 많은 팔로워수를 보유한 사용자에 대해서는 풀 모델이 사용됩니다.

### 랭킹 알고리즘

앞서 말했듯이 각 사용자와 연관된 트윗의 순위를 매기는 랭킹 알고리즘이 필요하게 됩니다.

예를 들어 페이스북은 [EdgeRank](https://en.wikipedia.org/wiki/EdgeRank) 알고리즘을 사용했던 적이 있습니다. 아래에 나온 것처럼 각 피드 아이템에 대한 랭크는 이렇게 정해집니다:

$$
Rank = Affinity \times Weight \times Decay
$$

여기서,

`Affinity`: 이것은 사용자에서부터 엣지를 만든 사람에까지 얼마나 가까운지를 나타냅니다. 사용자가 해당 엣지를 만든 사람에 좋아요를 많이 누르거나 댓글을 달거나 메시지를 보낸다면 선호도 관련성 값이 더 높아져 포스트의 랭크가 더 높게 나옵니다.

`Weight`: 가중치는 각 엣지에 지정됩니다. 댓글은 좋아요보다 더 높은 가중치를 가질 수 있기 때문에 더 많은 댓글이 달린 포스트는 더 높은 랭크를 받을 가능성이 높아집니다.

`Decay`: 이것은 엣지의 생성에 대한 측정입니다. 더 오래된 엣지가 더 낮은 값을 가지며 결국 랭크도 낮아지게 됩니다.

요즘 알고리즘은 훨씬 복잡하며 수천개의 요소들을 고려할 수 있는 기계학습을 사용하여 랭킹이 매겨집니다.

### 리트윗

리트윗은 추가 요구사항 중 하나입니다. 이 기능을 구현하려면 단순히 원래 트윗을 리트윗하는 유저의 이름으로 된 새 트윗을 만들고 `type`이라는 enum값과 `content`라는 속성을 수정하여 원래 트윗을 가리키게 만들 수 있습니다.

예를 들어 `type` enum 속성은 텍스트나 비디오와 같은 트윗의 종류가 될 수 있고 `content` 속성은 원래 트윗의 id가 될 수 있습니다. 아래 표에서 첫번째 행은 원래 트윗을 가리키며 두번째 행은 리트윗을 표현하는 방법입니다.

| id                  | userID              | type  | content                      | createdAt     |
| ------------------- | ------------------- | ----- | ---------------------------- | ------------- |
| ad34-291a-45f6-b36c | 7a2c-62c4-4dc8-b1bb | text  | Hey, this is my first tweet… | 1658905644054 |
| f064-49ad-9aa2-84a6 | 6aa2-2bc9-4331-879f | tweet | ad34-291a-45f6-b36c          | 1658906165427 |

이것이 가장 간단한 구현입니다. 더 개선하기 위해서는 리트윗을 저장하기 위한 테이블을 별도로 만들 수 있습니다.

### 검색

전통적인 DBMS는 간혹 성능이 충분하지 않기 때문에 따로 거대한 양의 데이터를 실시간에 가까울 정도로 빠르게 저장하고 검색하며 분석하여 결과를 밀리초 단위로 줄 수 있는 대체재가 필요합니다. 이 경우에 [Elasticsearch](https://www.elastic.co)가 도움이 될 수 있습니다.

[Elasticsearch](https://www.elastic.co)는 무료이며 열린 분산 검색 및 분석 엔진입니다. 엘라스틱서치는 모든 데이터 종류를 다루는데 그 종류는 텍스트, 숫자, 지리, 구조화된 데이터 그리고 구조화되지 않은 데이터까지 다양합니다. 이 엔진은 [Apache Lucene](https://lucene.apache.org) 기반으로 만들어졌습니다.

**트렌드 주제를 찾는 방법**

트렌딩 기능은 검색 기능을 기반할 것입니다. 최근 `N`초간 가장 많이 검색되는 질의나 해시태그, 주제들을 캐시한 뒤 배치 작업 메커니즘을 사용하여 매 `M`초마다 업데이트합니다. 랭킹 알고리즘 역시 트렌딩 주제에 사용하여 좀 더 가중치가 있고 사용자화된 주제를 제공할 수도 있습니다.

### 알림

푸시 알림은 어느 소셜 미디어 플랫폼이건 간에 통합적인 부분입니다. 여기에는 메시지 큐나 [Apache Kafka](https://kafka.apache.org)와 같은 메시지 브로커를 사용하여 알림 서비스가 [파이어베이스 클라우드 메시징(FCM)](https://firebase.google.com/docs/cloud-messaging)이나 [애플 푸시 알림 서비스(APNS)](https://developer.apple.com/documentation/usernotifications)쪽으로 요청을 보내 사용자 기기에 푸시 알림을 전달하게 할 수 있습니다.

_자세한 내용은 푸시 알림을 설명했던 [WhatsApp](#알림) 시스템 디자인 부분을 참고하시기 바랍니다._

## 세부 설계

설계상의 결정들에 대해 자세히 알아보겠습니다.

### 데이터 파티션

데이터베이스의 크기를 확장하기 위해서는 데이터를 나누어야 합니다. ([샤딩](#샤딩)이라고도 하는) 수평 확장이 좋은 시작이 될 겁니다. 그 외에도 아래와 같은 접근법을 사용할 수 있습니다:

- 해시 기반 파티션
- 리스트 기반 파티션
- 범위 기반 파티션
- 복합 파티션

위의 방법들을 사용해도 여전히 불균등한 데이터 분산이나 부하 분산이 있을 것이기 때문에 [일관된 해싱](#일관된-해싱)을 써서 문제를 해결할 수 있습니다.

_자세한 내용은 [샤딩](#샤딩) 항목과 [일관된 해싱](#일관된-해싱) 부분을 참고하시기 바랍니다._

### 맞팔로우

맞팔(서로 친구)을 맺을 수 있는 기능에 관해서는 모든 유저에 대해 소셜 그래프를 만들어볼 수 있습니다. 그래프의 각 노드는 유저 하나를 나타내며 방향이 있는 화살표(엣지)는 팔로워나 팔로워 당함을 표현합니다. 그런 뒤 유저들의 팔로워를 돌면서 반대로 유저가 팔로우할 수 있는 친구 후보를 제안할 수 있습니다. 이런 부분에서는 [Neo4j](https://neo4j.com)나 [ArangoDB](https://www.arangodb.com)와 같은 그래프 데이터베이스가 필요할 수 있습니다.

이 과정은 매우 단순한 알고리즘이기 때문에 추천의 정확도를 개선하려면 기계 학습을 사용하는 추천 모델을 알고리즘의 일부분으로 채용할 필요가 있습니다.

### 메트릭 및 분석

분석과 메트릭을 기록하는 것은 확장 요구사항 중에 하나입니다. [Apache Kafka](https://kafka.apache.org)를 통해 이벤트들을 발행할 것이기 때문에 이 이벤트를 처리하고 큰 스케일의 데이터 프로세싱을 하는 오픈 소스 분석 엔진인 [Apache Spark](https://spark.apache.org)에 데이터 분석을 돌릴 수 있습니다.

### 캐싱

소셜 미디어 어플리케이션에서는 사용자들이 최신 데이터를 가져오기를 기대하기 때문에 캐싱을 사용하는 데 있어 주의를 기울여야 합니다. 따라서 리소스 사용량 스파이크를 막기 위해 탑 20%가량의 트윗들을 캐싱할 수 있습니다.

효율성을 더욱 개선하려면 시스템 API에 페이징을 추가할 수 있습니다. 이 결정은 추가로 요청하지 않는 이상 오래된 메시지들을 받지 않게 하기 때문에 제한된 네트워크 대역폭을 가진 유저들에게 도움이 될 것입니다.

**캐시 추출 정책**

[Redis](https://redis.io)나 [Memcached](https://memcached.org)를 사용하여 일일 프래픽의 20%를 캐시할 수 있지만 어떤 캐시 추출 정책이 요구사항에 가장 잘 부합할까요?

여기에는 [Least Recently Used (LRU)](<https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)>)가 좋을 것입니다. 이 정책은 가장 나중에 사용됐던 키를 먼저 추출해냅니다.

**캐시 미스를 처리하는 방법**

캐시 미스가 발생하면 서버가 직접 데이터베이스에서 데이터를 가져오고 새 데이터로 캐시를 갱신합니다.

_자세한 내용은 [캐싱](#캐싱) 항목을 참고하시기 바랍니다._

### 미디어 접근 및 저장소

이미 알고 있겠지만 대부분의 저장소는 이미지나 비디오, 그외 파일들을 저장하는 데 사용될 것입니다. 미디어 서비스는 사용자 미디어 파일에 접근하고 저장하는 부분을 담당할 것입니다.

확장 가능하게 파일얼 저장하려면 어디에 저장해야 할까요? 아마, [오브젝트 저장소](#오브젝트-저장소)가 적합할 것입니다. 오브젝트 저장소는 데이터를 오브젝트라고 부르는 조각들로 나눕니다. 그런 다음 다른 여러 네트워크 시스템에 전파될 수 있는 단일 저장소에 오브젝트를 저장합니다. 또한 [HDFS](https://karanpratapsingh.com/courses/system-design/storage#hdfs)나 [GlusterFS](https://www.gluster.org)와 같은 분산 파일 저장소를 사용할 수도 있습니다.

### 콘텐츠 배포 네트워크

[콘텐츠 배포 네트워크 (CDN)](https://karanpratapsingh.com/courses/system-design/content-delivery-network)는 대역폭 비용을 줄이면서 콘텐츠의 가용성과 중복성을 증가시킵니다. 일반적으로 이미지나 비디오와 같은 정적 파일들을 CDN에서 제공합니다. [Amazon CloudFront](https://aws.amazon.com/cloudfront) 또는 [Cloudflare CDN](https://www.cloudflare.com/cdn)과 같은 서비스를 사용할 수 있습니다.

## 병목을 찾아내고 해결하기

![twitter-advanced-design](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/twitter/twitter-advanced-design.png)

이 설계에서 단일 실패점과 같은 병목을 찾아보고 해결해 봅시다:

- "서비스 중 하나라도 충돌하면 어떻게 할까?"
- "어떻게 컴포넌트 간에 트래픽을 분산할 것인가?"
- "어떻게 데이터베이스의 부하를 줄일 수 있을까?"
- "어떻게 캐시의 가용성을 개선할 수 있을까?"
- "어떻게 알림 시스템을 튼튼하게 만들 수 있을까?"
- "어떻게 미디어 저장소 비용을 줄일 수 있을까?"

더욱 복원력이 있는 시스템을 만들기 위해 아래와 같이 해볼 수 있습니다:

- 각 서비스를 여러 인스턴스로 실행한다.
- 클라이언트, 서버, 데이터베이스 및 캐시 서버 사이에 [로드 밸런서](#로드-밸런싱)를 사용합니다.
- 데이터베이스에 여러 개의 읽기 전용 레플리카를 사용합니다.
- 분산 시스템에서 메시지 단일 전달과 순서 처리는 쉽지 않기 때문에 [Apache Kafka](https://kafka.apache.org)나 [NATS](https://nats.io)와 같은 전용 [메시지 브로커](#메시지-브로커)를 사용하여 알림 시스템을 더욱 튼튼하게 만들 수 있습니다.
- 미디어 서버어 미디어 프로세싱과 압축 기능을 더하여 커다란 파일을 압축하여 많은 저장 공간을 확보하고 비용을 줄일 수 있습니다.

# 넷플릭스

[넷플릭스](https://netflix.com)와 같은 비디오 스트리밍 서비스를 설계해 봅시다. 비슷한 서비스로는 [아마존 프라임 비디오](https://www.primevideo.com), [디즈니 플러스](https://www.disneyplus.com), [Hulu](https://www.hulu.com), [유튜브](https://youtube.com), [Vimeo](https://vimeo.com) 등이 있습니다.

## 넷플릭스란

넷플릭스는 구독 기반의 스트리밍 서비스로서 멤버들은 인터넷에 연결된 기기를 통해 TV 프로그램과 영화를 볼 수 있습니다. 웹이나 iOS, 안드로이드, TV와 같은 플랫폼에서 동작합니다.

## 요구사항

이 시스템은 아래와 같은 요구사항을 만족해야 합니다:

### 기능적 요구사항

- 사용자는 영상을 스트리밍하고 공유할 수 있어야 합니다.
- 컨텐츠 팀(유튜브의 경우 사용자)은 새로운 영상(영화, TV 프로그램 에피소드 및 다른 컨텐츠)을 업로드할 수 있어야 합니다.
- 사용자는 제목이나 태그로 영상을 찾을 수 있어야 합니다.
- 사용자는 유튜브처럼 영상에 댓글을 남길 수 있어야 합니다.

### 비기능적 요구사항

- 최소한의 지연 시간과 최대 가용성
- 높은 신뢰도로 어떤 영상도 잃어버리지 않아야 합니다.
- 시스템이 확장 가능하며 효율적이어야 합니다.

### 추가 요구사항

- 특정 컨텐츠는 [지역 락](https://en.wikipedia.org/wiki/Geo-blocking)이 걸릴 수 있습니다.
- 사용자가 떠난 시점에서 영상을 다시 재생하는 기능.
- 영상의 메트릭과 분석을 기록.

## 추정 및 제약

추정 및 제약사항을 생각해 보겠습니다.

_참고: 규모나 트래픽 관련된 가정들은 면접관과 함께 확인하십시오._

### 트래픽

이 시스템은 읽기가 많을 것이기 때문에 총 사용자 수는 10억 명에 일일 활성 사용자 수(DAU) 2억명, 각 사용자는 하루에 평균적으로 5개의 영상을 본다고 가정합니다. 이 가정에 기반하면 하루에 10억 건의 영상 시청이 나오게 됩니다.

$$
2 \space 억 \times 5 \space 영상 = 10 \space 억/일
$$

읽기/쓰기 비율을 `200:1`로 가정하면 하루에 대략 5천만 건의 영상 업로드가 발생합니다.

$$
\frac{1}{200} \times 10 \space 억 = 5 \space 천만/일
$$

**시스템의 초당 요청수 구하기**

하루에 10억 건의 요청은 초당 1만 2천건의 요청으로 계산됩니다.

$$
\frac{10 \space 억}{(24 \space 시간 \times 3600 \space 초)} = \sim 1.2만 \space 요청/초
$$

### 저장소

각 영상의 용량이 평균적으로 100MB라고 가정하면 매일 5 PB의 저장소가 필요하게 됩니다.

$$
5 \space 천만 \times 100 \space MB = 5 \space PB/일
$$

그리고 10년이라는 시간 동안 18,250PB라는 엄청난 양의 저장소가 필요합니다.

$$
5 \space PB \times 365 \space 일 \times 10 \space 년 = \sim 18,250 \space PB
$$

### 대역폭

시스템이 매일 5 PB의 유입을 받아들여야 하기 때문에 초당 대략 58 GB의 대역폭을 처리할 수 있어야 합니다.

$$
\frac{5 \space PB}{(24 \space 시간 \times 3600 \space 초)} = \sim 58 \space GB/초
$$

### 고수준 추정

고수준 추정을 한 내용은 아래와 같습니다:

| Type                      | Estimate    |
| ------------------------- | ----------- |
| Daily active users (DAU)  | 200 million |
| Requests per second (RPS) | 12K/s       |
| Storage (per day)         | ~5 PB       |
| Storage (10 years)        | ~18,250 PB  |
| Bandwidth                 | ~58 GB/s    |

## 데이터 모델 설계

우리의 요구사항을 반영하는 일반적인 데이터 모델입니다.

![netflix-datamodel](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/netflix/netflix-datamodel.png)

아래와 같은 테이블로 구성되어 있습니다:

**users**

`name`, `email`, `dob` 등과 같은 사용자의 정보를 담고 있습니다.

**videos**

이 테이블은 이름에서 알 수 있듯이 `title`이나 `streamURL`, `tags` 등과 같은 영상과 그 속성을 저장합니다. 또한 그에 해당하는 `userID`도 저장합니다.

**tags**

이 테이블은 단순히 해당 영상과 관련된 태그를 저장합니다.

**views**

이 테이블은 해당 영상에 대한 시청 내역을 저장하는 데 사용합니다.

**comments**

이 테이블은 (유튜브처럼) 영상에 대한 모든 코멘트를 저장합니다.

### 데이터베이스 종류 선정

이 데이터 모델이 매우 관계적인 것처럼 보이지만 반드시 모든 데이터를 단일 데이터베이스 하나에 저장할 필요는 없습니다. 그렇게 한다면 확장성을 제한하고 빠르게 병목이 되기 쉽습니다.

여기서는 다른 서비스 간에 데이터를 나누어 각자가 특정 테이블에 오너십을 가지게 할 것입니다. 그런 다음 [PostgreSQL](https://www.postgresql.org)과 같은 관계형 데이터베이스나 [Apache Cassandra](https://cassandra.apache.org/_/index.html)와 같은 분산 NoSQL 데이터베이스를 사용할 수 있습니다.

## API 설계

서비스에 사용할 기본적인 API 설계를 해 봅시다:

### 영상 업로드

이 API는 주어진 바이트 스트림으로 서비스에 영상을 업로드할 수 있게 합니다.

```tsx
uploadVideo(title: string, description: string, data: Stream<byte>, tags?: string[]): boolean
```

**파라미터**

Title (`string`): 새 영상의 제목입니다.

Description (`string`): 새 영상에 대한 설명입니다.

Data (`Byte[]`): 영상 데이터의 바이트 스트림입니다.

Tags (`string[]`): 영상의 태그입니다 _(옵션)_.

**반환**

Result (`boolean`): 작업이 성공했는지 아닌지 표현합니다.

### 영상 스트리밍하기

이 API는 선호하는 코덱과 해상도로 영상을 스트리밍할 수 있게 해 줍니다.

```tsx
streamVideo(videoID: UUID, codec: Enum<string>, resolution: Tuple<int>, offset?: int): VideoStream
```

**파라미터**

Video ID (`UUID`): 스트리밍할 영상의 ID입니다.

Codec (`Enum<string>`): 요청한 영상을 재생하기 위해 필요한 [코덱](https://en.wikipedia.org/wiki/Video_codec)입니다. `h.265`, `h.264`, `VP9` 와 같은 값들이 있습니다.

Resolution (`Tuple<int>`): 요청한 영상의 [해상도](https://en.wikipedia.org/wiki/Display_resolution)입니다.

Offset (`int`): 영상을 특정 시점부터 재생하기 위해 얼마나 오프셋을 줄 지 초단위로 전달합니다 _(옵션)_.

**반환**

Stream (`VideoStream`): 요청한 영상의 데이터 스트림입니다.

### 영상 검색

이 API는 영상의 제목이나 태그로 찾을 수 있게 해 줍니다.

```tsx
searchVideo(query: string, nextPage?: string): Video[]
```

**파라미터**

Query (`string`): 사용자가 입력한 검색어입니다.

Next Page (`string`): 다음 페이지의 토큰으로, 페이징을 구현할 때 사용합니다 _(옵션)_.

**반환**

Videos (`Video[]`): 특정 검색어에 대한 모든 가능한 비디오의 목록입니다.

### 코멘트 남기기

이 API는 사용자가 (유튜브처럼) 영상에 코멘트를 남길 수 있게 해 줍니다.

```tsx
comment(videoID: UUID, comment: string): boolean
```

**파라미터**

VideoID (`UUID`): 사용자가 코멘트를 남기는 영상의 ID입니다.

Comment (`string`): 코멘트의 본문입니다.

**반환**

Result (`boolean`): 작업이 성공했는지 여부를 알려줍니다.

## 고수준 설계

시스템의 고수준 설계를 해 봅시다.

### 구조

여기에서는 수평적 확장과 서비스 간 디커플링이 간단한 [마이크로서비스 구조](#마이크로서비스)를 사용할 것입니다. 각 서비스는 담당하고 있는 모델에 소유권을 가집니다. 이 시스템을 핵심 서비스 몇 가지로 나누어 봅시다.

**사용자 서비스**

이 서비스는 인증이나 사용자 정보와 같은 사용자 관련된 작업들을 처리합니다.

**스트리밍 서비스**

스트리밍 서비스는 영상 스트리밍과 관련된 기능들을 처리합니다.

**검색 서비스**

이 서비스는 검색과 관련된 기능을 담당합니다. 추후에 자세히 설명하겠습니다.

**미디어 서비스**

이 서비스는 영상 업로드 및 처리를 담당합니다. 추후에 자세히 설명하겠습니다.

**분석 서비스**

이 서비스는 메트릭 및 분석을 다룹니다.

**서비스 간 통신과 서비스 디스커버리**

이 구조는 마이크로서비스 기반이기 때문에 각 서비스는 다른 서비스와 서로 통신하게 될 것입니다. 일반적으로 REST나 HTTP로도 잘 동작하지만 추가적인 성능 향상을 위해 더 가볍고 효율적인 [gRPC](#grpc)를 사용할 수 있습니다.

[서비스 디스커버리](#서비스-디스커버리) 역시 고려해야 할 주제입니다. 개별 서비스 간에 관리되고 관측 가능하며 안전한 통신을 지원하는 서비스 메쉬를 사용할 수 있습니다.

_참고: [REST, GraphQL, gRPC](#rest-graphql-grpc) 항목에서 각각이 어떻게 다른지 비교해 보십시오._

### 영상 처리

![video-processing-pipeline](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/netflix/video-processing-pipeline.png)

영상을 처리하는 데에는 수많은 변수들이 있습니다. 예를 들어, 최첨단 카메라로 촬영한 2시간짜리 로우 8K 영상의 평균적인 크기는 4 TB정도는 손쉽게 넘길 수 있기 때문에 처리 과정을 통해 용량과 배포 비용 모두 줄여야 합니다.

아래 내용은 컨텐츠 팀(또는 유튜브의 경우 유저)이 업로드한 영상을 처리하고 [메시지 큐](#메시지-큐)에 큐잉하는 방법입니다.

어떻게 작동하는지 알아봅시다:

- **파일 청커**

![file-chunking](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/netflix/file-chunking.png)

파일 청커는 처리 파이프라인의 가장 처음 단계입니다. 파일 청킹(file chunking)은 파일 하나를 청크(chunk)라고 하는 작은 조각으로 나누는 과정입니다. 저장소에 반복되는 데이터를 중복으로 저장하지 않도록 도와주며 바뀐 청크만을 선택하여 전송함으로 네트워크를 통해 전달되는 데이터량을 줄입니다.

대개 영상 파일은 타임스탬프 기반으로 같은 크기로 나눌 수 있지만 넷플릭스에서는 그 대신 장면에 기반하여 청크를 나눕니다. 이런 작은 변주가 더 나은 유저 경험에 거대한 변수가 될 수 있습니다. 클라이언트가 서버에 청크를 요청할 때 전체 장면이 받아지는 데 방해를 받을 가능성이 더 줄어들기 때문입니다.

- **컨텐츠 필터**

이 단계에서는 영상이 플랫폼의 컨텐츠 정책을 준수하는지 확인합니다. 넷플릭스처럼 미리 심사된 영상의 [컨텐츠 등급](https://en.wikipedia.org/wiki/Motion_picture_content_rating_system)을 따를 수도 있고 유튜브처럼 강제하는 방법도 있습니다.

이 전체 과정은 저작권, 사용권 침해 및 후방주의 확인과 같은 검사를 하는 기계학습 모델에 의해 이루어집니다. 만약 영상에 문제가 있다면 작업을 [데드레터 큐](#데드레터-큐(dead-letter-queues))에 넣어 제어 팀에서 추가적인 조사를 할 수 있게 합니다.

- **트랜스코더**

[트랜스코딩](https://en.wikipedia.org/wiki/Transcoding)은 원본 데이터가 비압축인 중간 포맷으로 디코드됐을 때 타겟 포맷으로 인코드하는 과정을 말앟ㅂ니다. 이 처리 과정은 서로 다른 [코덱](https://en.wikipedia.org/wiki/Video_codec)을 사용하여 비트레이트 조절, 이미지 다운샘플링 또는 미디어 재인코딩을 수행합니다.

그 결과로 더 작은 파일과 타겟 디바이스에 더 최적화된 포맷을 갖게 됩니다. [FFmpeg](https://ffmpeg.org)와 같은 단독 솔루션이나 [AWS Elemental MediaConvert](https://aws.amazon.com/mediaconvert)와 같은 클라우드 기반 솔루션을 사용하여 이 파이프라인을 구현할 수 있습니다.

- **품질 전환**

처리 파이프라인의 마지막 단계인 이것은 이름에서 알 수 있듯이 이전에 트랜스코드된 미디어를 4K나 1440p, 1080p, 720p 등의 다른 해상도로 변환하는 작업을 수행합니다.

이렇게 하여 유저의 요청에 따라 원하는 퀄리티의 영상을 받을 수 있게 되며 일단 미디어 처리가 끝나면 [HDFS](#hdfs), [GlusterFS](https://www.gluster.org), 또는 [Amazon S3](https://aws.amazon.com/s3)와 같은 [오브젝트 저장소](#오브젝트-저장소), 분산 파일 저장소에 업로드되어 나중에 스트리밍 도중에 받을 수 있게 됩니다.

_참고: 파이프라인의 일부로 자막이나 썸네일 생성 같은 추가적인 단계를 더할 수 있습니다._

**메시지 큐를 사용하는 이유**

영상 처리는 시간이 걸리는 작업이므로 [메시지 큐](#메시지-큐)를 사용하는 것이 합리적입니다. 메시지 큐는 또한 영상 처리와 업로드 기능을 디커플링합니다. [Amazon SQS](https://aws.amazon.com/sqs) 나 [RabbitMQ](https://www.rabbitmq.com)를 사용할 수 있습니다.

### 영상 스트리밍

영상 스트리밍은 클라이언트와 서버의 관점 모두 어려운 작업입니다. 게다가 인터넷 연결 속도가 사용자별로 편차가 심합니다. 사용자가 같은 콘텐츠를 다시 받지 않도록 하기 위해 [콘텐츠 배포 네트워크](#콘텐츠-배포-네트워크(cdn))를 사용할 수 있습니다.

넷플릭스는 [Open Connect](https://openconnect.netflix.com) 프로그램을 활용하여 한 단계 더 나아갑니다. 이 접근방법에서는 수천 개의 인터넷 서비스 제공자(ISP)와 파트너십을 맺어 트래픽을 현지화하고 콘텐츠를 더 효율적으로 배포합니다.

**넷플릭스의 Open Connect와 기존의 CDN이 다른 점**

넷플릭스 Open Connect는 넷플릭스의 영상 트래픽을 제공하려는 특수 목적으로 만들어진 [콘텐츠 배포 네트워크](#콘텐츠-배포-네트워크(cdn))입니다. 전세계적으로 대략 95%의 트래픽은 Open Connect와 ISP의 직접 연결과 그 고객들의 인터넷 연결에서 발생합니다.

현재 1000개의 개별 지역에 Open Connect 어플라이언스(OCA)를 가지고 있습니다. 장애에 대비하여 OCA는 페일오버가 가능하며 트래픽은 넷플릭스의 서버로 다시 라우팅됩니다.

그에 더해 안정성을 위해 설계된 [적응 비트레이트 스트리밍](https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming) 프로토콜을 사용할 수 있는데, 그 중에는 [HTTP 라이브 스트리밍(HLS)](https://en.wikipedia.org/wiki/HTTP_Live_Streaming)이 있습니다. 적응 비트레이트 스트리밍은 연결의 가능한 속력에 맞추어 재생을 최적화함으로 동적으로 네트워크 환경에 적응합니다.

마지막으로 사용자가 떠난 시간부터 영상을 재생하기 위해(추가 요구사항이었습니다) 간단히 `views` 테이블에 있는 `offset` 속성을 사용해 특정 타임스탬프의 장면 청크를 받아와 사용자가 그 지점부터 재개할 수 있도록 합니다.

### 검색

기존의 DBMS들은 때때로 성능이 충분하지 않을 수 있기 때문에 거대한 양의 데이터를 거의 밀리세컨드 단위, 실시간에 가깝도록 빠르게 저장하고 검색하고 분석할 수 있는 무언가가 필요합니다. [Elasticsearch](https://www.elastic.co)가 이 경우 도움이 될 수 있습니다.

[Elasticsearch](https://www.elastic.co)는 분산형 무료 오픈소스 검색/분석 엔진입니다. 엘라스틱서치는 텍스트와 숫자, 지리, 구조화된 데이터 및 비구조화된 데이터 타입 모두를 다룹니다. 이 엔진은 [Apache Lucene](https://lucene.apache.org)을 기반으로 만들어졌습니다.

**트렌드 콘텐츠를 식별하는 방법**

트렌드 기능은 검색 기능에 기반하게 될 것입니다. 매 `M`초마다 최근 `N`초간 가장 자주 검색된 쿼리를 배치 작업과 같은 방법으로 캐시할 수 있습니다.

### 공유

콘텐츠 공유는 어느 플랫폼에서나 중요한 부분입니다. 이 플랫폼의 경우 URL 줄이기 서비스와 같은 것을 사용하여 사용자들이 공유할 짧은 URL을 생성할 수 있습니다.

_더 자세한 내용은 [URL 줄이기](#url-줄이기) 시스템 설계를 참고하세요._

## 세부 설계

세부 설계 결정에 대한 부분을 더 자세히 알아보겠습니다.

### 데이터 분할

데이터베이스를 수평 확장하려면 데이터를 분할해야 합니다. ([샤딩](#샤딩)이라고도 하는) 수평 분할부터 시작하면 좋습니다. 그 외에 사용해볼 수 있는 분할 방법들은 아래와 같습니다:

- 해시 기반 분할
- 리스트 기반 분할
- 범위 기반 분할
- 복합 분할

이 분할 방법들을 사용해도 여전히 불균일한 데이터 또는 부하 분산이 있을 수 있습니다. 이 문제는 [일관된 해싱](#일관된-해싱)을 사용하여 해결할 수 있습니다.

_자세한 내용은 [샤딩](#샤딩)과 [일관된 해싱](#일관된-해싱) 항목을 참고하세요._

### 지역 락(Geo-blocking)

넷플릭스나 유튜브와 같은 플랫폼은 특정 지역이나 국가에 콘텐츠 제한을 하기 위해 [지역 락](https://en.wikipedia.org/wiki/Geo-blocking)을 사용합니다. 넷플릭스가 제작사나 배급사와 계약을 맺을 때 준수해야 하는 배포와 관련된 법률을 지키기 위해 먼저 구현했습니다. 유튜브의 경우 콘텐츠를 발행할 때 사용자가 제어하게 됩니다.

사용자의 위치는 [IP](#인터넷-프로토콜(ip))를 사용하거나 [Amazon CloudFront](https://aws.amazon.com/cloudfront)와 같이 지역 제한 기능이 있는 서비스의 프로필에 있는 지역 설정을 사용할 수도 있고 [Amazon Route53](https://aws.amazon.com/route53)의 [지리적 라우팅 정책](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html)를 사용하여 특정 지역이나 국가에 콘텐츠가 제한되는 경우 이를 차단하고 사용자를 에러 페이지로 돌립니다.

### 추천

넷플릭스에서는 사용자의 시청 기록을 사용하여 사용자가 다음에 볼 것 같은 영상을 예측하는 기계학습 모델을 사용합니다. [Collaborative Filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)과 같은 알고리즘을 사용할 수 있습니다.

하지만 (유튜브도 포함하여) 넷플릭스는 넷플릭스 추천 엔진이라는 자체 알고리즘을 사용하는데 여기서는 아래와 같은 정보를 추적합니다:

- 사용자의 나이, 성별, 위치와 같은 프로필
- 사용자의 탐색 및 스크롤 행위
- 사용자가 타이틀을 시청한 날짜/시간
- 콘텐츠를 스트리밍하는 디바이스
- 검색 횟수 및 검색어

_자세한 내용은 [넷플릭스 추천 연구소](https://research.netflix.com/research-area/recommendations)를 참고하세요._ 

### 메트릭 및 분석

분석과 메트릭을 기록하는 것은 추가 요구사항 중 하나입니다. 다른 서비스들에서 데이터를 캡쳐하여 대규모 데이터 처리를 위한 오픈 소스 분석 엔진인 [Apache Spark](https://spark.apache.org)를 사용하여 분석을 돌립니다. 그에 더해 중요한 메타데이터를 views 테이블에 저장하여 우리 데이터의 데이터 포인트를 늘립니다.

### 캐싱

캐싱은 스트리밍 플랫폼에서 중요합니다. 사용자 경험을 위해서는 가능한 한 많은 정적 미디어 콘텐츠를 캐시할 수 있어야 합니다. [Redis](https://redis.io)나 [Memcached](https://memcached.org)와 같은 솔루션을 사용할 수 있지만 어떤 캐시 추출 정책을 사용하는 것이 우리 니즈에 잘 맞을까요?

**캐시 추출 정책**

이 시스템에는 [Least Recently Used (LRU)](<https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)>)가 좋은 선택일 수 있습니다. 이 정책에서는 가장 적게 사용되는 키를 먼저 추출합니다.

**캐시 미스 처리**

캐시 미스가 발생하면 데이터베이스에 직접 접근하고 캐시를 새 항목으로 갱신합니다.

_자세한 내용은 [캐싱](#캐싱) 항목을 참고하세요._

### 미디어 스트리밍과 저장소

대부분의 저장소 공간이 썸네일과 비디오와 같은 미디어 파일들을 저장하는 데 사용될 것입니다. 이전에 이야기했던 것과 같이 미디어 서비스가 미디어 파일 업로드와 처리를 모두 담당할 것입니다.

여기에는 분산 파일 저장소인 [HDFS](https://karanpratapsingh.com/courses/system-design/storage#hdfs)나 [GlusterFS](https://www.gluster.org), 또는 [Amazon S3](https://aws.amazon.com/s3)과 같은 [오브젝트 저장소](#오브젝트-저장소)를 사용할 것입니다.

### 콘텐츠 배포 네트워크(CDN)

[콘텐츠 배포 네트워크(CDN)](#콘텐츠-배포-네트워크(cdn))는 대역폭 비용을 줄이면서 콘텐츠의 가용성과 중복성을 늘립니다. 일반적으로 이미지나 영상과 같은 정적 파일들은 CDN에서 제공합니다. 이 경우에 [Amazon CloudFront](https://aws.amazon.com/cloudfront)나 [Cloudflare CDN](https://www.cloudflare.com/cdn)과 같은 서비스를 사용할 수 있습니다.

## 병목을 찾고 해결하기

![netflix-advanced-design](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/netflix/netflix-advanced-design.png)

이 설계에서 단일 실패점과 같은 병목 지점을 찾고 해결해 봅시다:

- 서비스 하나가 충돌하게 된다면?
- 어떻게 컴포넌트 간 트래픽을 분산할 것인가?
- 어떻게 데이터베이스의 부하를 분산할 수 있을까?
- 어떻게 캐시의 가용성을 개선할 수 있을까?

시스템을 더욱 탄력적으로 만들기 위해 아래와 같이 할 수 있습니다:

- 각 서비스마다 여러 인스턴스를 실행하기
- 클라이언트와 서버, 데이터베이스, 캐시 서버 사이에 [로드 밸런서](#로드-밸런싱)를 도입하기
- 데이터베이스에 여러 읽기전용 레플리카를 두기
- 분산 캐시에 여러 인스턴스와 레플리카를 두기

# Uber

이번에는 [Uber](https://uber.com)와 같은 탑승 호출 서비스를 설계해 봅시다. 비슷한 서비스로는 [Lyft](https://www.lyft.com)나 [OLA Cabs](https://www.olacabs.com) 등이 있습니다.

## Uber란

Uber는 택시처럼 사용자가 차량이나 운전자를 예약하여 이동할 수 있게 해 주는 이동 서비스입니다. 서비스는 웹페이지와 안드로이드, iOS와 같은 모바일 플랫폼으로 이용할 수 있습니다.

## 요구사항

이 시스템은 아래와 같은 요구사항을 만족해야 합니다:

### 기능적 요구사항

두 종류의 사용자를 위한 시스템을 디자인할 것입니다: 승객과 운전자.

**승객**

- 승객은 주변에 있는 모든 차량들을 볼 수 있어야 합니다. 각 차량은 도달 시간과 가격 정보를 가집니다.
- 승객은 목적지로 가기 위한 차량을 예약할 수 있어야 합니다.
- 승객은 운전자의 위치를 볼 수 있어야 합니다.

**운전자**

- 운전자는 승객이 요청한 탑승을 수락하거나 거절할 수 있어야 합니다.
- 운전자가 탑승을 수락한 경우 승객을 태울 위치를 볼 수 있어야 합니다.
- 운전자는 목적지에 도착했을 때 여정을 완료할 수 있어야 합니다.

### 비기능적 요구사항

- 높은 신뢰성
- 높은 가용성과 최소한의 지연
- 확장 가능하며 효율적인 시스템

### 확장 요구사항

- 승객은 완료한 여정에 점수를 매길 수 있습니다.
- 요금 지불 처리
- 지표 및 분석

## 추정 및 제약

우선 얼마나 필요한지, 제약은 무엇인지 알아보는 것부터 시작하겠습니다.

_참고: 규모나 트래픽 관련한 가정들은 어떤 것이든 면접관에게 확인하시기 바랍니다._

### 트래픽

일일 활성 사용자수(DAU)는 1억명이고 100만명의 운전자가 있으며 평균적으로 플랫폼에서 매일 1000만건의 탑승이 발생한다고 가정해 봅시다.

각 사용자가 평균적으로 10건의 활동을(가능한 탑승 조회, 요금, 예약 등) 한다고 하면 매일 10억 건의 요청을 처리하게 될 것입니다.

$$
1 \space 억 \times 10 \space 활동(건수) = 10 \space 억/하루
$$

**시스템의 초당 요청수(RPS)**

매일 10억건의 요청은 대략 초당 12,000건의 요청으로 볼 수 있습니다.

$$
\frac{10 \space 억}{(24 \space 시간 \times 3600 \space 초)} = \sim 12,000 \space 요청/s
$$

### 저장소

메시지 크기가 평균적으로 400바이트라고 한다면 매일 대략 400GB의 데이터베이스 저장공간이 필요하게 됩니다.

$$
10 \space 억 \times 400 \space bytes = \sim 400 \space GB/하루
$$

그리고 10년간 1.4PB의 저장공간이 필요합니다.

$$
400 \space GB \times 10 \space 년 \times 365 \space 일 = \sim 1.4 \space PB
$$

### 대역폭

매일 400GB의 유입을 처리하기 때문에 최소 초당 4MB의 대역폭이 필요합니다.

$$
\frac{400 \space GB}{(24 \space 시간 \times 3600 \space 초)} = \sim 5 \space MB/s
$$

### 고수준 추정

고수준에서 추정한 값들은 아래와 같습니다:

| Type                      | Estimate    |
| ------------------------- | ----------- |
| Daily active users (DAU)  | 100 million |
| Requests per second (RPS) | 12K/s       |
| Storage (per day)         | ~400 GB     |
| Storage (10 years)        | ~1.4 PB     |
| Bandwidth                 | ~5 MB/s     |

## 데이터 모델 설계

요구사항을 반영하도록 일반적인 데이터 모델을 만들어보았습니다.

![uber-datamodel](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/uber/uber-datamodel.png)

각 테이블은 아래와 같습니다:

**customers**

이 테이블에는 이름이나(`name`) 이메일(`email`)과 같은 승객의 정보를 가집니다.

**drivers**

이 테이블에는 이름이나(`name`) 이메일(`email`), 생일(`dob`)와 같은 운전자의 정보를 가집니다.

**trips**

이 테이블은 승객이 요청한 여정을 저장합니다. 여정에는 출발 위치(`source`), 목적지(`destination`), 상태(`status`)와 같은 정보가 있습니다.

**cabs**

이 테이블은 운전자의 차량의 등록번호, 유헝(Uber Go나 Uber XL과 같은)과 같은 정보를 저장합니다.

**ratings**

이름에서 알 수 있듯이 이 테이블에는 여정의 점수(`rating`)와 후기(`feedback`)를 저장합니다.

**payments**

이 테이블은 관련된 `tripID`의 요금과 관련된 정보를 저장합니다.

### 데이터베이스의 종류

데이터 모델은 관계형으로 보이지만 꼭 데이터베이스 하나에 모든 것을 저장할 필요는 없습니다. 오히려 확장성을 제한하게 되어 손쉽게 병목이 될 수도 있습니다.

여기서는 데이터를 서비스별로 나누어 각 서비스가 특정 테이블에 오너십을 가지도록 할 것입니다. 그렇다면 사용 방법에 따라[PostgreSQL](https://www.postgresql.org)과 같은 관계형 데이터베이스나 [Apache Cassandra](https://cassandra.apache.org/_/index.html)와 같은 분산형 NoSQL 데이터베이스를 사용할 수도 있습니다.

## API 설계

이 서비스에서 사용할 기본적인 API 설계를 해 보겠습니다:

### 탑승 요청하기(requestRide)

승객은 이 API를 통해 탑승을 요청할 수 있습니다.

```tsx
requestRide(customerID: UUID, source: Tuple<float>, destination: Tuple<float>, cabType: Enum<string>, paymentMethod: Enum<string>): Ride
```

**파라미터**

Customer ID (`UUID`): 승객의 ID입니다.

Source (`Tuple<float>`): 여정의 시작 위치의 위도와 경도 두 숫자로 구성된 튜플입니다.

Destination (`Tuple<float>`): 여정의 도착 위치의 위도와 경도 두 숫자로 구성된 튜플입니다.

**반환**

Result (`Ride`): 여정의 탑승 정보입니다.

### 탑승 취소하기(cancelRide)

이 API는 승객이 탑승을 취소할 수 있게 해 줍니다.

```tsx
cancelRide(customerID: UUID, reason?: string): boolean
```

**파리미터**

Customer ID (`UUID`): 승객의 ID입니다.

Reason (`string`): 취소 사유입니다 _(선택사항)_.

**반환**

Result (`boolean`): 작업이 성공했는지 여부입니다.

### 탑승 수락 또는 거절하기

이 API는 운전자가 여정을 수락하거나 거절할 수 있게 해 줍니다.

```tsx
acceptRide(driverID: UUID, rideID: UUID): boolean
denyRide(driverID: UUID, rideID: UUID): boolean
```

**파라미터**

Driver ID (`UUID`): 운전자의 ID입니다.

Ride ID (`UUID`): 요청받은 탑승의 ID입니다.

**반환**

Result (`boolean`): 작업이 성공했는지 여부입니다.

### 여정을 시작하거나 종료하기

운전자는 이 API를 사용하여 여정을 시작/종료할 수 있습니다.

```tsx
startTrip(driverID: UUID, tripID: UUID): boolean
endTrip(driverID: UUID, tripID: UUID): boolean
```

**파라미터**

Driver ID (`UUID`): 운전자의 ID입니다.

Trip ID (`UUID`): 요청된 여정의 ID입니다.

**반환**

Result (`boolean`): 작업이 성공했는지 여부입니다.

### 점수 주기(rateTrip)

이 API를 통해 승객이 여정에 점수를 줄 수 있습니다.

```tsx
rateTrip(customerID: UUID, tripID: UUID, rating: int, feedback?: string): boolean
```

**파라미터**

Customer ID (`UUID`): 승객의 ID입니다.

Trip ID (`UUID`): 완료된 여정의 ID입니다.

Rating (`int`): 여정의 점수입니다.

Feedback (`string`): 이 여정에 대한 승객의 후기입니다 _(선택사항)_.

**반환**

Result (`boolean`): 작업이 성공했는지 여부입니다.

## 고수준 설계

이제 시스템의 고수준 설계를 해 보겠습니다.

### 구조

[마이크로서비스 구조](https://karanpratapsingh.com/courses/system-design/monoliths-microservices#microservices)를 사용할 것입니다. 이 구조가 수평 확장이 쉽고 서비스들 간 결합도를 낮추기도 어렵지 않기 때문입니다. 각 서비스는 각자의 데이터 모델에 오너십을 가지게 됩니다. 이 시스템을 몇몇 핵심 서비스로 나누어 보겠습니다.

**승객 서비스**

이 서비스는 인증이나 승객 정보와 같은 승객과 관련된 작업들을 처리합니다.

**운전자 서비스**

이 서비스는 인증이나 운전자 정보와 같은 운전자 관련된 작업들을 처리합니다.

**탑승 서비스**

이 서비스는 탑승 매칭과 쿼드트리 집계를 담당할 것입니다. 나중에 각각 설명할 것입니다.

**여정 서비스**

이 서비스는 시스템의 여정과 관련된 기능들을 처리합니다.

**지불 서비스**

이 서비스는 시스템의 요금 지불을 담당합니다.

**알림 서비스**

이 서비스는 사용자들에게 푸시 알림을 보냅니다. 나중에 설명할 것입니다.

**분석 서비스**

이 서비스는 지표 측정과 분석을 위해 사용될 것입니다.

**서비스 간 통신 및 서비스 디스커버리**

이 구조는 마이크로서비스 기반이기 떄문에 각 서비스들은 서로 통신을 하게 됩니다. 일반적으로 REST 또는 HTTP로도 잘 동작하지만 더 나은 성능을 위해 더 가볍고 효율적인 [gRPC](#grpc)를 사용할 수 있습니다.

[서비스 디스커버리](#서비스-디스커버리) 역시 고려해야 할 부분입니다. 서비스 사이에서 관리되고 관측 가능하며 안전한 통신 을 위해 서비스 메시를 사용할 수 있습니다.

_참고: [REST, GraphQL, gRPC](#rest-graphql-grpc)에 대해 알아보고 어떻게 다른지 비교해 보세요._

### 서비스가 동작하는 방식

아래 다이어그램에 서비스가 어떻게 동작할 지 설명해줍니다:

![uber-working](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/uber/uber-working.png)

1. 승객이 출발 위치, 목적지, 차량 타입, 지불 방법 등을 가지고 탑승을 요청합니다.
2. 탑승 서비스가 이 요청을 등록하고 근처의 운전자들을 찾아 도착 시간(ETA)을 계산합니다.
3. 요청이 주변 운전자들에게 발송되어 수락할 지 거부할 지 선택하도록 합니다.
4. 운전자가 탑승을 수락하면 승객은 탑승을 기다리는 동안 운전자의 실시간 위치와 추정 도착 시간(ETA)을 안내받습니다.
5. 승객이 탑승하면 운전자가 여정을 시작할 수 있습니다.
6. 목적지에 도착하면 운전자가 탑승을 완료처리하고 요금을 수령합니다.
7. 요금이 지불되면 승객은 점수를 남길 수 있고 필요한 경우 후기를 남길 수 있습니다.

### 위치 추적

어떻게 고객(승객 및 운전자)들의 실시간 위치 정보를 효율적으로 백엔드에 보내고 받을 수 있을까요? 두 가지 방법이 있습니다:

**풀 모델(Pull model)**

클라이언트가 주기적으로 서버에 현재 위치를 HTTP 요청으로 보내 ETA와 요금 정보를 획득합니다. 이 작업은 [Long polling](#롱폴링-long-polling)과 같은 방법으로도 가능합니다.

**푸시 모델(Push model)**

클라이언트와 서버가 오래 가는 연결을 맺은 뒤 새 데이터가 생기는 경우 클라이언트에게 전달합니다. [웹소켓](#웹소켓)이나 [서버 전송 이벤트(SSE)](#서버-전송-이벤트-sse)로 구현할 수 있습니다.

풀 모델은 서버에 불필요한 요청을 전송하여 오버헤드를 일으키고 응답 역시 대부분 비어있기 때문에 리소스를 낭비하게 됩니다. 지연을 줄이기 위해 [웹소켓](#웹소켓)을 사용한 푸시 모델을 사용하는 것이 더 좋은 선택입니다. 푸시 모델은 데이터가 발생하는 경우 클라이언트에 열려 있는 연결을 통해 딜레이 없이 전송이 가능하기 때문입니다. 또한 단방향 통신인 [SSE](#서버-전송-이벤트-sse)와는 다르게 웹소켓은 전이중 통신이기 때문에 더 유리합니다.

그에 더해 클라이언트 어플리케이션은 백그라운드에서도 GPS 위치를 전송할 수 있는 백그라운드 잡 메커니즘을 가지고 있어야 합니다.

_참고: [롱폴링, 웹소켓, SSE](#롱폴링-웹소켓-sse)에 대해 더 알아보세요._

### 탑승 매칭

효율적으로 근처 운전자들을 조회하고 저장하는 기능이 필요합니다. 이 설계에 적용할 수 있는 솔루션들을 찾아보겠습니다.

**SQL**

승객의 위도와 경도 정보에 접근할 수 있기 때문에 [PostgreSQL](https://www.postgresql.org)나 [MySQL](https://www.mysql.com)과 같은 데이터베이스를 통해 근처에 있는 운전자들을 위도와 경도(X, Y), 반경(R)을 사용하여 구할 수 있습니다. 

```sql
SELECT * FROM locations WHERE lat BETWEEN X-R AND X+R AND long BETWEEN Y-R AND Y+R
```

하지만 이 방법은 확장이 되지 않고 거대한 데이터세트에 쿼리를 하는 것은 굉장히 느릴 것입니다.

**지오해싱(Geohashing)**

[지오해싱(Geohashing)](#지오해싱(Geohashing))은 [지오코딩(geocoding)](https://en.wikipedia.org/wiki/Address_geocoding)의 일종으로 위도나 경도와 같은 지리 좌표를 짧은 문자열로 인코딩하는 것입니다. [Gustavo Niemeyer](https://twitter.com/gniemeyer)가 2008년에 고안하였습니다.

지오해시는 Base32 알파벳 인코딩을 사용하는 계층적 공간 인덱스입니다. 지오해시의 첫번째 글자는 32개의 구획 중 하나를 지치합니다. 해당 구획 역시 32개의 구획으로 나뉩니다. 다시 말해 지점 하나를 표현하기 위해 원하는 정밀도에 이를 때까지 비트를 사용하여 지도를 재귀적으로 작고 작은 구획들로 나누는 것입니다. 정밀도가 구획의 크기를 결정하게 됩니다.

![geohashing](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/geohashing-and-quadtrees/geohashing.png)

예를 들어, 샌프란시스코의 좌표 `37.7564, -122.4016`는 `9q8yy9mf`라는 지오해시로 변환됩니다.

승객의 지오해시를 이용하면 단순히 운전자의 지오해시와 비교하는 것만으로도 쉽게 가장 가까운 운전자를 찾을 수 있습니다. 더 좋은 성능을 원한다면 운전자의 지오해시를 메모리에 저장하고 인덱싱하여 더 빠르게 조회할 수 있습니다.

**쿼드트리**

[쿼드트리](#쿼드트리(quadtrees))는 트리 구조의 하나인데 각 노드가 네 개의 자식 노드를 가지는 특징이 있습니다. 쿼드트리는 재귀적으로 2차원의 공간을 4개의 4분면 또는 구역으로 나눕니다. 각각의 자식 노드 또는 리프 노드는 공간 정보를 저장합니다. 3차원 공간을 나누는데 사용하는 [Octrees](https://en.wikipedia.org/wiki/Octree)의 2차원 버전으로도 볼 수 있습니다.

![quadtree](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/geohashing-and-quadtrees/quadtree.png)

쿼드트리는 효율적으로 2차원 범위에 있는 지점을 찾을 수 있게 해 줍니다. 지점은 위도/경도로 표현할 수도 있고 (x, y)와 같은 카테시안 좌표로도 표현할 수 있습니다.

노드를 특정 임계점까지만 나누는 것으로 계산량을 줄일 수 있습니다.

![quadtree-subdivision](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-IV/geohashing-and-quadtrees/quadtree-subdivision.png)

[쿼드트리](쿼드트리(quadtrees))가 이 시스템에 적합해 보이기 때문에 운전자의 새 위치를 받을 때마다 쿼드트리를 업데이트하게 만들 수 있습니다. 쿼드트리 서버의 부하를 줄이기 위해 [Redis](https://redis.io)와 같은 인메모리 데이터베이스를 사용하여 최신 업데이트를 캐싱할 수 있습니다. 또한 [힐베르트 커브](https://en.wikipedia.org/wiki/Hilbert_curve)와 같은 매핑 알고리즘을 사용하여 효율적으로 승객과 가까운 근처 운전자들을 조회할 수 있습니다.

**경합(Race conditions)**

수많은 승객들이 동시다발적으로 탑승을 요청하는 경우 쉽게 경합이 발생할 수 있습니다. 이를 피하기 위해 [뮤텍스](https://en.wikipedia.org/wiki/Lock_(computer_science))로 탑승 매칭 로직을 감쌀 수 있습니다. 그에 더해 어떤 행동이든 기본적으로 트랜잭션으로 진행되어야 합니다. 

_더 자세한 내용은 [Transactions](#트랜잭션)과 [분산 트랜잭션](#분산-트랜잭션)을 참고하세요._

**근처에 있는 최적의 운전자 찾기**

이미 쿼드트리 서버를 통해 근처 운전자들의 목록을 가져올 수 있으므로 평균 점수나 위치, 이전 승객 후기 등을 파라미터로 활용하여 랭킹을 구할 수 있습니다. 이를 통해 최적의 운전자를 우선으로 보여줄 수 있습니다.

**높은 수요 처리하기**

높은 수요가 발생하는 경우 서지 가격(surge pricing)과 같은 개념을 사용할 수 있습니다. 서지 가격은 동적 가격 정책의 일종으로 제한된 공급에 수요가 증가하는 것에 대해 일시적으로 가격을 올리는 것입니다. 서지 가격은 여정의 기본 가격에 추가될 수 있습니다.

_더 자세한 내용은 우버의 [서지 가격이 동작하는 원리](https://www.uber.com/us/en/drive/driver-app/how-surge-works)를 참고하세요._

### 지불

대규모로 지불을 처리하는 것은 어렵기 때문에 시스템을 단순하게 만들기 위해서 [Stripe](https://stripe.com)나 [PayPal](https://www.paypal.com)과 같은 서드파티 프로세서를 사용할 수 있습니다. 지불 처리가 완료되면 사용자를 어플리케이션으로 돌아오게 하고 [웹훅](https://en.wikipedia.org/wiki/Webhook)을 사용하여 모든 지불 관련 데이터를 수집할 수 있습니다.

### 알림

푸시 알림은 플랫폼의 통합적인 부분이 될 것입니다. [Apache Kafka](https://kafka.apache.org)와 같은 메시지 큐 또는 메시지 브로커와 알림 서비스를 통하여 요청을 [파이어베이스 클라우드 메시징(FCM)](https://firebase.google.com/docs/cloud-messaging)이나 [애플 푸시 알림 서비스(APNS)](https://developer.apple.com/documentation/usernotifications)로 발송하여 사용자 디바이스에 푸시 알림을 전달하게 합니다.

_더 자세한 내용은 [WhatsApp](https://karanpratapsingh.com/courses/system-design/whatsapp#notifications) 시스템 설계에서 푸시 알림을 처리한 방법을 참고하세요._

## 세부 설계

설계상 결정 사항들을 자세히 살펴보겠습니다.

### 데이터 분할(data partitioning)

데이터베이스를 확장하기 위해서는 데이터를 분할해야 할 필요가 있습니다. 수평 분할, [샤딩](#샤딩)이 좋은 시작입니다. [파티션 스킴](#분할-기준(partition-criteria)) 또는 구역으로 데이터베이스를 샤딩할 수 있습니다. 우편번호와 같은 위치들을 구역으로 나누게 되면 고정된 노드에 해당 지역에 대한 모든 데이터를 효율적으로 저장할 수 있습니다. 하지만 여전히 데이터와 부하가 균일하지 않을 수 있기 때문에 [일관된 해싱](#일관된-해싱)을 사용하여 문제를 수정할 수 있습니다.

_더 자세한 내용은, [샤딩](#샤딩) and [일관된 해싱](#일관된-해싱) 문서를 참고하세요._

### 지표 측정과 분석

분석 데이터와 지표를 기록하는 것은 확장 요구사항의 일부입니다. 다른 서비스들에서 데이터를 수집하고 분석을 돌리는 데 오픈 소스이며 대규모 데이터 처리를 하는 단일화된 분석 엔진인[Apache Spark](https://spark.apache.org)를 사용할 수 있습니다. 그에 더해 핵심적인 메타데이터를 뷰 테이블에 저장하여 데이터 포인트를 늘릴 수 있습니다.

### 캐싱

위치 기반 플랫폼에 있어 캐싱은 중요사항입니다. 승객과 운전사의 위치를 빠르게 받기 위해 최근 위치를 캐시할 수 있어야 합니다. [Redis](https://redis.io)나 [Memcached](https://memcached.org)와 같은 솔루션을 사용할 수 있지만 어떤 추출 정책이 가장 요구사항에 적합할까요?

**캐시 추출 정책**

[Least Recently Used (LRU)](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU))가 이 시스템에 좋은 정책일 수 있습니다. 이 정책은 사용한 지 가장 오래된 키를 먼저 버립니다.

**캐시 미스 처리**

캐시 미스가 발생하는 경우 데이터베이스에 서버가 직접 접근하여 새 데이터로 캐시를 업데이트합니다.

_자세한 내용은 [캐싱](#캐싱) 문서를 참고하세요._

## 병목을 확인하고 해결하기

![uber-advanced-design](https://raw.githubusercontent.com/karanpratapsingh/portfolio/master/public/static/courses/system-design/chapter-V/uber/uber-advanced-design.png)

이 설계에서 단일 실패점과 같은 병목을 찾아 해결해 봅시다.

- "만약 서비스 하나가 충돌한다면?"
- "어떻게 컴포넌트 간에 트래픽을 분산할 것인가?"
- "어떻게 데이터베이스의 부하를 줄일 수 있을까?"
- "어떻게 캐시의 가용성을 개선할 수 있을까?"
- "어떻게 시스템을 견고하게 만들 수 있을까?"

시스템이 회복성을 가지기 위해 이런 작업들을 할 수 있습니다:

- 각 서비스마다 여러 인스턴스를 실행하기.
- 클라이언트와 서버, 데이터베이스, 캐시 서버 사이에 [로드밸런서](#로드-밸런싱) 사용하기.
- 데이터베이스에 읽기전용 레플리카 사용하기.
- 분산 캐시에 여러 인스턴스와 레플리카 사용하기.
- 분산 시스템에서 단 한번의 메시지 전송은 어렵기 때문에 [Apache Kafka](https://kafka.apache.org)나 [NATS](https://nats.io)와 같은 [메시지 브로커](#메시지-브로커)를 사용하여 알림 시스템을 더 견고하게 만들 수 있습니다.

# 다음 단계

축하합니다. 코스를 끝마치셨습니다!

시스템 설계의 기반을 알게 되었습니다. 추가적인 자료는 아래에 있습니다:

- [분산 시스템](https://www.youtube.com/watch?v=UEAMfLPZZhE&list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB) (by Dr. Martin Kleppmann)
- [시스템 설계 면접: 내부인의 가이드](https://www.amazon.in/System-Design-Interview-insiders-Second/dp/B08CMF2CQF)
- [마이크로서비스](https://microservices.io) (by Chris Richardson)
- [서버리스 컴퓨팅](https://en.wikipedia.org/wiki/Serverless_computing)
- [쿠버네티스](https://kubernetes.io)

또한 이 코스에서 배운 내용들을 실제 대규모로 적용하고 있는 기업들의 기술 블로그를 활발하게 구독하는 것도 추천드립니다: 

- [Microsoft Engineering](https://engineering.microsoft.com)
- [Google Research Blog](http://googleresearch.blogspot.com)
- [Netflix Tech Blog](http://techblog.netflix.com)
- [AWS Blog](https://aws.amazon.com/blogs/aws)
- [Facebook Engineering](https://www.facebook.com/Engineering)
- [Uber Engineering Blog](http://eng.uber.com)
- [Airbnb Engineering](http://nerds.airbnb.com)
- [GitHub Engineering Blog](https://github.blog/category/engineering)
- [Intel Software Blog](https://software.intel.com/en-us/blogs)
- [LinkedIn Engineering](http://engineering.linkedin.com/blog)
- [Paypal Developer Blog](https://medium.com/paypal-engineering)
- [Twitter Engineering](https://blog.twitter.com/engineering)

끝은 아니지만 마지막으로, 회사에서 새로 진행하는 프로젝트에 자원하여 시니어 엔지니어나 아키텍트에게서 여러분의 시스템 설계 기술을 발전하시기 바랍니다.

이 코스가 훌륭한 학습이 되었기를 바랍니다. 언제든지 피드백 부탁드리겠습니다.

이후 공부들도 응원합니다!

# 참고 자료

이 코스를 제작하는 데 참고한 자료들입니다.

- [Cloudflare learning center](https://www.cloudflare.com/learning)
- [IBM Blogs](https://www.ibm.com/blogs)
- [Fastly Blogs](https://www.fastly.com/blog)
- [NS1 Blogs](https://ns1.com/blog)
- [Grokking the System Design Interview](https://www.educative.io/courses/grokking-the-system-design-interview)
- [System Design Primer](https://github.com/donnemartin/system-design-primer)
- [AWS Blogs](https://aws.amazon.com/blogs)
- [Martin Fowler](https://martinfowler.com)
- [PagerDuty resources](https://www.pagerduty.com/resources)
- [VMWare Blogs](https://blogs.vmware.com/learning)

_여기 있는 모든 다이어그램들은 [Excalidraw](https://excalidraw.com)로 그려졌으며 [여기](https://github.com/karanpratapsingh/system-design/tree/main/diagrams)에서 볼 수 있습니다._
